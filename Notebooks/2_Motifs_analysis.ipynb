{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Motifs analysis\n",
    "\n",
    "Notebook to perform the discriminative motifs analysis. It requires a trained model but it is an independant analysis from the analysis of feature space and from the prototypes analysis.\n",
    "\n",
    "Motifs are extracted on the base of Class-Activation Maps (CAMs) which display the saliency of a class in a given input according to a model. CAMs towards any class can be computed regardless of the actual class of the input. This means that one can look for discriminative motifs of class B in an input of class A. However, for the sake of motif extraction, we don't use this feature of CAMs. Instead we produce CAMs towards the actual class of the input.\n",
    "\n",
    "The motif extraction procedure is as follow:\n",
    "1. Select trajectories from which to extract motifs.\n",
    "2. Compute CAM for each trajectory (saliency towards its own class).\n",
    "3. Binarize each time point into 'relevant' and 'non-relevant' to recognize input class.\n",
    "4. Optional but recommended, extend the 'relevant' regions to capture the full motifs. Also filter for motif length.\n",
    "5. Extract the longest 'relevant' stretches of time-points. These are the final motifs.\n",
    "\n",
    "In order to visualize these motifs, we propose to cluster them afterwards as follow:\n",
    "1. Build a distance matrix between the motifs with dynamic time warping (dtw)\n",
    "2. Cluster with hierarchical clustering.\n",
    "3. Visualize dynamics captured by each cluster.\n",
    "\n",
    "This clustering can be run in 2 modes: either patterns from every class are pooled together, either a separate clustering is run indepently for each class. In the 1st case, this will reflect the diversity of patterns at the dataset level and can reveal dynamics overlap between classes. In the second case, the emphasis is put on the diversity of dynamics induced by each class.\n",
    "\n",
    "\n",
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from load_data import DataProcesser\n",
    "from results_model import top_confidence_perclass, least_correlated_set\n",
    "from pattern_utils import extend_segments, create_cam, longest_segments, extract_pattern\n",
    "from class_dataset import myDataset, ToTensor, RandomCrop\n",
    "from dtaidistance import dtw, clustering\n",
    "from models import ConvNetCam\n",
    "from skimage.filters import threshold_li, threshold_mean\n",
    "import os\n",
    "from itertools import chain\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Parameters for the motifs extraction:\n",
    "- selected_set: str one of ['both', 'validation', 'training'], from which set of trajectories should motifs be extracted? For this purprose, extracting from training data also makes sense.\n",
    "- n_series_perclass: int, maximum number of series, per class, on which motif extraction is attempted.\n",
    "- n_pattern_perseries: int, maximum number of motifs to extract out of a single trajectory.\n",
    "- mode_series_selection: str one of ['top_confidence', 'least_correlated']. Mode to select the trajectories from which to extract the motifs (see Prototype analysis). If top confidence, the motifs might be heavily biased towards a representative subpopulation of the class. Hence, the output might not reflect the whole diversity of motifs induced by the class.\n",
    "- extend_patt: int, by how many points to extend motifs? After binarization into 'relevant' and 'non-relevant time points', the motifs are usually fragmented because a few points in their middle are improperly classified as 'non-relevant'. This parameter allows to extend each fragment by a number of time points (in both time directions) before extracting the actual patterns.\n",
    "- min_len_patt/max_len_patt: int, set minimum/maximum size of a motif. **/!\\ The size is given in number of time-points. This means that if the input has more than one channel, the actual length of the motifs will be divided across them.**\n",
    "\n",
    "Parameters for the motifs clustering:\n",
    "- center_patt: bool, whether to zero-center patterns prior to clustering. If the input is multivariate, each channel is independantly zero-centered. This matters for DTW calculation.\n",
    "- normalize_dtw: bool, whether to normalize DTW distance to the length of the trajectories. This is important to compare motifs of varying lengths.\n",
    "- export_perClass: bool, whether to run the motif clustering class per class.\n",
    "- export_allPooled: bool, whether to pool all motifs across classes for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_set = 'both'\n",
    "n_series_perclass = 125\n",
    "n_pattern_perseries = 1\n",
    "mode_series_selection = 'least_correlated'\n",
    "thresh_confidence = 0.9  # used in least_correlated mode to choose set of series with minimal classification confidence\n",
    "extend_patt = 0\n",
    "min_len_patt = 5\n",
    "max_len_patt = 400  # length to divide by nchannel\n",
    "\n",
    "export_perClass = True\n",
    "export_allPooled = True\n",
    "\n",
    "assert mode_series_selection in ['top_confidence', 'least_correlated']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and data\n",
    "\n",
    "- Pay attention to the order of 'meas_var', should be the same as for training the model!\n",
    "- Pay attention to trajectories preprocessing.\n",
    "- Set batch_size as high as memory allows for speed up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_file = '/home/marc/Dropbox/Work/TSclass_GF/data/ErkAkt_6GF_len240_repl2_trim100.zip'\n",
    "# data_file = '/home/marc/Dropbox/CNN_paper_MarcAntoine/forPaper/data_analysis/synthetic_len750_univariate_classAB.zip'\n",
    "data_file = '/home/marc/Dropbox/CNN_paper_MarcAntoine/forPaper/data_analysis/synthetic_len750.zip'\n",
    "# model_file = '/home/marc/Dropbox/Work/TSclass_GF/forPaper/models/ERK_AKT/2019-07-04-11:21:58_ErkAkt_6GF_len240_repl2_trim100.pytorch'\n",
    "# model_file = '/home/marc/Dropbox/CNN_paper_MarcAntoine/forPaper/models/FRST_classAB/2020-02-11-15:52:07_synthetic_len750_univariate_classAB.pytorch'\n",
    "model_file = '/home/marc/Dropbox/CNN_paper_MarcAntoine/forPaper/models/FRST_SCND/2020-02-11-14:44:57_synthetic_len750.pytorch'\n",
    "\n",
    "meas_var = None  # Set to None for auto detection\n",
    "start_time = None  # Set to None for auto detection\n",
    "end_time = None  # Set to None for auto detection\n",
    "\n",
    "batch_size = 200\n",
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if is_cuda else 'cpu')\n",
    "model = torch.load(model_file)\n",
    "model.eval()\n",
    "model.double()\n",
    "model.batch_size = batch_size\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pay attention that **data.process() is already centering the data**, so don't do a second time when loading the data in the dataloader. The **random crop** should before passing the trajectories in the model to ensure that the same crop is used as input and for extracting the patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations to perform when loading data into the model\n",
    "ls_transforms = transforms.Compose([RandomCrop(output_size=model.length, ignore_na_tails=True),\n",
    "                                                            ToTensor()])\n",
    "# Loading and PREPROCESSING\n",
    "data = DataProcesser(data_file)\n",
    "meas_var = data.detect_groups_times()['groups'] if meas_var is None else meas_var\n",
    "start_time = data.detect_groups_times()['times'][0] if start_time is None else start_time\n",
    "end_time = data.detect_groups_times()['times'][1] if end_time is None else end_time\n",
    "\n",
    "data.subset(sel_groups=meas_var, start_time=start_time, end_time=end_time)\n",
    "data.get_stats()\n",
    "data.process(method='center_train', independent_groups=True)  # do here and not in loader so can use in df\n",
    "data.crop_random(model.length, ignore_na_tails=True)\n",
    "data.split_sets(which='dataset')\n",
    "classes = tuple(data.classes.iloc[:, 1])\n",
    "classes_dict = data.classes['class']\n",
    "\n",
    "# Random crop before to keep the same in df as the ones passed in the model\n",
    "if selected_set == 'validation':\n",
    "    selected_data = myDataset(dataset=data.validation_set, transform=ls_transforms)\n",
    "    df = data.validation_set\n",
    "elif selected_set == 'training':\n",
    "    selected_data = myDataset(dataset=data.train_set, transform=ls_transforms)\n",
    "    df = data.train_set\n",
    "elif selected_set == 'both':\n",
    "    try:\n",
    "        selected_data = myDataset(dataset=data.dataset_cropped, transform=ls_transforms)\n",
    "        df = data.dataset_cropped\n",
    "    except:\n",
    "        selected_data = myDataset(dataset=data.dataset, transform=ls_transforms)\n",
    "        df = data.dataset\n",
    "\n",
    "data_loader = DataLoader(dataset=selected_data,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True,\n",
    "                         num_workers=4)\n",
    "# Dataframe used for retrieving trajectories. wide_to_long instead of melt because can do melting per group of columns\n",
    "df = pd.wide_to_long(df, stubnames=meas_var, i=[data.col_id, data.col_class], j='Time', sep='_', suffix='\\d+')\n",
    "df = df.reset_index()  # wide_to_long creates a multi-level Index, reset index to retrieve indexes in columns\n",
    "df.rename(columns={data.col_id: 'ID', data.col_class: 'Class'}, inplace=True)\n",
    "df['ID'] = df['ID'].astype('U32')\n",
    "del data  # free memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select trajectories from which to extract patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:29<00:00,  2.25it/s]"
     ]
    }
   ],
   "source": [
    "if mode_series_selection == 'least_correlated':\n",
    "    set_trajectories = least_correlated_set(model, data_loader, threshold_confidence=thresh_confidence, device=device,\n",
    "                                            n=n_series_perclass, labels_classes=classes_dict)\n",
    "elif mode_series_selection == 'top_confidence':\n",
    "    set_trajectories = top_confidence_perclass(model, data_loader, device=device, n=n_series_perclass,\n",
    "                                               labels_classes=classes_dict)\n",
    "\n",
    "# free some memory by keeping only relevant series\n",
    "selected_trajectories = set_trajectories['ID']\n",
    "df = df[df['ID'].isin(selected_trajectories)]\n",
    "# Make sure that class is an integer (especially when 0 or 1, could be read as boolean)\n",
    "df['Class'] = df['Class'].astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract patterns\n",
    "\n",
    "### Extract, extend and filter patterns. \n",
    "\n",
    "Outputs a report of how many trajectories were filtered out by size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 2/500 [00:00<00:37, 13.39it/s]\u001b[A\n",
      "  1%|          | 4/500 [00:00<00:33, 14.65it/s]\u001b[A\n",
      "  1%|          | 6/500 [00:00<00:31, 15.61it/s]\u001b[A\n",
      "  2%|▏         | 8/500 [00:00<00:30, 16.27it/s]\u001b[A\n",
      "  2%|▏         | 10/500 [00:00<00:29, 16.89it/s]\u001b[A\n",
      "  2%|▏         | 12/500 [00:00<00:28, 17.20it/s]\u001b[A\n",
      "  3%|▎         | 14/500 [00:00<00:27, 17.46it/s]\u001b[A\n",
      "  3%|▎         | 16/500 [00:00<00:27, 17.87it/s]\u001b[A\n",
      "  4%|▎         | 18/500 [00:01<00:26, 18.15it/s]\u001b[A\n",
      "  4%|▍         | 20/500 [00:01<00:26, 18.41it/s]\u001b[A\n",
      "  4%|▍         | 22/500 [00:01<00:25, 18.44it/s]\u001b[A\n",
      "  5%|▍         | 24/500 [00:01<00:25, 18.45it/s]\u001b[A\n",
      "  5%|▌         | 26/500 [00:01<00:26, 18.18it/s]\u001b[A\n",
      "  6%|▌         | 28/500 [00:01<00:26, 18.06it/s]\u001b[A\n",
      "  6%|▌         | 30/500 [00:01<00:26, 17.95it/s]\u001b[A\n",
      "  6%|▋         | 32/500 [00:01<00:26, 17.72it/s]\u001b[A\n",
      "  7%|▋         | 34/500 [00:01<00:26, 17.30it/s]\u001b[A\n",
      "  7%|▋         | 36/500 [00:02<00:26, 17.21it/s]\u001b[A\n",
      "  8%|▊         | 38/500 [00:02<00:26, 17.41it/s]\u001b[A\n",
      "  8%|▊         | 40/500 [00:02<00:26, 17.39it/s]\u001b[A\n",
      "  8%|▊         | 42/500 [00:02<00:26, 17.24it/s]\u001b[A\n",
      "  9%|▉         | 44/500 [00:02<00:27, 16.66it/s]\u001b[A\n",
      "  9%|▉         | 46/500 [00:02<00:26, 16.97it/s]\u001b[A\n",
      " 10%|▉         | 48/500 [00:02<00:26, 17.13it/s]\u001b[A\n",
      " 10%|█         | 50/500 [00:02<00:26, 17.15it/s]\u001b[A\n",
      " 10%|█         | 52/500 [00:02<00:25, 17.26it/s]\u001b[A\n",
      " 11%|█         | 54/500 [00:03<00:26, 16.71it/s]\u001b[A\n",
      " 11%|█         | 56/500 [00:03<00:26, 16.97it/s]\u001b[A\n",
      " 12%|█▏        | 58/500 [00:03<00:26, 16.93it/s]\u001b[A\n",
      " 12%|█▏        | 60/500 [00:03<00:25, 16.94it/s]\u001b[A\n",
      " 12%|█▏        | 62/500 [00:03<00:25, 16.92it/s]\u001b[A\n",
      " 13%|█▎        | 64/500 [00:03<00:26, 16.76it/s]\u001b[A\n",
      " 13%|█▎        | 66/500 [00:03<00:25, 16.71it/s]\u001b[A\n",
      " 14%|█▎        | 68/500 [00:03<00:25, 16.77it/s]\u001b[A\n",
      " 14%|█▍        | 70/500 [00:04<00:25, 16.88it/s]\u001b[A\n",
      " 14%|█▍        | 72/500 [00:04<00:25, 16.80it/s]\u001b[A\n",
      " 15%|█▍        | 74/500 [00:04<00:25, 16.80it/s]\u001b[A\n",
      " 15%|█▌        | 76/500 [00:04<00:25, 16.83it/s]\u001b[A\n",
      " 16%|█▌        | 78/500 [00:04<00:25, 16.30it/s]\u001b[A\n",
      " 16%|█▌        | 80/500 [00:04<00:25, 16.52it/s]\u001b[A\n",
      " 16%|█▋        | 82/500 [00:04<00:25, 16.68it/s]\u001b[A\n",
      " 17%|█▋        | 84/500 [00:04<00:24, 16.88it/s]\u001b[A\n",
      " 17%|█▋        | 86/500 [00:04<00:24, 16.96it/s]\u001b[A\n",
      " 18%|█▊        | 88/500 [00:05<00:24, 16.84it/s]\u001b[A\n",
      " 18%|█▊        | 90/500 [00:05<00:24, 16.65it/s]\u001b[A\n",
      " 18%|█▊        | 92/500 [00:05<00:24, 16.43it/s]\u001b[A\n",
      " 19%|█▉        | 94/500 [00:05<00:24, 16.34it/s]\u001b[A\n",
      " 19%|█▉        | 96/500 [00:05<00:25, 16.15it/s]\u001b[A\n",
      " 20%|█▉        | 98/500 [00:05<00:24, 16.24it/s]\u001b[A\n",
      " 20%|██        | 100/500 [00:05<00:24, 16.25it/s]\u001b[A\n",
      " 20%|██        | 102/500 [00:05<00:24, 16.46it/s]\u001b[A\n",
      " 21%|██        | 104/500 [00:06<00:24, 16.28it/s]\u001b[A\n",
      " 21%|██        | 106/500 [00:06<00:24, 16.19it/s]\u001b[A\n",
      " 22%|██▏       | 108/500 [00:06<00:24, 16.05it/s]\u001b[A\n",
      " 22%|██▏       | 110/500 [00:06<00:24, 15.66it/s]\u001b[A\n",
      " 22%|██▏       | 112/500 [00:06<00:24, 15.52it/s]\u001b[A\n",
      " 23%|██▎       | 114/500 [00:06<00:24, 15.60it/s]\u001b[A\n",
      " 23%|██▎       | 116/500 [00:06<00:24, 15.69it/s]\u001b[A\n",
      " 24%|██▎       | 118/500 [00:07<00:24, 15.69it/s]\u001b[A\n",
      " 24%|██▍       | 120/500 [00:07<00:24, 15.82it/s]\u001b[A\n",
      " 24%|██▍       | 122/500 [00:07<00:24, 15.72it/s]\u001b[A\n",
      " 25%|██▍       | 124/500 [00:07<00:23, 15.79it/s]\u001b[A\n",
      " 25%|██▌       | 126/500 [00:07<00:23, 15.76it/s]\u001b[A\n",
      " 26%|██▌       | 128/500 [00:07<00:23, 15.72it/s]\u001b[A\n",
      " 26%|██▌       | 130/500 [00:07<00:23, 15.68it/s]\u001b[A\n",
      " 26%|██▋       | 132/500 [00:07<00:24, 15.13it/s]\u001b[A\n",
      " 27%|██▋       | 134/500 [00:08<00:24, 14.97it/s]\u001b[A\n",
      " 27%|██▋       | 136/500 [00:08<00:24, 15.14it/s]\u001b[A\n",
      " 28%|██▊       | 138/500 [00:08<00:23, 15.10it/s]\u001b[A\n",
      " 28%|██▊       | 140/500 [00:08<00:23, 15.06it/s]\u001b[A\n",
      " 28%|██▊       | 142/500 [00:08<00:24, 14.83it/s]\u001b[A\n",
      " 29%|██▉       | 144/500 [00:08<00:23, 15.05it/s]\u001b[A\n",
      " 29%|██▉       | 146/500 [00:08<00:23, 15.19it/s]\u001b[A\n",
      " 30%|██▉       | 148/500 [00:08<00:22, 15.34it/s]\u001b[A\n",
      " 30%|███       | 150/500 [00:09<00:22, 15.46it/s]\u001b[A\n",
      " 30%|███       | 152/500 [00:09<00:22, 15.45it/s]\u001b[A\n",
      " 31%|███       | 154/500 [00:09<00:22, 15.47it/s]\u001b[A\n",
      " 31%|███       | 156/500 [00:09<00:22, 15.47it/s]\u001b[A\n",
      " 32%|███▏      | 158/500 [00:09<00:21, 15.56it/s]\u001b[A\n",
      " 32%|███▏      | 160/500 [00:09<00:22, 15.45it/s]\u001b[A\n",
      " 32%|███▏      | 162/500 [00:09<00:22, 15.10it/s]\u001b[A\n",
      " 33%|███▎      | 164/500 [00:10<00:22, 15.18it/s]\u001b[A\n",
      " 33%|███▎      | 166/500 [00:10<00:22, 15.09it/s]\u001b[A\n",
      " 34%|███▎      | 168/500 [00:10<00:22, 15.06it/s]\u001b[A\n",
      " 34%|███▍      | 170/500 [00:10<00:21, 15.14it/s]\u001b[A\n",
      " 34%|███▍      | 172/500 [00:10<00:21, 15.12it/s]\u001b[A\n",
      " 35%|███▍      | 174/500 [00:10<00:21, 15.13it/s]\u001b[A\n",
      " 35%|███▌      | 176/500 [00:10<00:21, 15.02it/s]\u001b[A\n",
      " 36%|███▌      | 178/500 [00:10<00:21, 15.05it/s]\u001b[A\n",
      " 36%|███▌      | 180/500 [00:11<00:21, 14.99it/s]\u001b[A\n",
      " 36%|███▋      | 182/500 [00:11<00:21, 14.95it/s]\u001b[A\n",
      " 37%|███▋      | 184/500 [00:11<00:21, 14.64it/s]\u001b[A\n",
      " 37%|███▋      | 186/500 [00:11<00:21, 14.39it/s]\u001b[A\n",
      " 38%|███▊      | 188/500 [00:11<00:22, 14.00it/s]\u001b[A\n",
      " 38%|███▊      | 190/500 [00:11<00:22, 13.62it/s]\u001b[A\n",
      " 38%|███▊      | 192/500 [00:11<00:22, 13.66it/s]\u001b[A\n",
      " 39%|███▉      | 194/500 [00:12<00:22, 13.71it/s]\u001b[A\n",
      " 39%|███▉      | 196/500 [00:12<00:21, 13.87it/s]\u001b[A\n",
      " 40%|███▉      | 198/500 [00:12<00:21, 13.90it/s]\u001b[A\n",
      " 40%|████      | 200/500 [00:12<00:21, 13.94it/s]\u001b[A\n",
      " 40%|████      | 202/500 [00:12<00:21, 13.91it/s]\u001b[A\n",
      " 41%|████      | 204/500 [00:12<00:21, 13.90it/s]\u001b[A\n",
      " 41%|████      | 206/500 [00:12<00:21, 13.87it/s]\u001b[A\n",
      " 42%|████▏     | 208/500 [00:13<00:20, 13.92it/s]\u001b[A\n",
      " 42%|████▏     | 210/500 [00:13<00:20, 14.05it/s]\u001b[A\n",
      " 42%|████▏     | 212/500 [00:13<00:20, 14.09it/s]\u001b[A\n",
      " 43%|████▎     | 214/500 [00:13<00:20, 14.08it/s]\u001b[A\n",
      " 43%|████▎     | 216/500 [00:13<00:20, 14.08it/s]\u001b[A\n",
      " 44%|████▎     | 218/500 [00:13<00:19, 14.16it/s]\u001b[A\n",
      " 44%|████▍     | 220/500 [00:13<00:19, 14.10it/s]\u001b[A\n",
      " 44%|████▍     | 222/500 [00:14<00:19, 14.18it/s]\u001b[A\n",
      " 45%|████▍     | 224/500 [00:14<00:19, 14.17it/s]\u001b[A\n",
      " 45%|████▌     | 226/500 [00:14<00:19, 14.18it/s]\u001b[A\n",
      " 46%|████▌     | 228/500 [00:14<00:19, 14.00it/s]\u001b[A\n",
      " 46%|████▌     | 230/500 [00:14<00:19, 13.94it/s]\u001b[A\n",
      " 46%|████▋     | 232/500 [00:14<00:19, 13.82it/s]\u001b[A\n",
      " 47%|████▋     | 234/500 [00:14<00:19, 13.94it/s]\u001b[A\n",
      " 47%|████▋     | 236/500 [00:15<00:18, 13.92it/s]\u001b[A\n",
      " 48%|████▊     | 238/500 [00:15<00:18, 13.91it/s]\u001b[A\n",
      " 48%|████▊     | 240/500 [00:15<00:18, 13.87it/s]\u001b[A\n",
      " 48%|████▊     | 242/500 [00:15<00:18, 13.89it/s]\u001b[A\n",
      " 49%|████▉     | 244/500 [00:15<00:18, 13.83it/s]\u001b[A\n",
      " 49%|████▉     | 246/500 [00:15<00:18, 13.68it/s]\u001b[A\n",
      " 50%|████▉     | 248/500 [00:15<00:18, 13.30it/s]\u001b[A\n",
      " 50%|█████     | 250/500 [00:16<00:19, 12.86it/s]\u001b[A\n",
      " 50%|█████     | 252/500 [00:16<00:19, 12.68it/s]\u001b[A\n",
      " 51%|█████     | 254/500 [00:16<00:19, 12.87it/s]\u001b[A\n",
      " 51%|█████     | 256/500 [00:16<00:18, 12.92it/s]\u001b[A\n",
      " 52%|█████▏    | 258/500 [00:16<00:18, 12.97it/s]\u001b[A\n",
      " 52%|█████▏    | 260/500 [00:16<00:18, 13.22it/s]\u001b[A\n",
      " 52%|█████▏    | 262/500 [00:17<00:17, 13.30it/s]\u001b[A\n",
      " 53%|█████▎    | 264/500 [00:17<00:17, 13.20it/s]\u001b[A\n",
      " 53%|█████▎    | 266/500 [00:17<00:18, 12.93it/s]\u001b[A\n",
      " 54%|█████▎    | 268/500 [00:17<00:18, 12.40it/s]\u001b[A\n",
      " 54%|█████▍    | 270/500 [00:17<00:18, 12.18it/s]\u001b[A\n",
      " 54%|█████▍    | 272/500 [00:17<00:18, 12.46it/s]\u001b[A\n",
      " 55%|█████▍    | 274/500 [00:18<00:17, 12.76it/s]\u001b[A\n",
      " 55%|█████▌    | 276/500 [00:18<00:17, 12.81it/s]\u001b[A\n",
      " 56%|█████▌    | 278/500 [00:18<00:17, 12.94it/s]\u001b[A\n",
      " 56%|█████▌    | 280/500 [00:18<00:17, 12.93it/s]\u001b[A\n",
      " 56%|█████▋    | 282/500 [00:18<00:16, 13.09it/s]\u001b[A\n",
      " 57%|█████▋    | 284/500 [00:18<00:16, 12.96it/s]\u001b[A\n",
      " 57%|█████▋    | 286/500 [00:18<00:16, 12.88it/s]\u001b[A\n",
      " 58%|█████▊    | 288/500 [00:19<00:16, 12.96it/s]\u001b[A\n",
      " 58%|█████▊    | 290/500 [00:19<00:16, 12.79it/s]\u001b[A\n",
      " 58%|█████▊    | 292/500 [00:19<00:16, 12.78it/s]\u001b[A\n",
      " 59%|█████▉    | 294/500 [00:19<00:16, 12.86it/s]\u001b[A\n",
      " 59%|█████▉    | 296/500 [00:19<00:15, 12.86it/s]\u001b[A\n",
      " 60%|█████▉    | 298/500 [00:19<00:15, 12.81it/s]\u001b[A\n",
      " 60%|██████    | 300/500 [00:20<00:15, 12.60it/s]\u001b[A\n",
      " 60%|██████    | 302/500 [00:20<00:15, 12.41it/s]\u001b[A\n",
      " 61%|██████    | 304/500 [00:20<00:15, 12.35it/s]\u001b[A\n",
      " 61%|██████    | 306/500 [00:20<00:15, 12.40it/s]\u001b[A\n",
      " 62%|██████▏   | 308/500 [00:20<00:15, 12.36it/s]\u001b[A\n",
      " 62%|██████▏   | 310/500 [00:20<00:15, 12.16it/s]\u001b[A\n",
      " 62%|██████▏   | 312/500 [00:21<00:15, 12.19it/s]\u001b[A\n",
      " 63%|██████▎   | 314/500 [00:21<00:15, 12.32it/s]\u001b[A\n",
      " 63%|██████▎   | 316/500 [00:21<00:14, 12.36it/s]\u001b[A\n",
      " 64%|██████▎   | 318/500 [00:21<00:14, 12.30it/s]\u001b[A\n",
      " 64%|██████▍   | 320/500 [00:21<00:14, 12.14it/s]\u001b[A\n",
      " 64%|██████▍   | 322/500 [00:21<00:14, 11.90it/s]\u001b[A\n",
      " 65%|██████▍   | 324/500 [00:22<00:14, 11.88it/s]\u001b[A\n",
      " 65%|██████▌   | 326/500 [00:22<00:14, 11.82it/s]\u001b[A\n",
      " 66%|██████▌   | 328/500 [00:22<00:14, 11.97it/s]\u001b[A\n",
      " 66%|██████▌   | 330/500 [00:22<00:14, 11.82it/s]\u001b[A\n",
      " 66%|██████▋   | 332/500 [00:22<00:14, 11.73it/s]\u001b[A\n",
      " 67%|██████▋   | 334/500 [00:22<00:13, 11.93it/s]\u001b[A\n",
      " 67%|██████▋   | 336/500 [00:23<00:13, 11.88it/s]\u001b[A\n",
      " 68%|██████▊   | 338/500 [00:23<00:13, 11.86it/s]\u001b[A\n",
      " 68%|██████▊   | 340/500 [00:23<00:13, 11.97it/s]\u001b[A\n",
      " 68%|██████▊   | 342/500 [00:23<00:13, 11.86it/s]\u001b[A\n",
      " 69%|██████▉   | 344/500 [00:23<00:13, 11.93it/s]\u001b[A\n",
      " 69%|██████▉   | 346/500 [00:23<00:12, 12.05it/s]\u001b[A\n",
      " 70%|██████▉   | 348/500 [00:24<00:12, 12.09it/s]\u001b[A\n",
      " 70%|███████   | 350/500 [00:24<00:12, 11.97it/s]\u001b[A\n",
      " 70%|███████   | 352/500 [00:24<00:12, 11.78it/s]\u001b[A\n",
      " 71%|███████   | 354/500 [00:24<00:12, 11.77it/s]\u001b[A\n",
      " 71%|███████   | 356/500 [00:24<00:12, 11.66it/s]\u001b[A\n",
      " 72%|███████▏  | 358/500 [00:24<00:12, 11.34it/s]\u001b[A\n",
      " 72%|███████▏  | 360/500 [00:25<00:12, 11.52it/s]\u001b[A\n",
      " 72%|███████▏  | 362/500 [00:25<00:12, 11.41it/s]\u001b[A\n",
      " 73%|███████▎  | 364/500 [00:25<00:12, 11.23it/s]\u001b[A\n",
      " 73%|███████▎  | 366/500 [00:25<00:11, 11.29it/s]\u001b[A\n",
      " 74%|███████▎  | 368/500 [00:25<00:11, 11.31it/s]\u001b[A\n",
      " 74%|███████▍  | 370/500 [00:25<00:11, 11.22it/s]\u001b[A\n",
      " 74%|███████▍  | 372/500 [00:26<00:11, 11.08it/s]\u001b[A\n",
      " 75%|███████▍  | 374/500 [00:26<00:11, 11.07it/s]\u001b[A\n",
      " 75%|███████▌  | 376/500 [00:26<00:10, 11.28it/s]\u001b[A\n",
      " 76%|███████▌  | 378/500 [00:26<00:10, 11.16it/s]\u001b[A\n",
      " 76%|███████▌  | 380/500 [00:26<00:10, 11.07it/s]\u001b[A\n",
      " 76%|███████▋  | 382/500 [00:27<00:10, 10.90it/s]\u001b[A\n",
      " 77%|███████▋  | 384/500 [00:27<00:10, 11.02it/s]\u001b[A\n",
      " 77%|███████▋  | 386/500 [00:27<00:10, 11.19it/s]\u001b[A\n",
      " 78%|███████▊  | 388/500 [00:27<00:09, 11.33it/s]\u001b[A\n",
      " 78%|███████▊  | 390/500 [00:27<00:09, 11.16it/s]\u001b[A\n",
      " 78%|███████▊  | 392/500 [00:27<00:10, 10.58it/s]\u001b[A\n",
      " 79%|███████▉  | 394/500 [00:28<00:10, 10.23it/s]\u001b[A\n",
      " 79%|███████▉  | 396/500 [00:28<00:10, 10.29it/s]\u001b[A\n",
      " 80%|███████▉  | 398/500 [00:28<00:09, 10.26it/s]\u001b[A\n",
      " 80%|████████  | 400/500 [00:29<00:16,  5.88it/s]\u001b[A\n",
      " 80%|████████  | 401/500 [00:29<00:15,  6.59it/s]\u001b[A\n",
      " 80%|████████  | 402/500 [00:29<00:13,  7.31it/s]\u001b[A\n",
      " 81%|████████  | 403/500 [00:29<00:12,  7.59it/s]\u001b[A\n",
      " 81%|████████  | 405/500 [00:29<00:11,  8.29it/s]\u001b[A\n",
      " 81%|████████▏ | 407/500 [00:30<00:10,  8.59it/s]\u001b[A\n",
      " 82%|████████▏ | 408/500 [00:30<00:10,  8.44it/s]\u001b[A\n",
      " 82%|████████▏ | 409/500 [00:30<00:11,  7.97it/s]\u001b[A\n",
      " 82%|████████▏ | 410/500 [00:30<00:11,  7.89it/s]\u001b[A\n",
      " 82%|████████▏ | 411/500 [00:30<00:10,  8.34it/s]\u001b[A\n",
      " 82%|████████▏ | 412/500 [00:30<00:13,  6.61it/s]\u001b[A\n",
      " 83%|████████▎ | 413/500 [00:30<00:12,  7.19it/s]\u001b[A\n",
      " 83%|████████▎ | 414/500 [00:30<00:11,  7.81it/s]\u001b[A\n",
      " 83%|████████▎ | 415/500 [00:31<00:11,  7.10it/s]\u001b[A\n",
      " 83%|████████▎ | 417/500 [00:31<00:11,  7.44it/s]\u001b[A\n",
      " 84%|████████▎ | 418/500 [00:31<00:10,  7.94it/s]\u001b[A\n",
      " 84%|████████▍ | 419/500 [00:31<00:09,  8.40it/s]\u001b[A\n",
      " 84%|████████▍ | 420/500 [00:31<00:13,  6.06it/s]\u001b[A\n",
      " 84%|████████▍ | 421/500 [00:31<00:11,  6.86it/s]\u001b[A\n",
      " 84%|████████▍ | 422/500 [00:32<00:10,  7.39it/s]\u001b[A\n",
      " 85%|████████▍ | 423/500 [00:32<00:12,  6.34it/s]\u001b[A\n",
      " 85%|████████▍ | 424/500 [00:32<00:13,  5.59it/s]\u001b[A\n",
      " 85%|████████▌ | 425/500 [00:32<00:14,  5.08it/s]\u001b[A\n",
      " 85%|████████▌ | 426/500 [00:32<00:13,  5.61it/s]\u001b[A\n",
      " 86%|████████▌ | 428/500 [00:33<00:10,  6.56it/s]\u001b[A\n",
      " 86%|████████▌ | 429/500 [00:33<00:09,  7.26it/s]\u001b[A\n",
      " 86%|████████▌ | 430/500 [00:33<00:09,  7.75it/s]\u001b[A\n",
      " 86%|████████▌ | 431/500 [00:33<00:09,  7.46it/s]\u001b[A\n",
      " 86%|████████▋ | 432/500 [00:33<00:08,  7.99it/s]\u001b[A\n",
      " 87%|████████▋ | 433/500 [00:33<00:08,  8.17it/s]\u001b[A\n",
      " 87%|████████▋ | 434/500 [00:33<00:07,  8.37it/s]\u001b[A\n",
      " 87%|████████▋ | 435/500 [00:33<00:10,  6.34it/s]\u001b[A\n",
      " 87%|████████▋ | 436/500 [00:34<00:10,  6.36it/s]\u001b[A\n",
      " 87%|████████▋ | 437/500 [00:34<00:10,  5.95it/s]\u001b[A\n",
      " 88%|████████▊ | 439/500 [00:34<00:08,  6.85it/s]\u001b[A\n",
      " 88%|████████▊ | 440/500 [00:34<00:08,  7.22it/s]\u001b[A\n",
      " 88%|████████▊ | 442/500 [00:35<00:09,  5.84it/s]\u001b[A\n",
      " 89%|████████▊ | 443/500 [00:35<00:08,  6.61it/s]\u001b[A\n",
      " 89%|████████▉ | 444/500 [00:35<00:07,  7.28it/s]\u001b[A\n",
      " 89%|████████▉ | 445/500 [00:35<00:07,  7.73it/s]\u001b[A\n",
      " 89%|████████▉ | 446/500 [00:35<00:06,  8.07it/s]\u001b[A\n",
      " 89%|████████▉ | 447/500 [00:35<00:06,  8.38it/s]\u001b[A\n",
      " 90%|████████▉ | 448/500 [00:35<00:06,  8.06it/s]\u001b[A\n",
      " 90%|████████▉ | 449/500 [00:35<00:06,  7.62it/s]\u001b[A\n",
      " 90%|█████████ | 450/500 [00:36<00:06,  8.08it/s]\u001b[A\n",
      " 90%|█████████ | 451/500 [00:36<00:08,  6.05it/s]\u001b[A\n",
      " 90%|█████████ | 452/500 [00:36<00:07,  6.80it/s]\u001b[A\n",
      " 91%|█████████ | 453/500 [00:36<00:06,  7.50it/s]\u001b[A\n",
      " 91%|█████████ | 454/500 [00:36<00:05,  7.82it/s]\u001b[A\n",
      " 91%|█████████ | 455/500 [00:36<00:06,  7.41it/s]\u001b[A\n",
      " 91%|█████████ | 456/500 [00:36<00:06,  6.71it/s]\u001b[A\n",
      " 91%|█████████▏| 457/500 [00:37<00:06,  6.99it/s]\u001b[A\n",
      " 92%|█████████▏| 458/500 [00:37<00:06,  6.76it/s]\u001b[A\n",
      " 92%|█████████▏| 459/500 [00:37<00:07,  5.78it/s]\u001b[A\n",
      " 92%|█████████▏| 460/500 [00:37<00:06,  6.28it/s]\u001b[A\n",
      " 92%|█████████▏| 461/500 [00:37<00:05,  6.86it/s]\u001b[A\n",
      " 92%|█████████▏| 462/500 [00:37<00:05,  6.77it/s]\u001b[A\n",
      " 93%|█████████▎| 463/500 [00:38<00:05,  7.10it/s]\u001b[A\n",
      " 93%|█████████▎| 464/500 [00:38<00:05,  6.67it/s]\u001b[A\n",
      " 93%|█████████▎| 465/500 [00:38<00:05,  6.52it/s]\u001b[A\n",
      " 93%|█████████▎| 466/500 [00:38<00:06,  5.10it/s]\u001b[A\n",
      " 93%|█████████▎| 467/500 [00:38<00:05,  5.95it/s]\u001b[A\n",
      " 94%|█████████▎| 468/500 [00:38<00:04,  6.54it/s]\u001b[A\n",
      " 94%|█████████▍| 469/500 [00:39<00:04,  6.69it/s]\u001b[A\n",
      " 94%|█████████▍| 470/500 [00:39<00:04,  6.77it/s]\u001b[A\n",
      " 94%|█████████▍| 471/500 [00:39<00:05,  5.47it/s]\u001b[A\n",
      " 94%|█████████▍| 472/500 [00:39<00:04,  6.21it/s]\u001b[A\n",
      " 95%|█████████▍| 473/500 [00:39<00:04,  6.63it/s]\u001b[A\n",
      " 95%|█████████▍| 474/500 [00:39<00:03,  6.78it/s]\u001b[A\n",
      " 95%|█████████▌| 475/500 [00:39<00:03,  6.32it/s]\u001b[A\n",
      " 95%|█████████▌| 476/500 [00:40<00:03,  6.39it/s]\u001b[A\n",
      " 95%|█████████▌| 477/500 [00:40<00:03,  6.75it/s]\u001b[A\n",
      " 96%|█████████▌| 478/500 [00:40<00:03,  6.84it/s]\u001b[A\n",
      " 96%|█████████▌| 479/500 [00:40<00:03,  6.06it/s]\u001b[A\n",
      " 96%|█████████▌| 480/500 [00:40<00:03,  6.55it/s]\u001b[A\n",
      " 96%|█████████▌| 481/500 [00:40<00:02,  6.68it/s]\u001b[A\n",
      " 96%|█████████▋| 482/500 [00:41<00:02,  6.28it/s]\u001b[A\n",
      " 97%|█████████▋| 483/500 [00:41<00:02,  6.13it/s]\u001b[A\n",
      " 97%|█████████▋| 484/500 [00:41<00:02,  6.00it/s]\u001b[A\n",
      " 97%|█████████▋| 485/500 [00:41<00:02,  5.73it/s]\u001b[A\n",
      " 97%|█████████▋| 486/500 [00:41<00:03,  4.67it/s]\u001b[A\n",
      " 97%|█████████▋| 487/500 [00:42<00:02,  5.48it/s]\u001b[A\n",
      " 98%|█████████▊| 488/500 [00:42<00:02,  5.91it/s]\u001b[A\n",
      " 98%|█████████▊| 489/500 [00:42<00:02,  5.41it/s]\u001b[A\n",
      " 98%|█████████▊| 490/500 [00:42<00:01,  6.00it/s]\u001b[A\n",
      " 98%|█████████▊| 491/500 [00:42<00:02,  4.45it/s]\u001b[A\n",
      " 98%|█████████▊| 492/500 [00:42<00:01,  5.26it/s]\u001b[A\n",
      " 99%|█████████▊| 493/500 [00:43<00:01,  5.64it/s]\u001b[A\n",
      " 99%|█████████▉| 494/500 [00:43<00:01,  5.13it/s]\u001b[A\n",
      " 99%|█████████▉| 495/500 [00:43<00:00,  5.51it/s]\u001b[A\n",
      " 99%|█████████▉| 496/500 [00:43<00:01,  3.71it/s]\u001b[A\n",
      " 99%|█████████▉| 497/500 [00:44<00:00,  4.50it/s]\u001b[A\n",
      "100%|█████████▉| 498/500 [00:44<00:00,  5.18it/s]\u001b[A\n",
      "100%|█████████▉| 499/500 [00:44<00:00,  5.29it/s]\u001b[A\n",
      "100%|██████████| 500/500 [00:44<00:00,  4.41it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Total number of patterns': 500, 'Number of patterns above maximum length': 22, 'Number of patterns below minimum length': 0}\n"
     ]
    }
   ],
   "source": [
    "store_patts = {i:[] for i in classes}\n",
    "model.batch_size = 1\n",
    "report_filter = {'Total number of patterns': 0,\n",
    "                 'Number of patterns above maximum length': 0,\n",
    "                 'Number of patterns below minimum length': 0}\n",
    "pbar = tqdm(total=len(selected_trajectories))\n",
    "for id_trajectory in selected_trajectories:\n",
    "    series_numpy = np.array(df.loc[df['ID'] == id_trajectory][meas_var]).astype('float').squeeze()\n",
    "    # Row: measurement; Col: time\n",
    "    if len(meas_var) >= 2:\n",
    "        series_numpy = series_numpy.transpose()\n",
    "    series_tensor = torch.tensor(series_numpy)\n",
    "    class_trajectory = df.loc[df['ID']==id_trajectory]['Class'].iloc[0]  # repeated value through all series\n",
    "    class_label = classes[class_trajectory]\n",
    "    cam = create_cam(model, array_series=series_tensor, feature_layer='features',\n",
    "                         device=device, clip=0, target_class=class_trajectory)\n",
    "    thresh = threshold_li(cam)\n",
    "    bincam = np.where(cam >= thresh, 1, 0)\n",
    "    bincam_ext = extend_segments(array=bincam, max_ext=extend_patt)\n",
    "    patterns = longest_segments(array=bincam_ext, k=n_pattern_perseries)\n",
    "    # Filter short/long patterns\n",
    "    report_filter['Total number of patterns'] += len(patterns)\n",
    "    report_filter['Number of patterns above maximum length'] += len([k for k in patterns.keys() if patterns[k] > max_len_patt])\n",
    "    report_filter['Number of patterns below minimum length'] += len([k for k in patterns.keys() if patterns[k] < min_len_patt])\n",
    "    patterns = {k: patterns[k] for k in patterns.keys() if (patterns[k] >= min_len_patt and\n",
    "                                                            patterns[k] <= max_len_patt)}\n",
    "    if len(patterns) > 0:\n",
    "        for pattern_position in list(patterns.keys()):\n",
    "            store_patts[class_label].append(extract_pattern(series_numpy, pattern_position, NA_fill=False))\n",
    "    pbar.update(1)\n",
    "\n",
    "print(report_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump patterns into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if export_allPooled:\n",
    "    concat_patts_allPooled = np.full((sum(map(len, store_patts.values())), len(meas_var) * max_len_patt), np.nan)\n",
    "    irow = 0\n",
    "for classe in classes:\n",
    "    concat_patts = np.full((len(store_patts[classe]), len(meas_var) * max_len_patt), np.nan)\n",
    "    for i, patt in enumerate(store_patts[classe]):\n",
    "        if len(meas_var) == 1:\n",
    "            len_patt = len(patt)\n",
    "            concat_patts[i, 0:len_patt] = patt\n",
    "        if len(meas_var) >= 2:\n",
    "            len_patt = patt.shape[1]\n",
    "            for j in range(len(meas_var)):\n",
    "                offset = j*max_len_patt\n",
    "                concat_patts[i, (0+offset):(len_patt+offset)] = patt[j, :]\n",
    "    if len(meas_var) == 1:\n",
    "        headers = ','.join([meas_var[0] + '_' + str(k) for k in range(max_len_patt)])\n",
    "        fout_patt = '/home/marc/Dropbox/Work/TSclass_GF/Notebooks/output/' + meas_var[0] +'/local_patterns/patt_uncorr_{}.csv.gz'.format(classe)\n",
    "        if export_perClass:\n",
    "            np.savetxt(fout_patt, concat_patts,\n",
    "                       delimiter=',', header=headers, comments='')\n",
    "    elif len(meas_var) >= 2:\n",
    "        headers = ','.join([meas + '_' + str(k) for meas in meas_var for k in range(max_len_patt)])\n",
    "        fout_patt = '/home/marc/Dropbox/Work/TSclass_GF/Notebooks/output/' + '_'.join(meas_var) +'/local_patterns/patt_uncorr_{}.csv.gz'.format(classe)\n",
    "        if export_perClass:\n",
    "            np.savetxt(fout_patt, concat_patts,\n",
    "                       delimiter=',', header=headers, comments='')\n",
    "    if export_allPooled:\n",
    "        concat_patts_allPooled[irow:(irow+concat_patts.shape[0]), :] = concat_patts\n",
    "        irow += concat_patts.shape[0]\n",
    "\n",
    "if export_allPooled:\n",
    "    concat_patts_allPooled = pd.DataFrame(concat_patts_allPooled)\n",
    "    concat_patts_allPooled.columns = headers.split(',')\n",
    "    pattID_col = [[classe] * len(store_patts[classe]) for classe in classes]\n",
    "    concat_patts_allPooled['pattID'] = [j+'_'+str(i) for i,j in enumerate(list(chain.from_iterable(pattID_col)))]\n",
    "    concat_patts_allPooled.set_index('pattID', inplace = True)\n",
    "    fout_patt = '/home/marc/Dropbox/Work/TSclass_GF/Notebooks/output/' + '_'.join(meas_var) + '/local_patterns/patt_uncorr_allPooled.csv.gz'.format(classe)\n",
    "    concat_patts_allPooled.to_csv(fout_patt, header=True, index=True, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build distance matrix between patterns with DTW\n",
    "\n",
    "This is done in R with the implementation of the *parallelDist* package. It is very efficient and has support for multivariate cases.\n",
    "\n",
    "Check next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
