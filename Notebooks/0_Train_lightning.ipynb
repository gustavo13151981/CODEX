{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training notebook\n",
    "\n",
    "This notebook will take you through all the steps to train a CNN classifier, the first step in a CODEX analysis. Follow the cells in order, modify them where needed and run them all. Hints and defaults values are suggested. Follow the training curves with Tensorboard to check whether you are satisfied with the model (good classification performance, moderated overfitting). You will probably have to give it a couple of runs before being fully satisfied. Keep in mind that the training will be orders of magnitude faster on GPU compared to CPU. If Pytorch and CUDA were properly set, the notebook will run on GPU without additional input from you.\n",
    "\n",
    "## Import libraries\n",
    "\n",
    "Make sure to import model class, dataset class and the required preprocessing operations (RandomCrop, ToTensor, Subtract...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import datetime\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "import os\n",
    "import zipfile\n",
    "import time\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "# Custom functions/classes\n",
    "path_to_module = '../source'  # Path where all the .py files are, relative to the notebook folder\n",
    "sys.path.append(path_to_module)\n",
    "from load_data import DataProcesser\n",
    "from train_utils import accuracy, AverageMeter\n",
    "from models import ConvNetCam, ConvNetCamBi\n",
    "from models_lightning import LitConvNetCam, LitConvNetCamBi\n",
    "from class_dataset import myDataset, ToTensor, Subtract, RandomShift, RandomNoise, RandomCrop, FixedCrop\n",
    "\n",
    "# For reproducibility\n",
    "myseed = 7\n",
    "pl.utilities.seed.seed_everything(myseed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define model architecture and set hyperparameters for training\n",
    "\n",
    "Models are defined in the file `models.py`. There are already 2 models defined in this file: ConvNetCam and ConvNetCamBi. Both are variations of the CNN architecture, the former is for univariate input, the second for bivariate input.\n",
    "\n",
    "Hyperparameters for training:\n",
    "- nepochs: int, number of training epochs.\n",
    "- batch_size: int, number of samples per batch.\n",
    "- lr: float, initial learning rate. This is the most important parameter to tweak to have a smooth learning curve. 1e-2 is usually a good starting value.\n",
    "- L2_reg: float, L2 regularization factor. This helps to prevent overfitting by penalizing large weights in the model parameters. In general try to always have a mild regularization, say 1e-3. Increase if you face overfitting issues.\n",
    "\n",
    "By default, the learning rate is scheduled to decrease by a factor gamma at fixed epochs.\n",
    "- lr_decrease_schedule: list, epochs at which to decrease.\n",
    "- lr_decrease_factor: float, factor by which the lr is multiplied at the specified epochs.\n",
    "\n",
    "Hyperparameters to set the model dimensions:\n",
    "- length: int, length of a trajectory. This is the input length as the model will expect it. Setting it to a smaller value than the actual length of the trajectories can be used for data augmentation (see RandomCrop).\n",
    "- nclass: int, number of output classes.\n",
    "- nfeatures: int, size of the input representation before the output layer. This also corresponds to the number of filters in the last convolution layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Dimensions of the model\n",
    "length = 220\n",
    "nclass = 13\n",
    "nfeatures = 10\n",
    "\n",
    "# Optimization parameters\n",
    "nepochs = 300\n",
    "batch_size = 60\n",
    "lr = 1e-2\n",
    "# By default, use a lr scheduler which will decrease the lr at fixed epochs by a factor\n",
    "lr_decrease_schedule = [75, 150, 250]\n",
    "lr_decrease_factor = 0.1\n",
    "L2_reg = 1e-3\n",
    "\n",
    "# Model definition\n",
    "#model = LitConvNetCam(batch_size=batch_size, lr_scheduler_milestones=lr_decrease_schedule, lr_gamma=lr_decrease_factor, nclass=nclass, length=length, nfeatures=nfeatures, lr=lr, L2_reg=L2_reg)  # for univariate input\n",
    "\n",
    "model = LitConvNetCamBi(batch_size=batch_size, lr_scheduler_milestones=lr_decrease_schedule, lr_gamma=lr_decrease_factor, nclass=nclass, length=length, nfeatures=nfeatures, lr=lr, L2_reg=L2_reg)  # for univariate input\n",
    "\n",
    "ngpu = -1  # number of GPUs to use, -1 means use all, 0 means use CPU\n",
    "\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load and process data, Data augmentation\n",
    "\n",
    "Define which data to load and whether/how to preprocess the batch. \n",
    "- data_file: str, path to a .zip that can be loaded as a DataProcesser. The zip archive must contain 3 files: one for the\n",
    " data (dataset.csv), one for the split train/validation/test (id_set.csv), one with the classes informations (classes.csv). See DataProcesser.read_archive() for format details.\n",
    "- meas_var: list of str or None, names of the measurement variables. In DataProcesser convention, this is the prefix in a\n",
    " column name that contains a measurement (time being the suffix). Pay attention to the order since this is how the dimensions of a sample of data will be ordered (i.e. 1st in the list will form 1st row of measurements in the sample, 2nd is the 2nd, etc...) If None, DataProcesser will extract automatically the measurement names and use the order of appearance in the column names.\n",
    "- start_time/end_time: int or None, use to subset data to a specific time range. If None, DataProcesser will extract automatically the range of time in the dataset columns. Useful to completely exclude some acquisition times where irrelevant measures are acquired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# data_file = '../sample_data/Synthetic_Univariate.zip'\n",
    "# data_file = '../sample_data/GrowthFactor_ErkAkt_Bivariate.zip'\n",
    "data_file = '/home/marc/Dropbox/ERK_p21_Albeck/data_forCNN/erk_p21_EtopEGF_noInsulin.zip'\n",
    "\n",
    "meas_var = None\n",
    "start_time = None\n",
    "end_time = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "data = DataProcesser(data_file)\n",
    "\n",
    "# Select measurements and times, subset classes and split the dataset\n",
    "meas_var = data.detect_groups_times()['groups'] if meas_var is None else meas_var\n",
    "start_time = data.detect_groups_times()['times'][0] if start_time is None else start_time\n",
    "end_time = data.detect_groups_times()['times'][1] if end_time is None else end_time\n",
    "data.subset(sel_groups=meas_var, start_time=start_time, end_time=end_time)\n",
    "data.get_stats()\n",
    "data.split_sets()\n",
    "\n",
    "# Input preprocessing, this is done sequentially, on the fly when the input is passed to the network\n",
    "average_perChannel = [data.stats['mu'][meas]['train'] for meas in meas_var]\n",
    "ls_transforms = transforms.Compose([\n",
    "    RandomCrop(output_size=length, ignore_na_tails=True),\n",
    "    Subtract(average_perChannel),\n",
    "    ToTensor()])\n",
    "\n",
    "# Define the dataset objects that associate data to preprocessing and define the content of a batch\n",
    "# A batch of myDataset contains: the trajectories, the trajectories identifier and the trajecotires class identifier\n",
    "data_train = myDataset(dataset=data.train_set, transform=ls_transforms)\n",
    "data_validation = myDataset(dataset=data.validation_set, transform=ls_transforms)\n",
    "\n",
    "if batch_size > len(data_train) or batch_size > len(data_validation):\n",
    "    raise ValueError('Batch size ({}) must be smaller than the number of trajectories in the training ({}) and the validation ({}) sets.'.format(batch_size, len(data_train), len(data_validation)))\n",
    "\n",
    "# Quick recap of the data content\n",
    "print('Channels order: {} \\nTime range: ({}, {}) \\nClasses: {}'.format(meas_var, start_time, end_time, list(data.dataset[data.col_class].unique())))\n",
    "nclass_data = len(list(data.dataset[data.col_class].unique()))\n",
    "if nclass != nclass_data:\n",
    "    warnings.warn('The number of classes in the model output ({}) is not equal to the number of classes in the data ({}).'.format(nclass, nclass_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some trajectories to check that the data loading and preprocessing is properly done. The curves appear here as they will be presented to the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_smpl = 6\n",
    "indx_smpl = np.random.randint(0, len(data_train), n_smpl)\n",
    "\n",
    "col_ids = []\n",
    "col_lab = []\n",
    "col_mes = []\n",
    "# Long format for seaborn grid, for loop to avoid multiple indexing\n",
    "# This would triggers preprocessing multiple times and add randomness\n",
    "for i in indx_smpl:\n",
    "    smpl = data_train[i]\n",
    "    col_ids.append(smpl['identifier'])\n",
    "    col_lab.append(smpl['label'].item())\n",
    "    col_mes.append(smpl['series'].numpy().transpose())\n",
    "col_ids = pd.Series(np.hstack(np.repeat(col_ids, length)))\n",
    "col_lab = pd.Series(np.hstack(np.repeat(col_lab, length)))\n",
    "col_mes = pd.DataFrame(np.vstack(col_mes), columns=meas_var)\n",
    "col_tim = pd.Series(np.tile(np.arange(0, length), n_smpl))\n",
    "\n",
    "df_smpl = pd.concat([col_ids, col_lab, col_tim, col_mes], axis=1)\n",
    "df_smpl.rename(columns={0: 'identifier', 1: 'label', 2:'time'}, inplace=True)\n",
    "df_smpl = df_smpl.melt(id_vars=['identifier', 'label', 'time'], value_vars=meas_var)\n",
    "\n",
    "sns.set_style('white')\n",
    "sns.set_context('notebook')\n",
    "grid = sns.FacetGrid(data=df_smpl, col='identifier', col_wrap=3, sharex=True)\n",
    "grid.map_dataframe(sns.lineplot, x='time', y='value', hue='variable')\n",
    "grid.set(xlabel='Time', ylabel='Measurement Value')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Tensorboard logs and model save file\n",
    "\n",
    "Define where the directory where training logs will be saved and the file where the model will be saved. By default, creates two subdirectories in the working directory: models and logs. An unique name for the logs and the model is created by appending the measurement name to the current timestamp.\n",
    "\n",
    "The model training can be monitored in real-time in Tensorboard with these logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "file_data = os.path.splitext(os.path.basename(data_file))[0]  # file name without extension\n",
    "timestamp = datetime.datetime.now().strftime('%Y-%m-%d-%H__%M__%S')\n",
    "\n",
    "dir_logs = 'logs/'\n",
    "subdir_logs = '_'.join(meas_var)\n",
    "file_logs =  '_'.join([timestamp, file_data])\n",
    "dir_model = 'models/' + '_'.join(meas_var)\n",
    "file_model = '_'.join([timestamp, file_data])\n",
    "\n",
    "if not os.path.exists(dir_model):\n",
    "    os.makedirs(dir_model)\n",
    "if not os.path.exists(dir_logs):\n",
    "    os.makedirs(dir_logs)\n",
    "\n",
    "tb_logger = pl.loggers.TensorBoardLogger(save_dir=dir_logs, name=subdir_logs, version=file_logs, default_hp_metric=False)\n",
    "#chckpt_callback = ModelCheckpoint(dirpath=dir_model, filename=file_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train the model\n",
    "\n",
    "Can follow the training in tensorboard with:\n",
    "```\n",
    "tensorboard --logdir \"path/to/logs\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=data_train,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4,\n",
    "                          drop_last=True)\n",
    "\n",
    "validation_loader = DataLoader(dataset=data_validation,\n",
    "                               batch_size=batch_size,\n",
    "                               shuffle=False,\n",
    "                               num_workers=4,\n",
    "                               drop_last=True)\n",
    "\n",
    "\n",
    "#from torchsampler import ImbalancedDatasetSampler\n",
    "\n",
    "#def custom_get_label(dataset, idx):\n",
    "#    #callback function used in imbalanced dataset loader.\n",
    "#    return int(dataset[idx]['label'])\n",
    "\n",
    "#train_loader = DataLoader(dataset=data_train,\n",
    "#                          batch_size=batch_size,\n",
    "#                          sampler=ImbalancedDatasetSampler(data_train, callback_get_label=custom_get_label),\n",
    "#                          num_workers=4,\n",
    "#                          drop_last=True)\n",
    "\n",
    "\n",
    "t0 = time.time()\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "trainer = pl.Trainer(callbacks=[\n",
    "     lr_monitor,\n",
    "     #chckpt_callback\n",
    "     ],\n",
    " max_epochs=nepochs,\n",
    "  min_epochs=nepochs,\n",
    "   gpus=ngpu,\n",
    "    log_every_n_steps=1,\n",
    "     logger=tb_logger)\n",
    "trainer.fit(model, train_loader, validation_loader)\n",
    "t1 = time.time()\n",
    "\n",
    "print('Elapsed time: {:.2f} min'.format((t1 - t0)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next\n",
    "\n",
    "- Make sure that you are satisfied with the model. A quick way to do this is to check the Tensorboard logs. Check in priority if the classification performance are good enough (top1/top2 indicators) and that there is no serious problems of overfitting (difference between the training and validation sets should remain reasonably small).\n",
    "- When possible, try to play around with the parameters before deciding on a given model. In priority, try to tweak the initial learning rate by a few orders of magnitude and the learning rate scheduler to see if you can get a smoothly converging training curve. If your model is overfitting try to reduce the number of features or to increase the L2 regularization strength.\n",
    "- When you are satisfied with the model, you can go on the next notebooks to identify prototype trajectories and to identify class-discriminative motifs with CAMs.\n",
    "- Alternatively, you can use the companion app to browse an interactive projection of the CNN features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Details and tips to setup loss and optimizer\n",
    "\n",
    "The loss (called criterion in pytorch convention) against which the model is trained is defined here. For regular classification tasks use the CrossEntropyLoss. All available options are listed here: https://pytorch.org/docs/1.1.0/nn.html#loss-functions\n",
    "\n",
    "The optimizer defines which algorithm is used to update the weights of the model through the training. L2 regularization is controlled by the \"weight_decay\" argument in the optimizer object. Adam optimizer with default parameters is a good default. All available options are listed here: https://pytorch.org/docs/1.1.0/optim.html#algorithms\n",
    "\n",
    "Scheduler defines how the learning rate should evolve through the training. In order to get a smooth training and better classification performance, it needs to be reduced after some epochs. One of the easiest scheduler is the MultiStepLR which reduces the learning rate by a factor gamma when milestone epochs are reached. To start, just try to divide the learning rate by a factor 0.1, 3-4 times through the training. All available options are listed here: https://pytorch.org/docs/1.1.0/optim.html#how-to-adjust-learning-rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}