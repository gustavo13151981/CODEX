{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training notebook\n",
    "\n",
    "This notebook will take you through all the steps to train a CNN classifier, the first step in a CODEX analysis. Follow the cells in order, modify them where needed and run them all. Hints and defaults values are suggested. Follow the training curves with Tensorboard to check whether you are satisfied with the model (good classification performance, moderated overfitting). You will probably have to give it a couple of runs before being fully satisfied. Keep in mind that the training will be orders of magnitude faster on GPU compared to CPU. If Pytorch and CUDA were properly set, the notebook will run on GPU without additional input from you.\n",
    "\n",
    "## Import libraries\n",
    "\n",
    "Make sure to import model class, dataset class and the required preprocessing operations (RandomCrop, ToTensor, Subtract...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import datetime\n",
    "from tensorboardX import SummaryWriter\n",
    "import os\n",
    "import zipfile\n",
    "import time\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "# Custom functions/classes\n",
    "path_to_module = '..'  # Path where all the .py files are, relative to the notebook folder\n",
    "sys.path.append(path_to_module)\n",
    "from load_data import DataProcesser\n",
    "from train_utils import accuracy, AverageMeter\n",
    "from models import ConvNetCam, ConvNetCamBi\n",
    "from class_dataset import myDataset, ToTensor, Subtract, RandomShift, RandomNoise, RandomCrop, FixedCrop\n",
    "\n",
    "# For reproducibility\n",
    "myseed = 7\n",
    "torch.manual_seed(myseed)\n",
    "torch.cuda.manual_seed(myseed)\n",
    "np.random.seed(myseed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Hyperparameters for training\n",
    "\n",
    "Hyperparameters for training:\n",
    "- nepochs: int, number of training epochs.\n",
    "- batch_size: int, number of samples per batch.\n",
    "- lr: float, initial learning rate. This is the most important parameter to tweak to have a smooth learning curve. 1e-2 is usually a good starting value.\n",
    "- L2_reg: float, L2 regularization factor. This helps to prevent overfitting by penalizing large weights in the model parameters. In general try to always have a mild regularization, say 1e-3. Increase if you face overfitting issues.\n",
    "\n",
    "Hyperparameters to set the model dimensions:\n",
    "- length: int, length of a trajectory. This is the input length as the model will expect it. Setting it to a smaller value than the actual length of the trajectories can be used for data augmentation (see RandomCrop).\n",
    "- nclass: int, number of output classes.\n",
    "- nfeatures: int, size of the input representation before the output layer. This also corresponds to the number of filters in the last convolution layer.\n",
    "- selected_classes: list, select only some classes from input dataset. Leave an empty list to use all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nepochs = 50\n",
    "batch_size = 256\n",
    "lr = 1e-2\n",
    "L2_reg = 1e-3\n",
    "\n",
    "length = 750\n",
    "nclass = 2\n",
    "nfeatures = 10\n",
    "selected_classes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load and process data, Data augmentation\n",
    "\n",
    "Define which data to load and whether/how to preprocess the batch. \n",
    "- data_file: str, path to a .zip that can be loaded as a DataProcesser. The zip archive must contain 3 files: one for the\n",
    " data (dataset.csv), one for the split train/validation/test (id_set.csv), one with the classes informations (classes.csv). See DataProcesser.read_archive() for format details.\n",
    "- meas_var: list of str or None, names of the measurement variables. In DataProcesser convention, this is the prefix in a\n",
    " column name that contains a measurement (time being the suffix). Pay attention to the order since this is how the dimensions of a sample of data will be ordered (i.e. 1st in the list will form 1st row of measurements in the sample, 2nd is the 2nd, etc...) If None, DataProcesser will extract automatically the measurement names and use the order of appearance in the column names.\n",
    "- start_time/end_time: int or None, use to subset data to a specific time range. If None, DataProcesser will extract automatically the range of time in the dataset columns. Useful to completely exclude some acquisition times where irrelevant measures are acquired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_file = 'C:/Users/Marc/Dropbox/Work/TSclass_GF/data/SynUni.zip'\n",
    "\n",
    "meas_var = None\n",
    "start_time = None\n",
    "end_time = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "data = DataProcesser(data_file)\n",
    "\n",
    "# Select measurements and times, subset classes and split the dataset\n",
    "meas_var = data.detect_groups_times()['groups'] if meas_var is None else meas_var\n",
    "start_time = data.detect_groups_times()['times'][0] if start_time is None else start_time\n",
    "end_time = data.detect_groups_times()['times'][1] if end_time is None else end_time\n",
    "data.subset(sel_groups=meas_var, start_time=start_time, end_time=end_time)\n",
    "if selected_classes:\n",
    "    data.dataset = data.dataset[data.dataset[data.col_class].isin(selected_classes)]\n",
    "data.get_stats()\n",
    "data.split_sets()\n",
    "\n",
    "# Input preprocessing, this is done sequentially, on the fly when the input is passed to the network\n",
    "average_perChannel = [data.stats['mu'][meas]['train'] for meas in meas_var]\n",
    "ls_transforms = transforms.Compose([\n",
    "    RandomCrop(output_size=length, ignore_na_tails=True),\n",
    "    Subtract(average_perChannel),\n",
    "    ToTensor()])\n",
    "\n",
    "# Define the dataset objects that associate data to preprocessing and define the content of a batch\n",
    "# A batch of myDataset contains: the trajectories, the trajectories identifier and the trajecotires class identifier\n",
    "data_train = myDataset(dataset=data.train_set, transform=ls_transforms)\n",
    "data_test = myDataset(dataset=data.validation_set, transform=ls_transforms)\n",
    "\n",
    "# Quick recap of the data content\n",
    "print('Channels order: {} \\nTime range: ({}, {}) \\nClasses: {}'.format(meas_var, start_time, end_time, list(data.dataset[data.col_class].unique())))\n",
    "if nclass != len(list(data.dataset[data.col_class].unique())):\n",
    "    warnings.warn('The number of classes in the model output is not equal to the number of classes in the data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some trajectories to check that the data loading and preprocessing is properly done. The curves appear here as they will be presented to the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_smpl = 6\n",
    "indx_smpl = np.random.randint(0, len(data_train), n_smpl)\n",
    "\n",
    "col_ids = []\n",
    "col_lab = []\n",
    "col_mes = []\n",
    "# Long format for seaborn grid, for loop to avoid multiple indexing\n",
    "# This would triggers preprocessing multiple times and add randomness\n",
    "for i in indx_smpl:\n",
    "    smpl = data_train[i]\n",
    "    col_ids.append(smpl['identifier'])\n",
    "    col_lab.append(smpl['label'].item())\n",
    "    col_mes.append(smpl['series'].numpy().transpose())\n",
    "col_ids = pd.Series(np.hstack(np.repeat(col_ids, length)))\n",
    "col_lab = pd.Series(np.hstack(np.repeat(col_lab, length)))\n",
    "col_mes = pd.DataFrame(np.vstack(col_mes), columns=meas_var)\n",
    "col_tim = pd.Series(np.tile(np.arange(0, length), n_smpl))\n",
    "\n",
    "df_smpl = pd.concat([col_ids, col_lab, col_tim, col_mes], axis=1)\n",
    "df_smpl.rename(columns={0: 'identifier', 1: 'label', 2:'time'}, inplace=True)\n",
    "df_smpl = df_smpl.melt(id_vars=['identifier', 'label', 'time'], value_vars=meas_var)\n",
    "\n",
    "sns.set_style('white')\n",
    "sns.set_context('notebook')\n",
    "grid = sns.FacetGrid(data=df_smpl, col='identifier', col_wrap=3, sharex=True)\n",
    "grid.map_dataframe(sns.lineplot, x='time', y='value', hue='variable')\n",
    "grid.set(xlabel='Time', ylabel='Measurement Value')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Resume training or new model\n",
    "\n",
    "Set to None for a new model, otherwise provide the path to a saved model file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "load_model = None\n",
    "# load_model = 'path/to/file.pytorch'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Tensorboard logs and model save file\n",
    "\n",
    "Define where the directory where training logs will be saved and the file where the model will be saved. By default, creates two subdirectories in the working directory: models and logs. An unique name for the logs and the model is created by appending the measurement name to the current timestamp.\n",
    "\n",
    "The model training can be monitored in real-time in Tensorboard with these logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "file_logs = os.path.splitext(os.path.basename(data_file))[0]  # file name without extension\n",
    "logs_str = 'logs/' + '_'.join(meas_var) + '/' + datetime.datetime.now().strftime('%Y-%m-%d-%H__%M__%S') + \\\n",
    "           '_' + file_logs + '/'\n",
    "writer = SummaryWriter(logs_str)\n",
    "save_model = 'models/' + logs_str.lstrip('logs/').rstrip('/') + '.pytorch'\n",
    "\n",
    "if not os.path.exists(file_logs):\n",
    "    os.makedirs(file_logs)\n",
    "if not os.path.exists('models/' + '_'.join(meas_var)):\n",
    "    os.makedirs('models/' + '_'.join(meas_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup model, loss and optimizer\n",
    "\n",
    "Create the model object and gives it dimensions that fit the previous parameters (length of the input, size of the latent representation...).\n",
    "\n",
    "The loss (called criterion in pytorch convention) against which the model is trained is defined here. For regular classification tasks use the CrossEntropyLoss. All available options are listed here: https://pytorch.org/docs/1.1.0/nn.html#loss-functions\n",
    "\n",
    "The optimizer defines which algorithm is used to update the weights of the model through the training. L2 regularization is controlled by the \"weight_decay\" argument in the optimizer object. Adam optimizer with default parameters is a good default. All available options are listed here: https://pytorch.org/docs/1.1.0/optim.html#algorithms\n",
    "\n",
    "Scheduler defines how the learning rate should evolve through the training. In order to get a smooth training and better classification performance, it needs to be reduced after some epochs. One of the easiest scheduler is the MultiStepLR which reduces the learning rate by a factor gamma when milestone epochs are reached. To start, just try to divide the learning rate by a factor 0.1, 3-4 times through the training. All available options are listed here: https://pytorch.org/docs/1.1.0/optim.html#how-to-adjust-learning-rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = ConvNetCam(batch_size=batch_size, nclass=nclass, length=length, nfeatures=nfeatures)\n",
    "# model = ConvNetCamBi(batch_size=batch_size, nclass=nclass, length=length, nfeatures=nfeatures)  # for bivariate input\n",
    "if load_model:\n",
    "    model.load_state_dict(torch.load(load_model))\n",
    "model.double()\n",
    "cuda_available = torch.cuda.is_available()\n",
    "if cuda_available:\n",
    "    model = model.cuda()\n",
    "    \n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=L2_reg)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[15, 35, 500], gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training loop\n",
    "\n",
    "Training of the model, you probably won't have anything to change here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def TrainModel(model, optimizer, criterion, scheduler, train_loader, test_loader, nepochs,\n",
    "               save_model=save_model, logs=True):\n",
    "    # ------------------------------------------------------------------------------------------------------------------\n",
    "    # Model, loss, optimizer\n",
    "    top1 = AverageMeter()\n",
    "    top2 = AverageMeter()\n",
    "\n",
    "    if logs:\n",
    "        print('Train logs saved at: {}'.format(logs_str))\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------------------------------\n",
    "    # Get adequate size of sample for nn.Conv layers\n",
    "    # Add a dummy channel dimension for conv1D layer (if multivariate, treat as a 2D plane with 1 channel)\n",
    "    assert len(train_loader.dataset[0]['series'].shape) == 2\n",
    "    nchannel, univar_length = train_loader.dataset[0]['series'].shape\n",
    "    if nchannel == 1:\n",
    "        view_size = (batch_size, 1, univar_length)\n",
    "    elif nchannel >= 2:\n",
    "        view_size = (batch_size, 1, nchannel, univar_length)\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------------------------------\n",
    "    # Training loop\n",
    "    for epoch in range(nepochs):\n",
    "        model.train()\n",
    "        top1.reset()\n",
    "        top2.reset()\n",
    "\n",
    "        loss_train = []\n",
    "        for i_batch, sample_batch in enumerate(train_loader):\n",
    "            series, label = sample_batch['series'], sample_batch['label']\n",
    "            if cuda_available:\n",
    "                series, label = series.cuda(), label.cuda()\n",
    "            series = series.view(view_size)\n",
    "            prediction = model(series)\n",
    "            #################################################\n",
    "            # Todo: Check if always necessary\n",
    "            label = label.type(torch.LongTensor)\n",
    "            label = label.cuda()\n",
    "            #################################################\n",
    "            loss = criterion(prediction, label)\n",
    "            loss_train.append(loss.cpu().detach().numpy())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i_batch % 25 == 0:\n",
    "                print('Training epoch: [{0}/{4}][{1}/{2}]; Loss: {3}'.format(epoch + 1, i_batch + 1, len(train_loader),\n",
    "                                                                             loss, nepochs))\n",
    "\n",
    "            prec1, prec2 = accuracy(prediction, label, topk=(1, 2))\n",
    "            top1.update(prec1[0], series.size(0))\n",
    "            top2.update(prec2[0], series.size(0))\n",
    "\n",
    "            if i_batch % 100 == 0:\n",
    "                print('Training Accuracy Epoch: [{0}]\\t'\n",
    "                      'Prec@1 {top1.val.data:.3f} ({top1.avg.data:.3f})\\t'\n",
    "                      'Prec@2 {top2.val.data:.3f} ({top2.avg.data:.3f})'.format(\n",
    "                    epoch, top1=top1, top2=top2))\n",
    "            if logs:\n",
    "                writer.add_scalar('Train/Loss', loss, epoch * len(train_loader) + i_batch + 1)\n",
    "                writer.add_scalar('Train/Top1', top1.val, epoch * len(train_loader) + i_batch + 1)\n",
    "                writer.add_scalar('Train/Top2', top2.val, epoch * len(train_loader) + i_batch + 1)\n",
    "        if logs:\n",
    "            writer.add_scalar('MeanEpoch/Train_Loss', np.mean(loss_train), epoch)\n",
    "            writer.add_scalar('MeanEpoch/Train_Top1', top1.avg, epoch)\n",
    "            writer.add_scalar('MeanEpoch/Train_Top2', top2.avg, epoch)\n",
    "\n",
    "        # --------------------------------------------------------------------------------------------------------------\n",
    "        # Evaluation loop\n",
    "        model.eval()\n",
    "        top1.reset()\n",
    "        top2.reset()\n",
    "        loss_eval = []\n",
    "        for i_batch, sample_batch in enumerate(test_loader):\n",
    "            series, label = sample_batch['series'], sample_batch['label']\n",
    "            if cuda_available:\n",
    "                series, label = series.cuda(), label.cuda()\n",
    "            series = series.view(view_size)\n",
    "            #################################################\n",
    "            # Todo: Check if always necessary\n",
    "            label = label.type(torch.LongTensor)\n",
    "            label = label.cuda()\n",
    "            #################################################\n",
    "            label = torch.autograd.Variable(label)\n",
    "            \n",
    "            prediction = model(series)\n",
    "            loss = criterion(prediction, label)\n",
    "            loss_eval.append(loss.cpu().detach().numpy())\n",
    "\n",
    "            prec1, prec2 = accuracy(prediction, label, topk=(1, 2))\n",
    "            top1.update(prec1[0], series.size(0))\n",
    "            top2.update(prec2[0], series.size(0))\n",
    "\n",
    "        # For validation loss, report only after the whole batch is processed\n",
    "        if logs:\n",
    "            writer.add_scalar('Val/Loss', loss, epoch * len(train_loader) + i_batch + 1)\n",
    "            writer.add_scalar('Val/Top1', top1.val, epoch * len(train_loader) + i_batch + 1)\n",
    "            writer.add_scalar('Val/Top2', top2.val, epoch * len(train_loader) + i_batch + 1)\n",
    "            writer.add_scalar('MeanEpoch/Val_Loss', np.mean(loss_eval), epoch)\n",
    "            writer.add_scalar('MeanEpoch/Val_Top1', top1.avg, epoch)\n",
    "            writer.add_scalar('MeanEpoch/Val_Top2', top2.avg, epoch)\n",
    "\n",
    "\n",
    "        print('===>>>\\t'\n",
    "              'Prec@1 ({top1.avg.data:.3f})\\t'\n",
    "              'Prec@2 ({top2.avg.data:.3f})'.format(top1=top1, top2=top2))\n",
    "\n",
    "        writer.add_scalar('LearningRate', optimizer.param_groups[0]['lr'], epoch)\n",
    "        scheduler.step()\n",
    "        \n",
    "        \n",
    "    if save_model:\n",
    "        torch.save(model, save_model)\n",
    "        print('Model saved at: {}'.format(save_model))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train the model\n",
    "\n",
    "Can follow the training in tensorboard with:\n",
    "```\n",
    "tensorboard --logdir \"path/to/logs\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=data_train,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4,\n",
    "                          drop_last=True)\n",
    "test_loader = DataLoader(dataset=data_test,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True,\n",
    "                         num_workers=4,\n",
    "                         drop_last=True)\n",
    "\n",
    "t0 = time.time()\n",
    "mymodel = TrainModel(model, optimizer, criterion, scheduler, train_loader, test_loader, nepochs)\n",
    "t1 = time.time()\n",
    "\n",
    "print('Elapsed time: {:.2f} min'.format((t1 - t0)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next\n",
    "\n",
    "- Make sure that you are satisfied with the model. A quick way to do this is to check the Tensorboard logs. Check in priority if the classification performance are good enough (top1/top2 indicators) and that there is no serious problems of overfitting (difference between the training and validation sets should remain reasonably small).\n",
    "- When possible, try to play around with the parameters before deciding on a given model. In priority, try to tweak the initial learning rate by a few orders of magnitude and the learning rate scheduler to see if you can get a smoothly converging training curve. If your model is overfitting try to reduce the number of features or to increase the L2 regularization strength.\n",
    "- When you are satisfied with the model, you can go on the next notebooks to identify prototype trajectories and to identify class-discriminative motifs with CAMs.\n",
    "- Alternatively, you can use the companion app to browse an interactive projection of the CNN features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
