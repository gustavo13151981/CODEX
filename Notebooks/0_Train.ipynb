{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training notebook\n",
    "\n",
    "## Import libraries\n",
    "\n",
    "Make sure to import model class, dataset class and associated preprocessing operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import datetime\n",
    "from tensorboardX import SummaryWriter\n",
    "import os\n",
    "import zipfile\n",
    "import time\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Custom functions/classes\n",
    "from load_data import DataProcesser\n",
    "from train_utils import accuracy, AverageMeter\n",
    "from models import ConvNetCam, ConvNetCamBi\n",
    "from class_dataset import myDataset, ToTensor, Subtract, RandomShift, RandomNoise, RandomCrop, FixedCrop\n",
    "\n",
    "# For reproducibility\n",
    "myseed = 7\n",
    "torch.manual_seed(myseed)\n",
    "torch.cuda.manual_seed(myseed)\n",
    "np.random.seed(myseed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Hyperparameters for training\n",
    "\n",
    "Hyperparameters for training:\n",
    "- nepochs: int, number of training epochs.\n",
    "- batch_size: int.\n",
    "- lr: float, initial learning rate.\n",
    "- L2_reg: float, L2 regularization factor. This helps to prevent overfitting. In general try to always have a mild regularization, say 1e-3. Increase if you face overfitting issues.\n",
    "\n",
    "Hyperparameters to setup the model dimensions:\n",
    "- length: int, length of a trajectory. This is the length as the model will expect it. Setting it to smaller value than the actual length can be used for preprocessing/data jittering.\n",
    "- nclass: int, number of output classes.\n",
    "- nfeatures: int, size of the input representation before output layer. This also corresponds to the number of filters in the last convolution layer.\n",
    "- selected_classes: list, select only some classes from input dataset. Leave empty to use all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nepochs = 2000\n",
    "batch_size = 128\n",
    "lr = 1e-4\n",
    "L2_reg = 1e-1\n",
    "\n",
    "length = 28\n",
    "nclass = 12\n",
    "nfeatures = 5\n",
    "selected_classes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load and process data, Data augmentation\n",
    "\n",
    "Define which data to load and whether/how to preprocess the batch. \n",
    "- data_file: str, path to a .zip that can be loaded as a DataProcesser. The archive must contain 3 files: one for the\n",
    " data, one for the split train/validation, one with the classes informations. See DataProcesser.read_archive().\n",
    "- meas_var: list of str or None, names of the measurement variables. In DataProcesser convention, this is the prefix in a\n",
    " column name that contains a measurement (time being the suffix). Pay attention to the order since this is how the dimensions of a sample of data will be ordered (i.e. 1st in the list will form 1st row of measurements in the sample, 2nd is the 2nd, etc...) If None, DataProcesser will extract automatically the measurement names and use the order of appearance in the column names.\n",
    "- start_time/end_time: int or None, use to subset data to a specific time range. If None, DataProcesser will extract automatically the range of time in the dataset columns. Useful to completely exclude some acquisition times where irrelevant measurements are acquired. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# data_file = '/home/marc/Dropbox/Work/TSclass_GF/data/ErkAkt_6GF_len240_repl2_trim100.zip'\n",
    "# data_file = '/home/marc/Dropbox/CNN_paper_MarcAntoine/forPaper/data_analysis/synthetic_len750_univariate_classAB.zip'\n",
    "# data_file = '/home/marc/Dropbox/CNN_paper_MarcAntoine/forPaper/data_analysis/synthetic_len750.zip'\n",
    "#data_file = '/home/marc/Dropbox/CNN_paper_MarcAntoine/forPaper/data_analysis/drosophila.zip'\n",
    "data_file='/home/marc/Desktop/WW_Archive.zip'\n",
    "meas_var = None\n",
    "start_time = 10\n",
    "end_time = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataProcesser(data_file, datatable=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels order: ['ERK'] \n",
      "Time range: (10, 40) \n",
      "Classes: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marc/Dropbox/Work/TSclass_GF/load_data.py:461: UserWarning: Data were not processed.\n",
      "  warnings.warn('Data were not processed.')\n"
     ]
    }
   ],
   "source": [
    "# Read default \n",
    "meas_var = data.detect_groups_times()['groups'] if meas_var is None else meas_var\n",
    "start_time = data.detect_groups_times()['times'][0] if start_time is None else start_time\n",
    "end_time = data.detect_groups_times()['times'][1] if end_time is None else end_time\n",
    "data.subset(sel_groups=meas_var, start_time=start_time, end_time=end_time)\n",
    "if selected_classes:\n",
    "    data.dataset = data.dataset[data.dataset[data.col_class].isin(selected_classes)]\n",
    "data.get_stats()\n",
    "# data.process(method='center_train', independent_groups=True)\n",
    "data.split_sets()\n",
    "\n",
    "subtract_numbers = [data.stats['mu'][meas]['train'] for meas in meas_var]\n",
    "ls_transforms = transforms.Compose([\n",
    "    RandomCrop(output_size=length, ignore_na_tails=True),\n",
    "    #transforms.RandomApply([RandomNoise(mu=0, sigma=0.02)]),\n",
    "    Subtract(subtract_numbers),\n",
    "    ToTensor()])\n",
    "\n",
    "data_train = myDataset(dataset=data.train_set, transform=ls_transforms)\n",
    "data_test = myDataset(dataset=data.validation_set, transform=ls_transforms)\n",
    "\n",
    "print('Channels order: {} \\nTime range: ({}, {}) \\nClasses: {}'.format(meas_var, start_time, end_time, list(data.dataset[data.col_class].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some trajectories to check that the data loading and processing is properly done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7ff6d7d80358>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 732.35x432 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_smpl = 6\n",
    "indx_smpl = np.random.randint(0, len(data_train), n_smpl)\n",
    "\n",
    "col_ids = []\n",
    "col_lab = []\n",
    "col_mes = []\n",
    "# Long format for seaborn grid, for loop to avoid multiple indexing\n",
    "# This would triggers preprocessing multiple times and add randomness\n",
    "for i in indx_smpl:\n",
    "    smpl = data_train[i]\n",
    "    col_ids.append(smpl['identifier'])\n",
    "    col_lab.append(smpl['label'].item())\n",
    "    col_mes.append(smpl['series'].numpy().transpose())\n",
    "col_ids = pd.Series(np.hstack(np.repeat(col_ids, length)))\n",
    "col_lab = pd.Series(np.hstack(np.repeat(col_lab, length)))\n",
    "col_mes = pd.DataFrame(np.vstack(col_mes), columns=meas_var)\n",
    "col_tim = pd.Series(np.tile(np.arange(0, length), n_smpl))\n",
    "\n",
    "df_smpl = pd.concat([col_ids, col_lab, col_tim, col_mes], axis=1)\n",
    "df_smpl.rename(columns={0: 'identifier', 1: 'label', 2:'time'}, inplace=True)\n",
    "df_smpl = df_smpl.melt(id_vars=['identifier', 'label', 'time'], value_vars=meas_var)\n",
    "\n",
    "sns.set_style('white')\n",
    "sns.set_context('notebook')\n",
    "grid = sns.FacetGrid(data=df_smpl, col='identifier', col_wrap=3, sharex=True)\n",
    "grid.map_dataframe(sns.lineplot, x='time', y='value', hue='variable')\n",
    "grid.set(xlabel='Time', ylabel='Measurement Value')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Resume training or new model\n",
    "\n",
    "Set to None for new model, otherwise provide the path to a saved model file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "load_model = None\n",
    "# load_model = 'path/to/file.pytorch'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Tensorboard logs and model save file\n",
    "\n",
    "Unique name for model with timestamp. Can follow training online with tensorboard with these logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "file_logs = os.path.splitext(os.path.basename(data_file))[0]  # file name without extension\n",
    "logs_str = 'logs/' + '_'.join(meas_var) + '/' + datetime.datetime.now().strftime('%Y-%m-%d-%H:%M:%S') + \\\n",
    "           '_' + file_logs + '/'\n",
    "writer = SummaryWriter(logs_str)\n",
    "save_model = 'models/' + logs_str.lstrip('logs/').rstrip('/') + '.pytorch'\n",
    "\n",
    "if not os.path.exists(file_logs):\n",
    "    os.makedirs(file_logs)\n",
    "if not os.path.exists('models/' + '_'.join(meas_var)):\n",
    "    os.makedirs('models/' + '_'.join(meas_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup model, loss and optimizer\n",
    "\n",
    "The model dimensions are tuned to fit the previous parameters. \n",
    "\n",
    "L2 regularization is controlled by the \"weight_decay\" in the optimizer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = ConvNetCam(batch_size=batch_size, nclass=nclass, length=length, nfeatures=nfeatures)\n",
    "if load_model:\n",
    "    model.load_state_dict(torch.load(load_model))\n",
    "model.double()\n",
    "cuda_available = torch.cuda.is_available()\n",
    "if cuda_available:\n",
    "    model = model.cuda()\n",
    "    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=L2_reg)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=5)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[1000, 1500, 2000, 2500], gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def TrainModel(model, optimizer, criterion, scheduler, train_loader, test_loader, nepochs,\n",
    "               save_model=save_model, logs=True, save_pyfiles=True):\n",
    "    # ------------------------------------------------------------------------------------------------------------------\n",
    "    # Model, loss, optimizer\n",
    "    top1 = AverageMeter()\n",
    "    top2 = AverageMeter()\n",
    "\n",
    "    # Create zip archive with all python file at execution time\n",
    "    if save_pyfiles:\n",
    "        lpy = [i for i in os.listdir(\".\") if i.endswith(\".py\")]\n",
    "        with zipfile.ZipFile(logs_str + \"AllPyFiles.zip\", mode='w') as zipMe:\n",
    "            for file in lpy:\n",
    "                zipMe.write(file, compress_type=zipfile.ZIP_DEFLATED)\n",
    "    if logs:\n",
    "        print('Train logs saved at: {}'.format(logs_str))\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------------------------------\n",
    "    # Get adequate size of sample for nn.Conv layers\n",
    "    # Add a dummy channel dimension for conv1D layer (if multivariate, treat as a 2D plane with 1 channel)\n",
    "    assert len(train_loader.dataset[0]['series'].shape) == 2\n",
    "    nchannel, univar_length = train_loader.dataset[0]['series'].shape\n",
    "    if nchannel == 1:\n",
    "        view_size = (batch_size, 1, univar_length)\n",
    "    elif nchannel >= 2:\n",
    "        view_size = (batch_size, 1, nchannel, univar_length)\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------------------------------\n",
    "    # Training loop\n",
    "    for epoch in range(nepochs):\n",
    "        model.train()\n",
    "        top1.reset()\n",
    "        top2.reset()\n",
    "\n",
    "        loss_train = []\n",
    "        for i_batch, sample_batch in enumerate(train_loader):\n",
    "            series, label = sample_batch['series'], sample_batch['label']\n",
    "            if cuda_available:\n",
    "                series, label = series.cuda(), label.cuda()\n",
    "            series = series.view(view_size)\n",
    "            prediction = model(series)\n",
    "            loss = criterion(prediction, label)\n",
    "            loss_train.append(loss.cpu().detach().numpy())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i_batch % 25 == 0:\n",
    "                print('Training epoch: [{0}/{4}][{1}/{2}]; Loss: {3}'.format(epoch + 1, i_batch + 1, len(train_loader),\n",
    "                                                                             loss, nepochs))\n",
    "\n",
    "            prec1, prec2 = accuracy(prediction, label, topk=(1, 2))\n",
    "            top1.update(prec1[0], series.size(0))\n",
    "            top2.update(prec2[0], series.size(0))\n",
    "\n",
    "            if i_batch % 100 == 0:\n",
    "                print('Training Accuracy Epoch: [{0}]\\t'\n",
    "                      'Prec@1 {top1.val.data:.3f} ({top1.avg.data:.3f})\\t'\n",
    "                      'Prec@2 {top2.val.data:.3f} ({top2.avg.data:.3f})'.format(\n",
    "                    epoch, top1=top1, top2=top2))\n",
    "            if logs:\n",
    "                writer.add_scalar('Train/Loss', loss, epoch * len(train_loader) + i_batch + 1)\n",
    "                writer.add_scalar('Train/Top1', top1.val, epoch * len(train_loader) + i_batch + 1)\n",
    "                writer.add_scalar('Train/Top2', top2.val, epoch * len(train_loader) + i_batch + 1)\n",
    "        if logs:\n",
    "            writer.add_scalar('MeanEpoch/Train_Loss', np.mean(loss_train), epoch)\n",
    "            writer.add_scalar('MeanEpoch/Train_Top1', top1.avg, epoch)\n",
    "            writer.add_scalar('MeanEpoch/Train_Top2', top2.avg, epoch)\n",
    "\n",
    "        # --------------------------------------------------------------------------------------------------------------\n",
    "        # Evaluation loop\n",
    "        model.eval()\n",
    "        top1.reset()\n",
    "        top2.reset()\n",
    "        loss_eval = []\n",
    "        for i_batch, sample_batch in enumerate(test_loader):\n",
    "            series, label = sample_batch['series'], sample_batch['label']\n",
    "            if cuda_available:\n",
    "                series, label = series.cuda(), label.cuda()\n",
    "            series = series.view(view_size)\n",
    "            label = torch.autograd.Variable(label)\n",
    "            \n",
    "            prediction = model(series)\n",
    "            loss = criterion(prediction, label)\n",
    "            loss_eval.append(loss.cpu().detach().numpy())\n",
    "\n",
    "            prec1, prec2 = accuracy(prediction, label, topk=(1, 2))\n",
    "            top1.update(prec1[0], series.size(0))\n",
    "            top2.update(prec2[0], series.size(0))\n",
    "\n",
    "        # For validation loss, report only after the whole batch is processed\n",
    "        if logs:\n",
    "            writer.add_scalar('Val/Loss', loss, epoch * len(train_loader) + i_batch + 1)\n",
    "            writer.add_scalar('Val/Top1', top1.val, epoch * len(train_loader) + i_batch + 1)\n",
    "            writer.add_scalar('Val/Top2', top2.val, epoch * len(train_loader) + i_batch + 1)\n",
    "            writer.add_scalar('MeanEpoch/Val_Loss', np.mean(loss_eval), epoch)\n",
    "            writer.add_scalar('MeanEpoch/Val_Top1', top1.avg, epoch)\n",
    "            writer.add_scalar('MeanEpoch/Val_Top2', top2.avg, epoch)\n",
    "\n",
    "\n",
    "        print('===>>>\\t'\n",
    "              'Prec@1 ({top1.avg.data:.3f})\\t'\n",
    "              'Prec@2 ({top2.avg.data:.3f})'.format(top1=top1, top2=top2))\n",
    "\n",
    "        writer.add_scalar('LearningRate', optimizer.param_groups[0]['lr'], epoch)\n",
    "        #scheduler.step(np.mean(loss_eval))\n",
    "        scheduler.step()\n",
    "        \n",
    "        \n",
    "    if save_model:\n",
    "        torch.save(model, save_model)\n",
    "        print('Model saved at: {}'.format(save_model))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Run the training\n",
    "\n",
    "Can follow the training in tensorboard with:\n",
    "```\n",
    "tensorboard --logdir \"path/to/logs\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train logs saved at: logs/ERK/2020-02-21-15:39:22_WW_Archive/\n",
      "Training epoch: [1/2000][1/4]; Loss: 2.528763921074193\n",
      "Training Accuracy Epoch: [0]\tPrec@1 11.719 (11.719)\tPrec@2 21.094 (21.094)\n",
      "===>>>\tPrec@1 (7.812)\tPrec@2 (12.500)\n",
      "Training epoch: [2/2000][1/4]; Loss: 2.5524611268939466\n",
      "Training Accuracy Epoch: [1]\tPrec@1 7.812 (7.812)\tPrec@2 14.062 (14.062)\n",
      "===>>>\tPrec@1 (10.938)\tPrec@2 (15.625)\n",
      "Training epoch: [3/2000][1/4]; Loss: 2.511398568324134\n",
      "Training Accuracy Epoch: [2]\tPrec@1 7.812 (7.812)\tPrec@2 25.000 (25.000)\n",
      "===>>>\tPrec@1 (10.156)\tPrec@2 (15.625)\n",
      "Training epoch: [4/2000][1/4]; Loss: 2.5407067264684047\n",
      "Training Accuracy Epoch: [3]\tPrec@1 9.375 (9.375)\tPrec@2 17.969 (17.969)\n",
      "===>>>\tPrec@1 (5.469)\tPrec@2 (17.188)\n",
      "Training epoch: [5/2000][1/4]; Loss: 2.5491181765220503\n",
      "Training Accuracy Epoch: [4]\tPrec@1 5.469 (5.469)\tPrec@2 17.969 (17.969)\n",
      "===>>>\tPrec@1 (7.031)\tPrec@2 (14.844)\n",
      "Training epoch: [6/2000][1/4]; Loss: 2.513113688362335\n",
      "Training Accuracy Epoch: [5]\tPrec@1 11.719 (11.719)\tPrec@2 18.750 (18.750)\n",
      "===>>>\tPrec@1 (7.031)\tPrec@2 (20.312)\n",
      "Training epoch: [7/2000][1/4]; Loss: 2.5120121409940035\n",
      "Training Accuracy Epoch: [6]\tPrec@1 10.156 (10.156)\tPrec@2 21.875 (21.875)\n",
      "===>>>\tPrec@1 (6.250)\tPrec@2 (11.719)\n",
      "Training epoch: [8/2000][1/4]; Loss: 2.5357982836500086\n",
      "Training Accuracy Epoch: [7]\tPrec@1 14.062 (14.062)\tPrec@2 23.438 (23.438)\n",
      "===>>>\tPrec@1 (6.250)\tPrec@2 (11.719)\n",
      "Training epoch: [9/2000][1/4]; Loss: 2.548610840450285\n",
      "Training Accuracy Epoch: [8]\tPrec@1 13.281 (13.281)\tPrec@2 17.969 (17.969)\n",
      "===>>>\tPrec@1 (7.031)\tPrec@2 (14.062)\n",
      "Training epoch: [10/2000][1/4]; Loss: 2.546026741822462\n",
      "Training Accuracy Epoch: [9]\tPrec@1 14.844 (14.844)\tPrec@2 25.000 (25.000)\n",
      "===>>>\tPrec@1 (4.688)\tPrec@2 (10.938)\n",
      "Training epoch: [11/2000][1/4]; Loss: 2.525302480142289\n",
      "Training Accuracy Epoch: [10]\tPrec@1 18.750 (18.750)\tPrec@2 27.344 (27.344)\n",
      "===>>>\tPrec@1 (6.250)\tPrec@2 (14.062)\n",
      "Training epoch: [12/2000][1/4]; Loss: 2.526931792375635\n",
      "Training Accuracy Epoch: [11]\tPrec@1 13.281 (13.281)\tPrec@2 20.312 (20.312)\n",
      "===>>>\tPrec@1 (6.250)\tPrec@2 (14.062)\n",
      "Training epoch: [13/2000][1/4]; Loss: 2.5279966674164265\n",
      "Training Accuracy Epoch: [12]\tPrec@1 13.281 (13.281)\tPrec@2 21.875 (21.875)\n",
      "===>>>\tPrec@1 (8.594)\tPrec@2 (14.844)\n",
      "Training epoch: [14/2000][1/4]; Loss: 2.5052989783818926\n",
      "Training Accuracy Epoch: [13]\tPrec@1 9.375 (9.375)\tPrec@2 23.438 (23.438)\n",
      "===>>>\tPrec@1 (9.375)\tPrec@2 (14.844)\n",
      "Training epoch: [15/2000][1/4]; Loss: 2.5235737182345614\n",
      "Training Accuracy Epoch: [14]\tPrec@1 14.062 (14.062)\tPrec@2 23.438 (23.438)\n",
      "===>>>\tPrec@1 (9.375)\tPrec@2 (14.844)\n",
      "Training epoch: [16/2000][1/4]; Loss: 2.525425845641192\n",
      "Training Accuracy Epoch: [15]\tPrec@1 10.156 (10.156)\tPrec@2 20.312 (20.312)\n",
      "===>>>\tPrec@1 (8.594)\tPrec@2 (14.844)\n",
      "Training epoch: [17/2000][1/4]; Loss: 2.520620003728908\n",
      "Training Accuracy Epoch: [16]\tPrec@1 10.938 (10.938)\tPrec@2 23.438 (23.438)\n",
      "===>>>\tPrec@1 (10.156)\tPrec@2 (17.188)\n",
      "Training epoch: [18/2000][1/4]; Loss: 2.49143303369281\n",
      "Training Accuracy Epoch: [17]\tPrec@1 16.406 (16.406)\tPrec@2 25.000 (25.000)\n",
      "===>>>\tPrec@1 (10.156)\tPrec@2 (18.750)\n",
      "Training epoch: [19/2000][1/4]; Loss: 2.4997891760356263\n",
      "Training Accuracy Epoch: [18]\tPrec@1 11.719 (11.719)\tPrec@2 24.219 (24.219)\n",
      "===>>>\tPrec@1 (8.594)\tPrec@2 (18.750)\n",
      "Training epoch: [20/2000][1/4]; Loss: 2.5178132389857835\n",
      "Training Accuracy Epoch: [19]\tPrec@1 14.062 (14.062)\tPrec@2 25.000 (25.000)\n",
      "===>>>\tPrec@1 (9.375)\tPrec@2 (19.531)\n",
      "Training epoch: [21/2000][1/4]; Loss: 2.5170541711746206\n",
      "Training Accuracy Epoch: [20]\tPrec@1 12.500 (12.500)\tPrec@2 24.219 (24.219)\n",
      "===>>>\tPrec@1 (10.156)\tPrec@2 (21.094)\n",
      "Training epoch: [22/2000][1/4]; Loss: 2.53053198340488\n",
      "Training Accuracy Epoch: [21]\tPrec@1 10.938 (10.938)\tPrec@2 22.656 (22.656)\n",
      "===>>>\tPrec@1 (8.594)\tPrec@2 (21.094)\n",
      "Training epoch: [23/2000][1/4]; Loss: 2.5187460697776123\n",
      "Training Accuracy Epoch: [22]\tPrec@1 13.281 (13.281)\tPrec@2 28.125 (28.125)\n",
      "===>>>\tPrec@1 (9.375)\tPrec@2 (19.531)\n",
      "Training epoch: [24/2000][1/4]; Loss: 2.471040162667133\n",
      "Training Accuracy Epoch: [23]\tPrec@1 18.750 (18.750)\tPrec@2 32.031 (32.031)\n",
      "===>>>\tPrec@1 (10.156)\tPrec@2 (18.750)\n",
      "Training epoch: [25/2000][1/4]; Loss: 2.520495637759025\n",
      "Training Accuracy Epoch: [24]\tPrec@1 10.938 (10.938)\tPrec@2 25.781 (25.781)\n",
      "===>>>\tPrec@1 (9.375)\tPrec@2 (17.969)\n",
      "Training epoch: [26/2000][1/4]; Loss: 2.5072962542542134\n",
      "Training Accuracy Epoch: [25]\tPrec@1 13.281 (13.281)\tPrec@2 25.781 (25.781)\n",
      "===>>>\tPrec@1 (12.500)\tPrec@2 (21.094)\n",
      "Training epoch: [27/2000][1/4]; Loss: 2.4931736463769285\n",
      "Training Accuracy Epoch: [26]\tPrec@1 15.625 (15.625)\tPrec@2 28.125 (28.125)\n",
      "===>>>\tPrec@1 (11.719)\tPrec@2 (20.312)\n",
      "Training epoch: [28/2000][1/4]; Loss: 2.514435460088935\n",
      "Training Accuracy Epoch: [27]\tPrec@1 12.500 (12.500)\tPrec@2 27.344 (27.344)\n",
      "===>>>\tPrec@1 (10.938)\tPrec@2 (22.656)\n",
      "Training epoch: [29/2000][1/4]; Loss: 2.4975866153910875\n",
      "Training Accuracy Epoch: [28]\tPrec@1 16.406 (16.406)\tPrec@2 28.906 (28.906)\n",
      "===>>>\tPrec@1 (8.594)\tPrec@2 (19.531)\n",
      "Training epoch: [30/2000][1/4]; Loss: 2.4924975685477118\n",
      "Training Accuracy Epoch: [29]\tPrec@1 13.281 (13.281)\tPrec@2 23.438 (23.438)\n",
      "===>>>\tPrec@1 (7.812)\tPrec@2 (19.531)\n",
      "Training epoch: [31/2000][1/4]; Loss: 2.5151041574336337\n",
      "Training Accuracy Epoch: [30]\tPrec@1 11.719 (11.719)\tPrec@2 21.875 (21.875)\n",
      "===>>>\tPrec@1 (10.156)\tPrec@2 (21.094)\n",
      "Training epoch: [32/2000][1/4]; Loss: 2.499094344084353\n",
      "Training Accuracy Epoch: [31]\tPrec@1 14.844 (14.844)\tPrec@2 28.906 (28.906)\n",
      "===>>>\tPrec@1 (10.156)\tPrec@2 (18.750)\n",
      "Training epoch: [33/2000][1/4]; Loss: 2.528118505190777\n",
      "Training Accuracy Epoch: [32]\tPrec@1 11.719 (11.719)\tPrec@2 21.094 (21.094)\n",
      "===>>>\tPrec@1 (8.594)\tPrec@2 (19.531)\n",
      "Training epoch: [34/2000][1/4]; Loss: 2.5079510179049733\n",
      "Training Accuracy Epoch: [33]\tPrec@1 10.938 (10.938)\tPrec@2 25.000 (25.000)\n",
      "===>>>\tPrec@1 (8.594)\tPrec@2 (20.312)\n",
      "Training epoch: [35/2000][1/4]; Loss: 2.471484900477014\n",
      "Training Accuracy Epoch: [34]\tPrec@1 15.625 (15.625)\tPrec@2 28.125 (28.125)\n",
      "===>>>\tPrec@1 (10.938)\tPrec@2 (25.000)\n",
      "Training epoch: [36/2000][1/4]; Loss: 2.4860620668017273\n",
      "Training Accuracy Epoch: [35]\tPrec@1 16.406 (16.406)\tPrec@2 30.469 (30.469)\n",
      "===>>>\tPrec@1 (9.375)\tPrec@2 (18.750)\n",
      "Training epoch: [37/2000][1/4]; Loss: 2.4535324938305654\n",
      "Training Accuracy Epoch: [36]\tPrec@1 13.281 (13.281)\tPrec@2 29.688 (29.688)\n",
      "===>>>\tPrec@1 (8.594)\tPrec@2 (19.531)\n",
      "Training epoch: [38/2000][1/4]; Loss: 2.4874507058775115\n",
      "Training Accuracy Epoch: [37]\tPrec@1 17.969 (17.969)\tPrec@2 29.688 (29.688)\n",
      "===>>>\tPrec@1 (10.156)\tPrec@2 (21.094)\n",
      "Training epoch: [39/2000][1/4]; Loss: 2.4481112229374364\n",
      "Training Accuracy Epoch: [38]\tPrec@1 14.844 (14.844)\tPrec@2 29.688 (29.688)\n",
      "===>>>\tPrec@1 (11.719)\tPrec@2 (22.656)\n",
      "Training epoch: [40/2000][1/4]; Loss: 2.498257815707757\n",
      "Training Accuracy Epoch: [39]\tPrec@1 14.062 (14.062)\tPrec@2 28.125 (28.125)\n",
      "===>>>\tPrec@1 (10.938)\tPrec@2 (24.219)\n",
      "Training epoch: [41/2000][1/4]; Loss: 2.4981277830487327\n",
      "Training Accuracy Epoch: [40]\tPrec@1 9.375 (9.375)\tPrec@2 22.656 (22.656)\n",
      "===>>>\tPrec@1 (9.375)\tPrec@2 (20.312)\n",
      "Training epoch: [42/2000][1/4]; Loss: 2.488670825315254\n",
      "Training Accuracy Epoch: [41]\tPrec@1 14.844 (14.844)\tPrec@2 29.688 (29.688)\n",
      "===>>>\tPrec@1 (8.594)\tPrec@2 (19.531)\n",
      "Training epoch: [43/2000][1/4]; Loss: 2.48811152863423\n",
      "Training Accuracy Epoch: [42]\tPrec@1 11.719 (11.719)\tPrec@2 25.781 (25.781)\n",
      "===>>>\tPrec@1 (10.938)\tPrec@2 (20.312)\n",
      "Training epoch: [44/2000][1/4]; Loss: 2.4792325952473133\n",
      "Training Accuracy Epoch: [43]\tPrec@1 20.312 (20.312)\tPrec@2 32.031 (32.031)\n",
      "===>>>\tPrec@1 (10.156)\tPrec@2 (21.094)\n",
      "Training epoch: [45/2000][1/4]; Loss: 2.471419941044362\n",
      "Training Accuracy Epoch: [44]\tPrec@1 17.188 (17.188)\tPrec@2 28.125 (28.125)\n",
      "===>>>\tPrec@1 (9.375)\tPrec@2 (21.094)\n",
      "Training epoch: [46/2000][1/4]; Loss: 2.4697073612271705\n",
      "Training Accuracy Epoch: [45]\tPrec@1 16.406 (16.406)\tPrec@2 25.781 (25.781)\n",
      "===>>>\tPrec@1 (7.812)\tPrec@2 (19.531)\n",
      "Training epoch: [47/2000][1/4]; Loss: 2.4807354510665283\n",
      "Training Accuracy Epoch: [46]\tPrec@1 14.062 (14.062)\tPrec@2 25.000 (25.000)\n",
      "===>>>\tPrec@1 (10.938)\tPrec@2 (25.781)\n",
      "Training epoch: [48/2000][1/4]; Loss: 2.4858603767560523\n",
      "Training Accuracy Epoch: [47]\tPrec@1 13.281 (13.281)\tPrec@2 26.562 (26.562)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>>>\tPrec@1 (10.938)\tPrec@2 (21.094)\n",
      "Training epoch: [49/2000][1/4]; Loss: 2.4695219627821587\n",
      "Training Accuracy Epoch: [48]\tPrec@1 13.281 (13.281)\tPrec@2 30.469 (30.469)\n",
      "===>>>\tPrec@1 (6.250)\tPrec@2 (17.969)\n",
      "Training epoch: [50/2000][1/4]; Loss: 2.4603242760617787\n",
      "Training Accuracy Epoch: [49]\tPrec@1 15.625 (15.625)\tPrec@2 30.469 (30.469)\n",
      "===>>>\tPrec@1 (8.594)\tPrec@2 (22.656)\n",
      "Training epoch: [51/2000][1/4]; Loss: 2.4897480403350225\n",
      "Training Accuracy Epoch: [50]\tPrec@1 13.281 (13.281)\tPrec@2 28.125 (28.125)\n",
      "===>>>\tPrec@1 (10.938)\tPrec@2 (21.094)\n",
      "Training epoch: [52/2000][1/4]; Loss: 2.4705920779733086\n",
      "Training Accuracy Epoch: [51]\tPrec@1 15.625 (15.625)\tPrec@2 30.469 (30.469)\n",
      "===>>>\tPrec@1 (9.375)\tPrec@2 (20.312)\n",
      "Training epoch: [53/2000][1/4]; Loss: 2.476060750780632\n",
      "Training Accuracy Epoch: [52]\tPrec@1 14.062 (14.062)\tPrec@2 26.562 (26.562)\n",
      "===>>>\tPrec@1 (10.938)\tPrec@2 (21.094)\n",
      "Training epoch: [54/2000][1/4]; Loss: 2.492565491420977\n",
      "Training Accuracy Epoch: [53]\tPrec@1 12.500 (12.500)\tPrec@2 22.656 (22.656)\n",
      "===>>>\tPrec@1 (8.594)\tPrec@2 (20.312)\n",
      "Training epoch: [55/2000][1/4]; Loss: 2.4733754740757776\n",
      "Training Accuracy Epoch: [54]\tPrec@1 14.062 (14.062)\tPrec@2 28.125 (28.125)\n",
      "===>>>\tPrec@1 (10.938)\tPrec@2 (21.094)\n",
      "Training epoch: [56/2000][1/4]; Loss: 2.4574560452561878\n",
      "Training Accuracy Epoch: [55]\tPrec@1 17.188 (17.188)\tPrec@2 28.125 (28.125)\n",
      "===>>>\tPrec@1 (9.375)\tPrec@2 (20.312)\n",
      "Training epoch: [57/2000][1/4]; Loss: 2.4750965060036045\n",
      "Training Accuracy Epoch: [56]\tPrec@1 13.281 (13.281)\tPrec@2 25.781 (25.781)\n",
      "===>>>\tPrec@1 (9.375)\tPrec@2 (21.875)\n",
      "Training epoch: [58/2000][1/4]; Loss: 2.48037118682538\n",
      "Training Accuracy Epoch: [57]\tPrec@1 13.281 (13.281)\tPrec@2 27.344 (27.344)\n",
      "===>>>\tPrec@1 (8.594)\tPrec@2 (23.438)\n",
      "Training epoch: [59/2000][1/4]; Loss: 2.4695288259745727\n",
      "Training Accuracy Epoch: [58]\tPrec@1 13.281 (13.281)\tPrec@2 29.688 (29.688)\n",
      "===>>>\tPrec@1 (9.375)\tPrec@2 (21.875)\n",
      "Training epoch: [60/2000][1/4]; Loss: 2.4460280528280345\n",
      "Training Accuracy Epoch: [59]\tPrec@1 13.281 (13.281)\tPrec@2 30.469 (30.469)\n",
      "===>>>\tPrec@1 (8.594)\tPrec@2 (19.531)\n",
      "Training epoch: [61/2000][1/4]; Loss: 2.4629591766342798\n",
      "Training Accuracy Epoch: [60]\tPrec@1 17.969 (17.969)\tPrec@2 28.906 (28.906)\n",
      "===>>>\tPrec@1 (10.938)\tPrec@2 (23.438)\n",
      "Training epoch: [62/2000][1/4]; Loss: 2.4969091760252944\n",
      "Training Accuracy Epoch: [61]\tPrec@1 14.062 (14.062)\tPrec@2 28.906 (28.906)\n",
      "===>>>\tPrec@1 (7.031)\tPrec@2 (17.969)\n",
      "Training epoch: [63/2000][1/4]; Loss: 2.443470110308558\n",
      "Training Accuracy Epoch: [62]\tPrec@1 11.719 (11.719)\tPrec@2 30.469 (30.469)\n",
      "===>>>\tPrec@1 (7.031)\tPrec@2 (21.094)\n",
      "Training epoch: [64/2000][1/4]; Loss: 2.47857201318682\n",
      "Training Accuracy Epoch: [63]\tPrec@1 13.281 (13.281)\tPrec@2 27.344 (27.344)\n",
      "===>>>\tPrec@1 (9.375)\tPrec@2 (21.875)\n",
      "Training epoch: [65/2000][1/4]; Loss: 2.448506625949768\n",
      "Training Accuracy Epoch: [64]\tPrec@1 18.750 (18.750)\tPrec@2 32.031 (32.031)\n",
      "===>>>\tPrec@1 (4.688)\tPrec@2 (17.969)\n",
      "Training epoch: [66/2000][1/4]; Loss: 2.465980119400462\n",
      "Training Accuracy Epoch: [65]\tPrec@1 11.719 (11.719)\tPrec@2 24.219 (24.219)\n",
      "===>>>\tPrec@1 (6.250)\tPrec@2 (21.875)\n",
      "Training epoch: [67/2000][1/4]; Loss: 2.4573818732414323\n",
      "Training Accuracy Epoch: [66]\tPrec@1 14.844 (14.844)\tPrec@2 30.469 (30.469)\n",
      "===>>>\tPrec@1 (7.812)\tPrec@2 (22.656)\n",
      "Training epoch: [68/2000][1/4]; Loss: 2.50106122266257\n",
      "Training Accuracy Epoch: [67]\tPrec@1 13.281 (13.281)\tPrec@2 25.000 (25.000)\n",
      "===>>>\tPrec@1 (6.250)\tPrec@2 (18.750)\n",
      "Training epoch: [69/2000][1/4]; Loss: 2.447887111180232\n",
      "Training Accuracy Epoch: [68]\tPrec@1 14.844 (14.844)\tPrec@2 32.031 (32.031)\n",
      "===>>>\tPrec@1 (7.812)\tPrec@2 (18.750)\n",
      "Training epoch: [70/2000][1/4]; Loss: 2.501415065434016\n",
      "Training Accuracy Epoch: [69]\tPrec@1 9.375 (9.375)\tPrec@2 17.969 (17.969)\n",
      "===>>>\tPrec@1 (6.250)\tPrec@2 (17.969)\n",
      "Training epoch: [71/2000][1/4]; Loss: 2.491607322661165\n",
      "Training Accuracy Epoch: [70]\tPrec@1 14.844 (14.844)\tPrec@2 26.562 (26.562)\n",
      "===>>>\tPrec@1 (8.594)\tPrec@2 (18.750)\n",
      "Training epoch: [72/2000][1/4]; Loss: 2.45417509089742\n",
      "Training Accuracy Epoch: [71]\tPrec@1 17.969 (17.969)\tPrec@2 32.812 (32.812)\n",
      "===>>>\tPrec@1 (8.594)\tPrec@2 (23.438)\n",
      "Training epoch: [73/2000][1/4]; Loss: 2.4699818767902393\n",
      "Training Accuracy Epoch: [72]\tPrec@1 17.969 (17.969)\tPrec@2 29.688 (29.688)\n",
      "===>>>\tPrec@1 (9.375)\tPrec@2 (25.000)\n",
      "Training epoch: [74/2000][1/4]; Loss: 2.427915456248462\n",
      "Training Accuracy Epoch: [73]\tPrec@1 21.094 (21.094)\tPrec@2 36.719 (36.719)\n",
      "===>>>\tPrec@1 (10.156)\tPrec@2 (22.656)\n",
      "Training epoch: [75/2000][1/4]; Loss: 2.4505757507975074\n",
      "Training Accuracy Epoch: [74]\tPrec@1 17.969 (17.969)\tPrec@2 29.688 (29.688)\n",
      "===>>>\tPrec@1 (9.375)\tPrec@2 (20.312)\n",
      "Training epoch: [76/2000][1/4]; Loss: 2.488399168879306\n",
      "Training Accuracy Epoch: [75]\tPrec@1 17.969 (17.969)\tPrec@2 29.688 (29.688)\n",
      "===>>>\tPrec@1 (9.375)\tPrec@2 (21.875)\n",
      "Training epoch: [77/2000][1/4]; Loss: 2.471513086101702\n",
      "Training Accuracy Epoch: [76]\tPrec@1 18.750 (18.750)\tPrec@2 31.250 (31.250)\n",
      "===>>>\tPrec@1 (10.156)\tPrec@2 (21.875)\n",
      "Training epoch: [78/2000][1/4]; Loss: 2.4697895981284903\n",
      "Training Accuracy Epoch: [77]\tPrec@1 15.625 (15.625)\tPrec@2 32.031 (32.031)\n",
      "===>>>\tPrec@1 (9.375)\tPrec@2 (21.875)\n",
      "Training epoch: [79/2000][1/4]; Loss: 2.467989251966533\n",
      "Training Accuracy Epoch: [78]\tPrec@1 14.844 (14.844)\tPrec@2 28.906 (28.906)\n",
      "===>>>\tPrec@1 (13.281)\tPrec@2 (22.656)\n",
      "Training epoch: [80/2000][1/4]; Loss: 2.4652566037624926\n",
      "Training Accuracy Epoch: [79]\tPrec@1 16.406 (16.406)\tPrec@2 33.594 (33.594)\n",
      "===>>>\tPrec@1 (7.812)\tPrec@2 (21.094)\n",
      "Training epoch: [81/2000][1/4]; Loss: 2.4841575530357956\n",
      "Training Accuracy Epoch: [80]\tPrec@1 15.625 (15.625)\tPrec@2 26.562 (26.562)\n",
      "===>>>\tPrec@1 (11.719)\tPrec@2 (25.781)\n",
      "Training epoch: [82/2000][1/4]; Loss: 2.463322061699673\n",
      "Training Accuracy Epoch: [81]\tPrec@1 15.625 (15.625)\tPrec@2 27.344 (27.344)\n",
      "===>>>\tPrec@1 (7.812)\tPrec@2 (17.969)\n",
      "Training epoch: [83/2000][1/4]; Loss: 2.4515902085020125\n",
      "Training Accuracy Epoch: [82]\tPrec@1 17.188 (17.188)\tPrec@2 31.250 (31.250)\n",
      "===>>>\tPrec@1 (8.594)\tPrec@2 (17.969)\n",
      "Training epoch: [84/2000][1/4]; Loss: 2.4305627180270983\n",
      "Training Accuracy Epoch: [83]\tPrec@1 17.188 (17.188)\tPrec@2 31.250 (31.250)\n",
      "===>>>\tPrec@1 (7.812)\tPrec@2 (19.531)\n",
      "Training epoch: [85/2000][1/4]; Loss: 2.4843268005646295\n",
      "Training Accuracy Epoch: [84]\tPrec@1 13.281 (13.281)\tPrec@2 23.438 (23.438)\n",
      "===>>>\tPrec@1 (10.156)\tPrec@2 (24.219)\n",
      "Training epoch: [86/2000][1/4]; Loss: 2.470181434408141\n",
      "Training Accuracy Epoch: [85]\tPrec@1 16.406 (16.406)\tPrec@2 24.219 (24.219)\n",
      "===>>>\tPrec@1 (7.031)\tPrec@2 (21.875)\n",
      "Training epoch: [87/2000][1/4]; Loss: 2.4668054609314325\n",
      "Training Accuracy Epoch: [86]\tPrec@1 17.969 (17.969)\tPrec@2 28.125 (28.125)\n",
      "===>>>\tPrec@1 (7.812)\tPrec@2 (14.062)\n",
      "Training epoch: [88/2000][1/4]; Loss: 2.4452985095271473\n",
      "Training Accuracy Epoch: [87]\tPrec@1 16.406 (16.406)\tPrec@2 32.031 (32.031)\n",
      "===>>>\tPrec@1 (9.375)\tPrec@2 (21.094)\n",
      "Training epoch: [89/2000][1/4]; Loss: 2.4588674498880705\n",
      "Training Accuracy Epoch: [88]\tPrec@1 13.281 (13.281)\tPrec@2 30.469 (30.469)\n",
      "===>>>\tPrec@1 (8.594)\tPrec@2 (18.750)\n",
      "Training epoch: [90/2000][1/4]; Loss: 2.5017231738030525\n",
      "Training Accuracy Epoch: [89]\tPrec@1 14.844 (14.844)\tPrec@2 24.219 (24.219)\n",
      "===>>>\tPrec@1 (10.156)\tPrec@2 (23.438)\n",
      "Training epoch: [91/2000][1/4]; Loss: 2.463426362516349\n",
      "Training Accuracy Epoch: [90]\tPrec@1 16.406 (16.406)\tPrec@2 28.125 (28.125)\n",
      "===>>>\tPrec@1 (8.594)\tPrec@2 (23.438)\n",
      "Training epoch: [92/2000][1/4]; Loss: 2.4444890116361986\n",
      "Training Accuracy Epoch: [91]\tPrec@1 14.062 (14.062)\tPrec@2 32.031 (32.031)\n",
      "===>>>\tPrec@1 (9.375)\tPrec@2 (21.875)\n",
      "Training epoch: [93/2000][1/4]; Loss: 2.4690683656918866\n",
      "Training Accuracy Epoch: [92]\tPrec@1 12.500 (12.500)\tPrec@2 25.000 (25.000)\n",
      "===>>>\tPrec@1 (11.719)\tPrec@2 (23.438)\n",
      "Training epoch: [94/2000][1/4]; Loss: 2.476888192006005\n",
      "Training Accuracy Epoch: [93]\tPrec@1 15.625 (15.625)\tPrec@2 30.469 (30.469)\n",
      "===>>>\tPrec@1 (14.062)\tPrec@2 (25.000)\n",
      "Training epoch: [95/2000][1/4]; Loss: 2.4838445577196056\n",
      "Training Accuracy Epoch: [94]\tPrec@1 10.156 (10.156)\tPrec@2 23.438 (23.438)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (24.219)\n",
      "Training epoch: [96/2000][1/4]; Loss: 2.442121724038347\n",
      "Training Accuracy Epoch: [95]\tPrec@1 17.188 (17.188)\tPrec@2 30.469 (30.469)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>>>\tPrec@1 (13.281)\tPrec@2 (21.875)\n",
      "Training epoch: [97/2000][1/4]; Loss: 2.475892570423039\n",
      "Training Accuracy Epoch: [96]\tPrec@1 13.281 (13.281)\tPrec@2 23.438 (23.438)\n",
      "===>>>\tPrec@1 (14.062)\tPrec@2 (24.219)\n",
      "Training epoch: [98/2000][1/4]; Loss: 2.4490453628590947\n",
      "Training Accuracy Epoch: [97]\tPrec@1 14.062 (14.062)\tPrec@2 30.469 (30.469)\n",
      "===>>>\tPrec@1 (9.375)\tPrec@2 (17.969)\n",
      "Training epoch: [99/2000][1/4]; Loss: 2.4803631411307103\n",
      "Training Accuracy Epoch: [98]\tPrec@1 14.062 (14.062)\tPrec@2 24.219 (24.219)\n",
      "===>>>\tPrec@1 (12.500)\tPrec@2 (22.656)\n",
      "Training epoch: [100/2000][1/4]; Loss: 2.4307456399340386\n",
      "Training Accuracy Epoch: [99]\tPrec@1 18.750 (18.750)\tPrec@2 32.812 (32.812)\n",
      "===>>>\tPrec@1 (10.938)\tPrec@2 (22.656)\n",
      "Training epoch: [101/2000][1/4]; Loss: 2.448453038070838\n",
      "Training Accuracy Epoch: [100]\tPrec@1 18.750 (18.750)\tPrec@2 35.156 (35.156)\n",
      "===>>>\tPrec@1 (14.062)\tPrec@2 (23.438)\n",
      "Training epoch: [102/2000][1/4]; Loss: 2.461535120815952\n",
      "Training Accuracy Epoch: [101]\tPrec@1 14.062 (14.062)\tPrec@2 30.469 (30.469)\n",
      "===>>>\tPrec@1 (11.719)\tPrec@2 (19.531)\n",
      "Training epoch: [103/2000][1/4]; Loss: 2.432497431056525\n",
      "Training Accuracy Epoch: [102]\tPrec@1 16.406 (16.406)\tPrec@2 32.812 (32.812)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (24.219)\n",
      "Training epoch: [104/2000][1/4]; Loss: 2.4534848416555444\n",
      "Training Accuracy Epoch: [103]\tPrec@1 15.625 (15.625)\tPrec@2 28.125 (28.125)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (24.219)\n",
      "Training epoch: [105/2000][1/4]; Loss: 2.453707453976367\n",
      "Training Accuracy Epoch: [104]\tPrec@1 13.281 (13.281)\tPrec@2 28.906 (28.906)\n",
      "===>>>\tPrec@1 (10.156)\tPrec@2 (21.094)\n",
      "Training epoch: [106/2000][1/4]; Loss: 2.436157825423693\n",
      "Training Accuracy Epoch: [105]\tPrec@1 19.531 (19.531)\tPrec@2 32.812 (32.812)\n",
      "===>>>\tPrec@1 (12.500)\tPrec@2 (21.875)\n",
      "Training epoch: [107/2000][1/4]; Loss: 2.4396316307060477\n",
      "Training Accuracy Epoch: [106]\tPrec@1 19.531 (19.531)\tPrec@2 31.250 (31.250)\n",
      "===>>>\tPrec@1 (14.062)\tPrec@2 (23.438)\n",
      "Training epoch: [108/2000][1/4]; Loss: 2.4651533659928604\n",
      "Training Accuracy Epoch: [107]\tPrec@1 13.281 (13.281)\tPrec@2 25.000 (25.000)\n",
      "===>>>\tPrec@1 (12.500)\tPrec@2 (20.312)\n",
      "Training epoch: [109/2000][1/4]; Loss: 2.431680081521921\n",
      "Training Accuracy Epoch: [108]\tPrec@1 19.531 (19.531)\tPrec@2 34.375 (34.375)\n",
      "===>>>\tPrec@1 (13.281)\tPrec@2 (18.750)\n",
      "Training epoch: [110/2000][1/4]; Loss: 2.4261630670772574\n",
      "Training Accuracy Epoch: [109]\tPrec@1 15.625 (15.625)\tPrec@2 31.250 (31.250)\n",
      "===>>>\tPrec@1 (10.938)\tPrec@2 (22.656)\n",
      "Training epoch: [111/2000][1/4]; Loss: 2.4524428762104566\n",
      "Training Accuracy Epoch: [110]\tPrec@1 15.625 (15.625)\tPrec@2 26.562 (26.562)\n",
      "===>>>\tPrec@1 (13.281)\tPrec@2 (21.875)\n",
      "Training epoch: [112/2000][1/4]; Loss: 2.443708157618483\n",
      "Training Accuracy Epoch: [111]\tPrec@1 16.406 (16.406)\tPrec@2 29.688 (29.688)\n",
      "===>>>\tPrec@1 (14.062)\tPrec@2 (21.875)\n",
      "Training epoch: [113/2000][1/4]; Loss: 2.4876796084769333\n",
      "Training Accuracy Epoch: [112]\tPrec@1 12.500 (12.500)\tPrec@2 24.219 (24.219)\n",
      "===>>>\tPrec@1 (12.500)\tPrec@2 (21.094)\n",
      "Training epoch: [114/2000][1/4]; Loss: 2.421004614969934\n",
      "Training Accuracy Epoch: [113]\tPrec@1 21.094 (21.094)\tPrec@2 35.156 (35.156)\n",
      "===>>>\tPrec@1 (12.500)\tPrec@2 (22.656)\n",
      "Training epoch: [115/2000][1/4]; Loss: 2.4751241729671647\n",
      "Training Accuracy Epoch: [114]\tPrec@1 15.625 (15.625)\tPrec@2 25.781 (25.781)\n",
      "===>>>\tPrec@1 (11.719)\tPrec@2 (20.312)\n",
      "Training epoch: [116/2000][1/4]; Loss: 2.4633715437412076\n",
      "Training Accuracy Epoch: [115]\tPrec@1 14.062 (14.062)\tPrec@2 25.781 (25.781)\n",
      "===>>>\tPrec@1 (13.281)\tPrec@2 (22.656)\n",
      "Training epoch: [117/2000][1/4]; Loss: 2.485042676885286\n",
      "Training Accuracy Epoch: [116]\tPrec@1 11.719 (11.719)\tPrec@2 21.094 (21.094)\n",
      "===>>>\tPrec@1 (12.500)\tPrec@2 (19.531)\n",
      "Training epoch: [118/2000][1/4]; Loss: 2.44095535858217\n",
      "Training Accuracy Epoch: [117]\tPrec@1 16.406 (16.406)\tPrec@2 32.812 (32.812)\n",
      "===>>>\tPrec@1 (11.719)\tPrec@2 (21.875)\n",
      "Training epoch: [119/2000][1/4]; Loss: 2.4413724964853936\n",
      "Training Accuracy Epoch: [118]\tPrec@1 16.406 (16.406)\tPrec@2 28.125 (28.125)\n",
      "===>>>\tPrec@1 (11.719)\tPrec@2 (24.219)\n",
      "Training epoch: [120/2000][1/4]; Loss: 2.4563954756619792\n",
      "Training Accuracy Epoch: [119]\tPrec@1 16.406 (16.406)\tPrec@2 27.344 (27.344)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (23.438)\n",
      "Training epoch: [121/2000][1/4]; Loss: 2.4119410349520005\n",
      "Training Accuracy Epoch: [120]\tPrec@1 27.344 (27.344)\tPrec@2 35.156 (35.156)\n",
      "===>>>\tPrec@1 (10.156)\tPrec@2 (19.531)\n",
      "Training epoch: [122/2000][1/4]; Loss: 2.4215681561549873\n",
      "Training Accuracy Epoch: [121]\tPrec@1 20.312 (20.312)\tPrec@2 32.031 (32.031)\n",
      "===>>>\tPrec@1 (11.719)\tPrec@2 (25.000)\n",
      "Training epoch: [123/2000][1/4]; Loss: 2.441998981542681\n",
      "Training Accuracy Epoch: [122]\tPrec@1 21.094 (21.094)\tPrec@2 29.688 (29.688)\n",
      "===>>>\tPrec@1 (13.281)\tPrec@2 (21.875)\n",
      "Training epoch: [124/2000][1/4]; Loss: 2.4438240043857986\n",
      "Training Accuracy Epoch: [123]\tPrec@1 18.750 (18.750)\tPrec@2 30.469 (30.469)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (19.531)\n",
      "Training epoch: [125/2000][1/4]; Loss: 2.4344610860822242\n",
      "Training Accuracy Epoch: [124]\tPrec@1 22.656 (22.656)\tPrec@2 34.375 (34.375)\n",
      "===>>>\tPrec@1 (12.500)\tPrec@2 (21.875)\n",
      "Training epoch: [126/2000][1/4]; Loss: 2.454180202952106\n",
      "Training Accuracy Epoch: [125]\tPrec@1 14.844 (14.844)\tPrec@2 26.562 (26.562)\n",
      "===>>>\tPrec@1 (11.719)\tPrec@2 (22.656)\n",
      "Training epoch: [127/2000][1/4]; Loss: 2.4447423572530402\n",
      "Training Accuracy Epoch: [126]\tPrec@1 17.188 (17.188)\tPrec@2 27.344 (27.344)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (22.656)\n",
      "Training epoch: [128/2000][1/4]; Loss: 2.418396633941629\n",
      "Training Accuracy Epoch: [127]\tPrec@1 22.656 (22.656)\tPrec@2 34.375 (34.375)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (24.219)\n",
      "Training epoch: [129/2000][1/4]; Loss: 2.4455854624634124\n",
      "Training Accuracy Epoch: [128]\tPrec@1 17.188 (17.188)\tPrec@2 28.125 (28.125)\n",
      "===>>>\tPrec@1 (13.281)\tPrec@2 (21.094)\n",
      "Training epoch: [130/2000][1/4]; Loss: 2.4890833281263998\n",
      "Training Accuracy Epoch: [129]\tPrec@1 15.625 (15.625)\tPrec@2 25.000 (25.000)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (22.656)\n",
      "Training epoch: [131/2000][1/4]; Loss: 2.4541373177523487\n",
      "Training Accuracy Epoch: [130]\tPrec@1 17.969 (17.969)\tPrec@2 29.688 (29.688)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (21.875)\n",
      "Training epoch: [132/2000][1/4]; Loss: 2.4216095886944196\n",
      "Training Accuracy Epoch: [131]\tPrec@1 17.969 (17.969)\tPrec@2 33.594 (33.594)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (21.094)\n",
      "Training epoch: [133/2000][1/4]; Loss: 2.437317224960758\n",
      "Training Accuracy Epoch: [132]\tPrec@1 17.969 (17.969)\tPrec@2 31.250 (31.250)\n",
      "===>>>\tPrec@1 (14.062)\tPrec@2 (25.000)\n",
      "Training epoch: [134/2000][1/4]; Loss: 2.441849350037579\n",
      "Training Accuracy Epoch: [133]\tPrec@1 17.969 (17.969)\tPrec@2 29.688 (29.688)\n",
      "===>>>\tPrec@1 (12.500)\tPrec@2 (25.000)\n",
      "Training epoch: [135/2000][1/4]; Loss: 2.447034793151864\n",
      "Training Accuracy Epoch: [134]\tPrec@1 17.969 (17.969)\tPrec@2 32.031 (32.031)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (23.438)\n",
      "Training epoch: [136/2000][1/4]; Loss: 2.416456890509795\n",
      "Training Accuracy Epoch: [135]\tPrec@1 20.312 (20.312)\tPrec@2 28.906 (28.906)\n",
      "===>>>\tPrec@1 (13.281)\tPrec@2 (23.438)\n",
      "Training epoch: [137/2000][1/4]; Loss: 2.424628202126831\n",
      "Training Accuracy Epoch: [136]\tPrec@1 14.062 (14.062)\tPrec@2 28.125 (28.125)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (24.219)\n",
      "Training epoch: [138/2000][1/4]; Loss: 2.4457009390416866\n",
      "Training Accuracy Epoch: [137]\tPrec@1 18.750 (18.750)\tPrec@2 26.562 (26.562)\n",
      "===>>>\tPrec@1 (12.500)\tPrec@2 (21.094)\n",
      "Training epoch: [139/2000][1/4]; Loss: 2.4433491896715327\n",
      "Training Accuracy Epoch: [138]\tPrec@1 17.188 (17.188)\tPrec@2 28.125 (28.125)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (26.562)\n",
      "Training epoch: [140/2000][1/4]; Loss: 2.4278233963458167\n",
      "Training Accuracy Epoch: [139]\tPrec@1 25.000 (25.000)\tPrec@2 35.938 (35.938)\n",
      "===>>>\tPrec@1 (13.281)\tPrec@2 (22.656)\n",
      "Training epoch: [141/2000][1/4]; Loss: 2.4243757669207993\n",
      "Training Accuracy Epoch: [140]\tPrec@1 20.312 (20.312)\tPrec@2 35.156 (35.156)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (21.875)\n",
      "Training epoch: [142/2000][1/4]; Loss: 2.4420783029258346\n",
      "Training Accuracy Epoch: [141]\tPrec@1 18.750 (18.750)\tPrec@2 27.344 (27.344)\n",
      "===>>>\tPrec@1 (12.500)\tPrec@2 (21.875)\n",
      "Training epoch: [143/2000][1/4]; Loss: 2.4465657378836116\n",
      "Training Accuracy Epoch: [142]\tPrec@1 17.188 (17.188)\tPrec@2 31.250 (31.250)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (25.000)\n",
      "Training epoch: [144/2000][1/4]; Loss: 2.4225444310075583\n",
      "Training Accuracy Epoch: [143]\tPrec@1 22.656 (22.656)\tPrec@2 30.469 (30.469)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>>>\tPrec@1 (18.750)\tPrec@2 (26.562)\n",
      "Training epoch: [145/2000][1/4]; Loss: 2.457136116148197\n",
      "Training Accuracy Epoch: [144]\tPrec@1 17.188 (17.188)\tPrec@2 25.781 (25.781)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (25.000)\n",
      "Training epoch: [146/2000][1/4]; Loss: 2.4775092292922944\n",
      "Training Accuracy Epoch: [145]\tPrec@1 17.969 (17.969)\tPrec@2 23.438 (23.438)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (27.344)\n",
      "Training epoch: [147/2000][1/4]; Loss: 2.413728639674719\n",
      "Training Accuracy Epoch: [146]\tPrec@1 25.000 (25.000)\tPrec@2 36.719 (36.719)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (24.219)\n",
      "Training epoch: [148/2000][1/4]; Loss: 2.417660151674093\n",
      "Training Accuracy Epoch: [147]\tPrec@1 21.875 (21.875)\tPrec@2 35.156 (35.156)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (22.656)\n",
      "Training epoch: [149/2000][1/4]; Loss: 2.4546254983409126\n",
      "Training Accuracy Epoch: [148]\tPrec@1 16.406 (16.406)\tPrec@2 28.125 (28.125)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (26.562)\n",
      "Training epoch: [150/2000][1/4]; Loss: 2.4773031500073293\n",
      "Training Accuracy Epoch: [149]\tPrec@1 13.281 (13.281)\tPrec@2 22.656 (22.656)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (23.438)\n",
      "Training epoch: [151/2000][1/4]; Loss: 2.4104032341263095\n",
      "Training Accuracy Epoch: [150]\tPrec@1 25.000 (25.000)\tPrec@2 33.594 (33.594)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (24.219)\n",
      "Training epoch: [152/2000][1/4]; Loss: 2.4489461607362877\n",
      "Training Accuracy Epoch: [151]\tPrec@1 20.312 (20.312)\tPrec@2 28.125 (28.125)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (24.219)\n",
      "Training epoch: [153/2000][1/4]; Loss: 2.4396045573695395\n",
      "Training Accuracy Epoch: [152]\tPrec@1 21.094 (21.094)\tPrec@2 27.344 (27.344)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (22.656)\n",
      "Training epoch: [154/2000][1/4]; Loss: 2.4560491764062102\n",
      "Training Accuracy Epoch: [153]\tPrec@1 14.844 (14.844)\tPrec@2 26.562 (26.562)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.125)\n",
      "Training epoch: [155/2000][1/4]; Loss: 2.4113631560937847\n",
      "Training Accuracy Epoch: [154]\tPrec@1 26.562 (26.562)\tPrec@2 32.812 (32.812)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (22.656)\n",
      "Training epoch: [156/2000][1/4]; Loss: 2.437884134133709\n",
      "Training Accuracy Epoch: [155]\tPrec@1 19.531 (19.531)\tPrec@2 31.250 (31.250)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (25.000)\n",
      "Training epoch: [157/2000][1/4]; Loss: 2.439886644283678\n",
      "Training Accuracy Epoch: [156]\tPrec@1 18.750 (18.750)\tPrec@2 29.688 (29.688)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (27.344)\n",
      "Training epoch: [158/2000][1/4]; Loss: 2.398318493685671\n",
      "Training Accuracy Epoch: [157]\tPrec@1 21.875 (21.875)\tPrec@2 33.594 (33.594)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (23.438)\n",
      "Training epoch: [159/2000][1/4]; Loss: 2.4389769576901297\n",
      "Training Accuracy Epoch: [158]\tPrec@1 18.750 (18.750)\tPrec@2 26.562 (26.562)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (27.344)\n",
      "Training epoch: [160/2000][1/4]; Loss: 2.4253449957810713\n",
      "Training Accuracy Epoch: [159]\tPrec@1 25.000 (25.000)\tPrec@2 30.469 (30.469)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (23.438)\n",
      "Training epoch: [161/2000][1/4]; Loss: 2.4106998033852527\n",
      "Training Accuracy Epoch: [160]\tPrec@1 22.656 (22.656)\tPrec@2 35.938 (35.938)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (24.219)\n",
      "Training epoch: [162/2000][1/4]; Loss: 2.4265679567488494\n",
      "Training Accuracy Epoch: [161]\tPrec@1 18.750 (18.750)\tPrec@2 32.812 (32.812)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (24.219)\n",
      "Training epoch: [163/2000][1/4]; Loss: 2.413915404062185\n",
      "Training Accuracy Epoch: [162]\tPrec@1 25.000 (25.000)\tPrec@2 35.156 (35.156)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (22.656)\n",
      "Training epoch: [164/2000][1/4]; Loss: 2.431381594386749\n",
      "Training Accuracy Epoch: [163]\tPrec@1 22.656 (22.656)\tPrec@2 32.031 (32.031)\n",
      "===>>>\tPrec@1 (14.062)\tPrec@2 (20.312)\n",
      "Training epoch: [165/2000][1/4]; Loss: 2.4848910466888063\n",
      "Training Accuracy Epoch: [164]\tPrec@1 13.281 (13.281)\tPrec@2 22.656 (22.656)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (25.000)\n",
      "Training epoch: [166/2000][1/4]; Loss: 2.40292019037841\n",
      "Training Accuracy Epoch: [165]\tPrec@1 20.312 (20.312)\tPrec@2 31.250 (31.250)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (25.781)\n",
      "Training epoch: [167/2000][1/4]; Loss: 2.3863476587494477\n",
      "Training Accuracy Epoch: [166]\tPrec@1 28.125 (28.125)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (26.562)\n",
      "Training epoch: [168/2000][1/4]; Loss: 2.4174937496380484\n",
      "Training Accuracy Epoch: [167]\tPrec@1 23.438 (23.438)\tPrec@2 34.375 (34.375)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (25.781)\n",
      "Training epoch: [169/2000][1/4]; Loss: 2.4560949318256626\n",
      "Training Accuracy Epoch: [168]\tPrec@1 10.938 (10.938)\tPrec@2 22.656 (22.656)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (23.438)\n",
      "Training epoch: [170/2000][1/4]; Loss: 2.398400287021771\n",
      "Training Accuracy Epoch: [169]\tPrec@1 21.094 (21.094)\tPrec@2 32.812 (32.812)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (25.000)\n",
      "Training epoch: [171/2000][1/4]; Loss: 2.4000963769717036\n",
      "Training Accuracy Epoch: [170]\tPrec@1 26.562 (26.562)\tPrec@2 35.938 (35.938)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (21.875)\n",
      "Training epoch: [172/2000][1/4]; Loss: 2.4137047443120903\n",
      "Training Accuracy Epoch: [171]\tPrec@1 21.094 (21.094)\tPrec@2 33.594 (33.594)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (27.344)\n",
      "Training epoch: [173/2000][1/4]; Loss: 2.420789359296246\n",
      "Training Accuracy Epoch: [172]\tPrec@1 17.969 (17.969)\tPrec@2 31.250 (31.250)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (27.344)\n",
      "Training epoch: [174/2000][1/4]; Loss: 2.401523593763116\n",
      "Training Accuracy Epoch: [173]\tPrec@1 28.125 (28.125)\tPrec@2 34.375 (34.375)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (25.000)\n",
      "Training epoch: [175/2000][1/4]; Loss: 2.3967278879903233\n",
      "Training Accuracy Epoch: [174]\tPrec@1 21.094 (21.094)\tPrec@2 36.719 (36.719)\n",
      "===>>>\tPrec@1 (13.281)\tPrec@2 (21.875)\n",
      "Training epoch: [176/2000][1/4]; Loss: 2.4179605198892173\n",
      "Training Accuracy Epoch: [175]\tPrec@1 18.750 (18.750)\tPrec@2 35.938 (35.938)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (20.312)\n",
      "Training epoch: [177/2000][1/4]; Loss: 2.4223563067601432\n",
      "Training Accuracy Epoch: [176]\tPrec@1 21.094 (21.094)\tPrec@2 32.812 (32.812)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (21.094)\n",
      "Training epoch: [178/2000][1/4]; Loss: 2.407400334234855\n",
      "Training Accuracy Epoch: [177]\tPrec@1 24.219 (24.219)\tPrec@2 32.031 (32.031)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (27.344)\n",
      "Training epoch: [179/2000][1/4]; Loss: 2.4539498334162992\n",
      "Training Accuracy Epoch: [178]\tPrec@1 17.969 (17.969)\tPrec@2 25.781 (25.781)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (26.562)\n",
      "Training epoch: [180/2000][1/4]; Loss: 2.3820179251011626\n",
      "Training Accuracy Epoch: [179]\tPrec@1 27.344 (27.344)\tPrec@2 35.938 (35.938)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (22.656)\n",
      "Training epoch: [181/2000][1/4]; Loss: 2.4165973570315327\n",
      "Training Accuracy Epoch: [180]\tPrec@1 21.094 (21.094)\tPrec@2 28.906 (28.906)\n",
      "===>>>\tPrec@1 (13.281)\tPrec@2 (20.312)\n",
      "Training epoch: [182/2000][1/4]; Loss: 2.4103603898156027\n",
      "Training Accuracy Epoch: [181]\tPrec@1 20.312 (20.312)\tPrec@2 32.031 (32.031)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (27.344)\n",
      "Training epoch: [183/2000][1/4]; Loss: 2.43421246820956\n",
      "Training Accuracy Epoch: [182]\tPrec@1 17.188 (17.188)\tPrec@2 29.688 (29.688)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (25.781)\n",
      "Training epoch: [184/2000][1/4]; Loss: 2.410907686880171\n",
      "Training Accuracy Epoch: [183]\tPrec@1 19.531 (19.531)\tPrec@2 35.156 (35.156)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (22.656)\n",
      "Training epoch: [185/2000][1/4]; Loss: 2.400765135583725\n",
      "Training Accuracy Epoch: [184]\tPrec@1 19.531 (19.531)\tPrec@2 35.156 (35.156)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (25.000)\n",
      "Training epoch: [186/2000][1/4]; Loss: 2.407825725501951\n",
      "Training Accuracy Epoch: [185]\tPrec@1 25.781 (25.781)\tPrec@2 34.375 (34.375)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (25.000)\n",
      "Training epoch: [187/2000][1/4]; Loss: 2.4138784960870994\n",
      "Training Accuracy Epoch: [186]\tPrec@1 19.531 (19.531)\tPrec@2 30.469 (30.469)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (21.875)\n",
      "Training epoch: [188/2000][1/4]; Loss: 2.4205500376278617\n",
      "Training Accuracy Epoch: [187]\tPrec@1 19.531 (19.531)\tPrec@2 30.469 (30.469)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (26.562)\n",
      "Training epoch: [189/2000][1/4]; Loss: 2.4442530588471394\n",
      "Training Accuracy Epoch: [188]\tPrec@1 18.750 (18.750)\tPrec@2 28.906 (28.906)\n",
      "===>>>\tPrec@1 (13.281)\tPrec@2 (21.875)\n",
      "Training epoch: [190/2000][1/4]; Loss: 2.393048292059123\n",
      "Training Accuracy Epoch: [189]\tPrec@1 23.438 (23.438)\tPrec@2 36.719 (36.719)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (25.000)\n",
      "Training epoch: [191/2000][1/4]; Loss: 2.4371308964444793\n",
      "Training Accuracy Epoch: [190]\tPrec@1 19.531 (19.531)\tPrec@2 29.688 (29.688)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (23.438)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: [192/2000][1/4]; Loss: 2.4261020222033727\n",
      "Training Accuracy Epoch: [191]\tPrec@1 17.969 (17.969)\tPrec@2 29.688 (29.688)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (24.219)\n",
      "Training epoch: [193/2000][1/4]; Loss: 2.3971398776891766\n",
      "Training Accuracy Epoch: [192]\tPrec@1 22.656 (22.656)\tPrec@2 31.250 (31.250)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (24.219)\n",
      "Training epoch: [194/2000][1/4]; Loss: 2.400366356289423\n",
      "Training Accuracy Epoch: [193]\tPrec@1 20.312 (20.312)\tPrec@2 30.469 (30.469)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (23.438)\n",
      "Training epoch: [195/2000][1/4]; Loss: 2.405430237890202\n",
      "Training Accuracy Epoch: [194]\tPrec@1 17.188 (17.188)\tPrec@2 31.250 (31.250)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (25.000)\n",
      "Training epoch: [196/2000][1/4]; Loss: 2.3778163323522428\n",
      "Training Accuracy Epoch: [195]\tPrec@1 28.125 (28.125)\tPrec@2 38.281 (38.281)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (26.562)\n",
      "Training epoch: [197/2000][1/4]; Loss: 2.416130120830905\n",
      "Training Accuracy Epoch: [196]\tPrec@1 22.656 (22.656)\tPrec@2 34.375 (34.375)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (25.000)\n",
      "Training epoch: [198/2000][1/4]; Loss: 2.4078859784288547\n",
      "Training Accuracy Epoch: [197]\tPrec@1 19.531 (19.531)\tPrec@2 31.250 (31.250)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (23.438)\n",
      "Training epoch: [199/2000][1/4]; Loss: 2.3751777957092335\n",
      "Training Accuracy Epoch: [198]\tPrec@1 25.781 (25.781)\tPrec@2 36.719 (36.719)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (25.000)\n",
      "Training epoch: [200/2000][1/4]; Loss: 2.4249793593200475\n",
      "Training Accuracy Epoch: [199]\tPrec@1 18.750 (18.750)\tPrec@2 27.344 (27.344)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (24.219)\n",
      "Training epoch: [201/2000][1/4]; Loss: 2.403037126120098\n",
      "Training Accuracy Epoch: [200]\tPrec@1 20.312 (20.312)\tPrec@2 33.594 (33.594)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (25.781)\n",
      "Training epoch: [202/2000][1/4]; Loss: 2.4054437916338576\n",
      "Training Accuracy Epoch: [201]\tPrec@1 23.438 (23.438)\tPrec@2 34.375 (34.375)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (27.344)\n",
      "Training epoch: [203/2000][1/4]; Loss: 2.4136512621366517\n",
      "Training Accuracy Epoch: [202]\tPrec@1 21.875 (21.875)\tPrec@2 28.906 (28.906)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (23.438)\n",
      "Training epoch: [204/2000][1/4]; Loss: 2.4033651607075734\n",
      "Training Accuracy Epoch: [203]\tPrec@1 20.312 (20.312)\tPrec@2 28.906 (28.906)\n",
      "===>>>\tPrec@1 (13.281)\tPrec@2 (20.312)\n",
      "Training epoch: [205/2000][1/4]; Loss: 2.385910292578113\n",
      "Training Accuracy Epoch: [204]\tPrec@1 22.656 (22.656)\tPrec@2 35.156 (35.156)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (28.906)\n",
      "Training epoch: [206/2000][1/4]; Loss: 2.422921481529743\n",
      "Training Accuracy Epoch: [205]\tPrec@1 17.188 (17.188)\tPrec@2 25.000 (25.000)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (23.438)\n",
      "Training epoch: [207/2000][1/4]; Loss: 2.4371196898698124\n",
      "Training Accuracy Epoch: [206]\tPrec@1 17.188 (17.188)\tPrec@2 28.125 (28.125)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (24.219)\n",
      "Training epoch: [208/2000][1/4]; Loss: 2.4053216079410467\n",
      "Training Accuracy Epoch: [207]\tPrec@1 19.531 (19.531)\tPrec@2 35.156 (35.156)\n",
      "===>>>\tPrec@1 (13.281)\tPrec@2 (21.875)\n",
      "Training epoch: [209/2000][1/4]; Loss: 2.4148160665112925\n",
      "Training Accuracy Epoch: [208]\tPrec@1 21.875 (21.875)\tPrec@2 31.250 (31.250)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (25.000)\n",
      "Training epoch: [210/2000][1/4]; Loss: 2.383786399088919\n",
      "Training Accuracy Epoch: [209]\tPrec@1 25.781 (25.781)\tPrec@2 34.375 (34.375)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (28.125)\n",
      "Training epoch: [211/2000][1/4]; Loss: 2.4127731972853965\n",
      "Training Accuracy Epoch: [210]\tPrec@1 21.094 (21.094)\tPrec@2 28.906 (28.906)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (25.000)\n",
      "Training epoch: [212/2000][1/4]; Loss: 2.3976567988136477\n",
      "Training Accuracy Epoch: [211]\tPrec@1 17.188 (17.188)\tPrec@2 35.156 (35.156)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (21.875)\n",
      "Training epoch: [213/2000][1/4]; Loss: 2.3803368083665535\n",
      "Training Accuracy Epoch: [212]\tPrec@1 24.219 (24.219)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (23.438)\n",
      "Training epoch: [214/2000][1/4]; Loss: 2.4455634242417865\n",
      "Training Accuracy Epoch: [213]\tPrec@1 14.062 (14.062)\tPrec@2 24.219 (24.219)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (23.438)\n",
      "Training epoch: [215/2000][1/4]; Loss: 2.393715689359724\n",
      "Training Accuracy Epoch: [214]\tPrec@1 25.000 (25.000)\tPrec@2 32.031 (32.031)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (24.219)\n",
      "Training epoch: [216/2000][1/4]; Loss: 2.387671464699131\n",
      "Training Accuracy Epoch: [215]\tPrec@1 25.781 (25.781)\tPrec@2 35.938 (35.938)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (29.688)\n",
      "Training epoch: [217/2000][1/4]; Loss: 2.3923746897108846\n",
      "Training Accuracy Epoch: [216]\tPrec@1 21.875 (21.875)\tPrec@2 32.812 (32.812)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (28.906)\n",
      "Training epoch: [218/2000][1/4]; Loss: 2.402164100865307\n",
      "Training Accuracy Epoch: [217]\tPrec@1 22.656 (22.656)\tPrec@2 31.250 (31.250)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (26.562)\n",
      "Training epoch: [219/2000][1/4]; Loss: 2.3887830902012963\n",
      "Training Accuracy Epoch: [218]\tPrec@1 23.438 (23.438)\tPrec@2 35.938 (35.938)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (22.656)\n",
      "Training epoch: [220/2000][1/4]; Loss: 2.4133860538722254\n",
      "Training Accuracy Epoch: [219]\tPrec@1 17.188 (17.188)\tPrec@2 27.344 (27.344)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (21.875)\n",
      "Training epoch: [221/2000][1/4]; Loss: 2.3812713036088837\n",
      "Training Accuracy Epoch: [220]\tPrec@1 26.562 (26.562)\tPrec@2 32.812 (32.812)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (26.562)\n",
      "Training epoch: [222/2000][1/4]; Loss: 2.374499079457952\n",
      "Training Accuracy Epoch: [221]\tPrec@1 21.094 (21.094)\tPrec@2 39.844 (39.844)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (22.656)\n",
      "Training epoch: [223/2000][1/4]; Loss: 2.417456554380137\n",
      "Training Accuracy Epoch: [222]\tPrec@1 18.750 (18.750)\tPrec@2 31.250 (31.250)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (28.906)\n",
      "Training epoch: [224/2000][1/4]; Loss: 2.357395733917898\n",
      "Training Accuracy Epoch: [223]\tPrec@1 24.219 (24.219)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (24.219)\n",
      "Training epoch: [225/2000][1/4]; Loss: 2.3843901637779283\n",
      "Training Accuracy Epoch: [224]\tPrec@1 25.000 (25.000)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (24.219)\n",
      "Training epoch: [226/2000][1/4]; Loss: 2.401945870153953\n",
      "Training Accuracy Epoch: [225]\tPrec@1 21.094 (21.094)\tPrec@2 32.812 (32.812)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (23.438)\n",
      "Training epoch: [227/2000][1/4]; Loss: 2.390908109777428\n",
      "Training Accuracy Epoch: [226]\tPrec@1 25.781 (25.781)\tPrec@2 33.594 (33.594)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (26.562)\n",
      "Training epoch: [228/2000][1/4]; Loss: 2.4067966797420386\n",
      "Training Accuracy Epoch: [227]\tPrec@1 23.438 (23.438)\tPrec@2 32.031 (32.031)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (27.344)\n",
      "Training epoch: [229/2000][1/4]; Loss: 2.37876307367234\n",
      "Training Accuracy Epoch: [228]\tPrec@1 25.781 (25.781)\tPrec@2 35.938 (35.938)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (28.125)\n",
      "Training epoch: [230/2000][1/4]; Loss: 2.352930255244949\n",
      "Training Accuracy Epoch: [229]\tPrec@1 28.906 (28.906)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (27.344)\n",
      "Training epoch: [231/2000][1/4]; Loss: 2.3799450367119532\n",
      "Training Accuracy Epoch: [230]\tPrec@1 24.219 (24.219)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (24.219)\n",
      "Training epoch: [232/2000][1/4]; Loss: 2.4175769803341978\n",
      "Training Accuracy Epoch: [231]\tPrec@1 15.625 (15.625)\tPrec@2 25.781 (25.781)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (29.688)\n",
      "Training epoch: [233/2000][1/4]; Loss: 2.4038774810652592\n",
      "Training Accuracy Epoch: [232]\tPrec@1 18.750 (18.750)\tPrec@2 29.688 (29.688)\n",
      "===>>>\tPrec@1 (14.062)\tPrec@2 (23.438)\n",
      "Training epoch: [234/2000][1/4]; Loss: 2.3915623886049997\n",
      "Training Accuracy Epoch: [233]\tPrec@1 21.094 (21.094)\tPrec@2 34.375 (34.375)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (26.562)\n",
      "Training epoch: [235/2000][1/4]; Loss: 2.408022732016373\n",
      "Training Accuracy Epoch: [234]\tPrec@1 20.312 (20.312)\tPrec@2 31.250 (31.250)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (24.219)\n",
      "Training epoch: [236/2000][1/4]; Loss: 2.3802076358097937\n",
      "Training Accuracy Epoch: [235]\tPrec@1 26.562 (26.562)\tPrec@2 35.156 (35.156)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (28.125)\n",
      "Training epoch: [237/2000][1/4]; Loss: 2.4197037215719837\n",
      "Training Accuracy Epoch: [236]\tPrec@1 17.969 (17.969)\tPrec@2 29.688 (29.688)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (21.875)\n",
      "Training epoch: [238/2000][1/4]; Loss: 2.367976219112954\n",
      "Training Accuracy Epoch: [237]\tPrec@1 25.000 (25.000)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (23.438)\n",
      "Training epoch: [239/2000][1/4]; Loss: 2.351712546956886\n",
      "Training Accuracy Epoch: [238]\tPrec@1 28.906 (28.906)\tPrec@2 40.625 (40.625)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>>>\tPrec@1 (20.312)\tPrec@2 (26.562)\n",
      "Training epoch: [240/2000][1/4]; Loss: 2.4100431283146304\n",
      "Training Accuracy Epoch: [239]\tPrec@1 24.219 (24.219)\tPrec@2 31.250 (31.250)\n",
      "===>>>\tPrec@1 (14.062)\tPrec@2 (24.219)\n",
      "Training epoch: [241/2000][1/4]; Loss: 2.3649679782296285\n",
      "Training Accuracy Epoch: [240]\tPrec@1 27.344 (27.344)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (26.562)\n",
      "Training epoch: [242/2000][1/4]; Loss: 2.383428861517081\n",
      "Training Accuracy Epoch: [241]\tPrec@1 23.438 (23.438)\tPrec@2 32.812 (32.812)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (25.000)\n",
      "Training epoch: [243/2000][1/4]; Loss: 2.3914540544531326\n",
      "Training Accuracy Epoch: [242]\tPrec@1 22.656 (22.656)\tPrec@2 29.688 (29.688)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (27.344)\n",
      "Training epoch: [244/2000][1/4]; Loss: 2.3489732410969713\n",
      "Training Accuracy Epoch: [243]\tPrec@1 25.781 (25.781)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (29.688)\n",
      "Training epoch: [245/2000][1/4]; Loss: 2.389507066800222\n",
      "Training Accuracy Epoch: [244]\tPrec@1 24.219 (24.219)\tPrec@2 32.031 (32.031)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (26.562)\n",
      "Training epoch: [246/2000][1/4]; Loss: 2.379502878662548\n",
      "Training Accuracy Epoch: [245]\tPrec@1 20.312 (20.312)\tPrec@2 30.469 (30.469)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (23.438)\n",
      "Training epoch: [247/2000][1/4]; Loss: 2.41437865147745\n",
      "Training Accuracy Epoch: [246]\tPrec@1 17.969 (17.969)\tPrec@2 30.469 (30.469)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.125)\n",
      "Training epoch: [248/2000][1/4]; Loss: 2.36525047762931\n",
      "Training Accuracy Epoch: [247]\tPrec@1 21.875 (21.875)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (28.125)\n",
      "Training epoch: [249/2000][1/4]; Loss: 2.4124420305292076\n",
      "Training Accuracy Epoch: [248]\tPrec@1 17.969 (17.969)\tPrec@2 26.562 (26.562)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (22.656)\n",
      "Training epoch: [250/2000][1/4]; Loss: 2.3917807550048935\n",
      "Training Accuracy Epoch: [249]\tPrec@1 17.969 (17.969)\tPrec@2 30.469 (30.469)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (28.125)\n",
      "Training epoch: [251/2000][1/4]; Loss: 2.3788828847590007\n",
      "Training Accuracy Epoch: [250]\tPrec@1 23.438 (23.438)\tPrec@2 33.594 (33.594)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (28.906)\n",
      "Training epoch: [252/2000][1/4]; Loss: 2.4017192958892357\n",
      "Training Accuracy Epoch: [251]\tPrec@1 21.094 (21.094)\tPrec@2 30.469 (30.469)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (24.219)\n",
      "Training epoch: [253/2000][1/4]; Loss: 2.35986495690477\n",
      "Training Accuracy Epoch: [252]\tPrec@1 22.656 (22.656)\tPrec@2 38.281 (38.281)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (26.562)\n",
      "Training epoch: [254/2000][1/4]; Loss: 2.3961459829448817\n",
      "Training Accuracy Epoch: [253]\tPrec@1 20.312 (20.312)\tPrec@2 32.031 (32.031)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (25.781)\n",
      "Training epoch: [255/2000][1/4]; Loss: 2.385279342913308\n",
      "Training Accuracy Epoch: [254]\tPrec@1 23.438 (23.438)\tPrec@2 34.375 (34.375)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (28.125)\n",
      "Training epoch: [256/2000][1/4]; Loss: 2.3813172759166283\n",
      "Training Accuracy Epoch: [255]\tPrec@1 22.656 (22.656)\tPrec@2 33.594 (33.594)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (25.781)\n",
      "Training epoch: [257/2000][1/4]; Loss: 2.369337459678034\n",
      "Training Accuracy Epoch: [256]\tPrec@1 25.000 (25.000)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (28.125)\n",
      "Training epoch: [258/2000][1/4]; Loss: 2.410405952863148\n",
      "Training Accuracy Epoch: [257]\tPrec@1 17.188 (17.188)\tPrec@2 27.344 (27.344)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (23.438)\n",
      "Training epoch: [259/2000][1/4]; Loss: 2.391609083203766\n",
      "Training Accuracy Epoch: [258]\tPrec@1 17.188 (17.188)\tPrec@2 32.031 (32.031)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (26.562)\n",
      "Training epoch: [260/2000][1/4]; Loss: 2.392776072266662\n",
      "Training Accuracy Epoch: [259]\tPrec@1 22.656 (22.656)\tPrec@2 28.906 (28.906)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (28.125)\n",
      "Training epoch: [261/2000][1/4]; Loss: 2.3687107778314362\n",
      "Training Accuracy Epoch: [260]\tPrec@1 23.438 (23.438)\tPrec@2 34.375 (34.375)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.906)\n",
      "Training epoch: [262/2000][1/4]; Loss: 2.392353141599524\n",
      "Training Accuracy Epoch: [261]\tPrec@1 17.969 (17.969)\tPrec@2 31.250 (31.250)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (25.781)\n",
      "Training epoch: [263/2000][1/4]; Loss: 2.364288313412412\n",
      "Training Accuracy Epoch: [262]\tPrec@1 24.219 (24.219)\tPrec@2 35.156 (35.156)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (25.781)\n",
      "Training epoch: [264/2000][1/4]; Loss: 2.4113932976408434\n",
      "Training Accuracy Epoch: [263]\tPrec@1 21.094 (21.094)\tPrec@2 28.906 (28.906)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (27.344)\n",
      "Training epoch: [265/2000][1/4]; Loss: 2.3604894493247985\n",
      "Training Accuracy Epoch: [264]\tPrec@1 24.219 (24.219)\tPrec@2 36.719 (36.719)\n",
      "===>>>\tPrec@1 (14.062)\tPrec@2 (25.781)\n",
      "Training epoch: [266/2000][1/4]; Loss: 2.3774333671656316\n",
      "Training Accuracy Epoch: [265]\tPrec@1 22.656 (22.656)\tPrec@2 35.938 (35.938)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (31.250)\n",
      "Training epoch: [267/2000][1/4]; Loss: 2.3676719118496603\n",
      "Training Accuracy Epoch: [266]\tPrec@1 28.125 (28.125)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (23.438)\n",
      "Training epoch: [268/2000][1/4]; Loss: 2.3753396287977226\n",
      "Training Accuracy Epoch: [267]\tPrec@1 24.219 (24.219)\tPrec@2 34.375 (34.375)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (25.781)\n",
      "Training epoch: [269/2000][1/4]; Loss: 2.3528399214816775\n",
      "Training Accuracy Epoch: [268]\tPrec@1 25.000 (25.000)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.906)\n",
      "Training epoch: [270/2000][1/4]; Loss: 2.360811131255788\n",
      "Training Accuracy Epoch: [269]\tPrec@1 22.656 (22.656)\tPrec@2 36.719 (36.719)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (28.906)\n",
      "Training epoch: [271/2000][1/4]; Loss: 2.3479239606227322\n",
      "Training Accuracy Epoch: [270]\tPrec@1 28.125 (28.125)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (28.125)\n",
      "Training epoch: [272/2000][1/4]; Loss: 2.3603105597083376\n",
      "Training Accuracy Epoch: [271]\tPrec@1 25.781 (25.781)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (24.219)\n",
      "Training epoch: [273/2000][1/4]; Loss: 2.3554052493005053\n",
      "Training Accuracy Epoch: [272]\tPrec@1 26.562 (26.562)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (25.000)\n",
      "Training epoch: [274/2000][1/4]; Loss: 2.3536446076111437\n",
      "Training Accuracy Epoch: [273]\tPrec@1 24.219 (24.219)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (25.781)\n",
      "Training epoch: [275/2000][1/4]; Loss: 2.3499540819742553\n",
      "Training Accuracy Epoch: [274]\tPrec@1 25.000 (25.000)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (25.000)\n",
      "Training epoch: [276/2000][1/4]; Loss: 2.393764780741472\n",
      "Training Accuracy Epoch: [275]\tPrec@1 21.094 (21.094)\tPrec@2 30.469 (30.469)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (28.906)\n",
      "Training epoch: [277/2000][1/4]; Loss: 2.364079331222849\n",
      "Training Accuracy Epoch: [276]\tPrec@1 26.562 (26.562)\tPrec@2 35.938 (35.938)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.125)\n",
      "Training epoch: [278/2000][1/4]; Loss: 2.3663719263514023\n",
      "Training Accuracy Epoch: [277]\tPrec@1 25.000 (25.000)\tPrec@2 35.156 (35.156)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (26.562)\n",
      "Training epoch: [279/2000][1/4]; Loss: 2.408146563916372\n",
      "Training Accuracy Epoch: [278]\tPrec@1 15.625 (15.625)\tPrec@2 29.688 (29.688)\n",
      "===>>>\tPrec@1 (12.500)\tPrec@2 (25.781)\n",
      "Training epoch: [280/2000][1/4]; Loss: 2.3807922131140966\n",
      "Training Accuracy Epoch: [279]\tPrec@1 24.219 (24.219)\tPrec@2 32.031 (32.031)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (33.594)\n",
      "Training epoch: [281/2000][1/4]; Loss: 2.3400461118490137\n",
      "Training Accuracy Epoch: [280]\tPrec@1 25.781 (25.781)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (22.656)\n",
      "Training epoch: [282/2000][1/4]; Loss: 2.3618462045393\n",
      "Training Accuracy Epoch: [281]\tPrec@1 25.000 (25.000)\tPrec@2 35.938 (35.938)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (29.688)\n",
      "Training epoch: [283/2000][1/4]; Loss: 2.354400525170915\n",
      "Training Accuracy Epoch: [282]\tPrec@1 25.781 (25.781)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (25.781)\n",
      "Training epoch: [284/2000][1/4]; Loss: 2.376303841319505\n",
      "Training Accuracy Epoch: [283]\tPrec@1 21.094 (21.094)\tPrec@2 35.156 (35.156)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (26.562)\n",
      "Training epoch: [285/2000][1/4]; Loss: 2.35179302580943\n",
      "Training Accuracy Epoch: [284]\tPrec@1 24.219 (24.219)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (25.781)\n",
      "Training epoch: [286/2000][1/4]; Loss: 2.380039824400221\n",
      "Training Accuracy Epoch: [285]\tPrec@1 20.312 (20.312)\tPrec@2 35.156 (35.156)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (28.906)\n",
      "Training epoch: [287/2000][1/4]; Loss: 2.3644863768497655\n",
      "Training Accuracy Epoch: [286]\tPrec@1 26.562 (26.562)\tPrec@2 38.281 (38.281)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.906)\n",
      "Training epoch: [288/2000][1/4]; Loss: 2.3477061106880246\n",
      "Training Accuracy Epoch: [287]\tPrec@1 27.344 (27.344)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (28.125)\n",
      "Training epoch: [289/2000][1/4]; Loss: 2.3694183747997277\n",
      "Training Accuracy Epoch: [288]\tPrec@1 22.656 (22.656)\tPrec@2 35.938 (35.938)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.031)\n",
      "Training epoch: [290/2000][1/4]; Loss: 2.354620523260503\n",
      "Training Accuracy Epoch: [289]\tPrec@1 26.562 (26.562)\tPrec@2 36.719 (36.719)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (24.219)\n",
      "Training epoch: [291/2000][1/4]; Loss: 2.352891975400953\n",
      "Training Accuracy Epoch: [290]\tPrec@1 25.781 (25.781)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (25.781)\n",
      "Training epoch: [292/2000][1/4]; Loss: 2.3324599720536825\n",
      "Training Accuracy Epoch: [291]\tPrec@1 28.906 (28.906)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (26.562)\n",
      "Training epoch: [293/2000][1/4]; Loss: 2.346370837813483\n",
      "Training Accuracy Epoch: [292]\tPrec@1 27.344 (27.344)\tPrec@2 39.844 (39.844)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (25.781)\n",
      "Training epoch: [294/2000][1/4]; Loss: 2.35964189891402\n",
      "Training Accuracy Epoch: [293]\tPrec@1 25.000 (25.000)\tPrec@2 33.594 (33.594)\n",
      "===>>>\tPrec@1 (14.062)\tPrec@2 (19.531)\n",
      "Training epoch: [295/2000][1/4]; Loss: 2.3785849047619516\n",
      "Training Accuracy Epoch: [294]\tPrec@1 20.312 (20.312)\tPrec@2 35.156 (35.156)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (29.688)\n",
      "Training epoch: [296/2000][1/4]; Loss: 2.360377155554647\n",
      "Training Accuracy Epoch: [295]\tPrec@1 25.781 (25.781)\tPrec@2 36.719 (36.719)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.906)\n",
      "Training epoch: [297/2000][1/4]; Loss: 2.3808646297108353\n",
      "Training Accuracy Epoch: [296]\tPrec@1 21.875 (21.875)\tPrec@2 32.031 (32.031)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (25.781)\n",
      "Training epoch: [298/2000][1/4]; Loss: 2.359356003232948\n",
      "Training Accuracy Epoch: [297]\tPrec@1 26.562 (26.562)\tPrec@2 33.594 (33.594)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (27.344)\n",
      "Training epoch: [299/2000][1/4]; Loss: 2.345777837012556\n",
      "Training Accuracy Epoch: [298]\tPrec@1 23.438 (23.438)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (24.219)\n",
      "Training epoch: [300/2000][1/4]; Loss: 2.3646071699616362\n",
      "Training Accuracy Epoch: [299]\tPrec@1 24.219 (24.219)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (30.469)\n",
      "Training epoch: [301/2000][1/4]; Loss: 2.3481422935775433\n",
      "Training Accuracy Epoch: [300]\tPrec@1 25.000 (25.000)\tPrec@2 38.281 (38.281)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (30.469)\n",
      "Training epoch: [302/2000][1/4]; Loss: 2.380444937735769\n",
      "Training Accuracy Epoch: [301]\tPrec@1 21.094 (21.094)\tPrec@2 31.250 (31.250)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (30.469)\n",
      "Training epoch: [303/2000][1/4]; Loss: 2.4027072047675713\n",
      "Training Accuracy Epoch: [302]\tPrec@1 15.625 (15.625)\tPrec@2 27.344 (27.344)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (25.000)\n",
      "Training epoch: [304/2000][1/4]; Loss: 2.3395266779627106\n",
      "Training Accuracy Epoch: [303]\tPrec@1 29.688 (29.688)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (31.250)\n",
      "Training epoch: [305/2000][1/4]; Loss: 2.368350362936054\n",
      "Training Accuracy Epoch: [304]\tPrec@1 22.656 (22.656)\tPrec@2 34.375 (34.375)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (26.562)\n",
      "Training epoch: [306/2000][1/4]; Loss: 2.3670117356159737\n",
      "Training Accuracy Epoch: [305]\tPrec@1 21.094 (21.094)\tPrec@2 33.594 (33.594)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (25.781)\n",
      "Training epoch: [307/2000][1/4]; Loss: 2.3452267006832397\n",
      "Training Accuracy Epoch: [306]\tPrec@1 28.125 (28.125)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (28.125)\n",
      "Training epoch: [308/2000][1/4]; Loss: 2.359114452805403\n",
      "Training Accuracy Epoch: [307]\tPrec@1 23.438 (23.438)\tPrec@2 35.938 (35.938)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (30.469)\n",
      "Training epoch: [309/2000][1/4]; Loss: 2.351822083862104\n",
      "Training Accuracy Epoch: [308]\tPrec@1 29.688 (29.688)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (29.688)\n",
      "Training epoch: [310/2000][1/4]; Loss: 2.3247719317723474\n",
      "Training Accuracy Epoch: [309]\tPrec@1 30.469 (30.469)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (27.344)\n",
      "Training epoch: [311/2000][1/4]; Loss: 2.347377361049571\n",
      "Training Accuracy Epoch: [310]\tPrec@1 28.125 (28.125)\tPrec@2 38.281 (38.281)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (24.219)\n",
      "Training epoch: [312/2000][1/4]; Loss: 2.378043982934348\n",
      "Training Accuracy Epoch: [311]\tPrec@1 25.000 (25.000)\tPrec@2 39.844 (39.844)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (26.562)\n",
      "Training epoch: [313/2000][1/4]; Loss: 2.3309281887958044\n",
      "Training Accuracy Epoch: [312]\tPrec@1 25.781 (25.781)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.906)\n",
      "Training epoch: [314/2000][1/4]; Loss: 2.3401979038156178\n",
      "Training Accuracy Epoch: [313]\tPrec@1 23.438 (23.438)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (27.344)\n",
      "Training epoch: [315/2000][1/4]; Loss: 2.387840513975838\n",
      "Training Accuracy Epoch: [314]\tPrec@1 19.531 (19.531)\tPrec@2 30.469 (30.469)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (28.906)\n",
      "Training epoch: [316/2000][1/4]; Loss: 2.3389622949265974\n",
      "Training Accuracy Epoch: [315]\tPrec@1 30.469 (30.469)\tPrec@2 39.844 (39.844)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (26.562)\n",
      "Training epoch: [317/2000][1/4]; Loss: 2.3508933047906337\n",
      "Training Accuracy Epoch: [316]\tPrec@1 28.125 (28.125)\tPrec@2 39.844 (39.844)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (28.125)\n",
      "Training epoch: [318/2000][1/4]; Loss: 2.3225334453129047\n",
      "Training Accuracy Epoch: [317]\tPrec@1 31.250 (31.250)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (29.688)\n",
      "Training epoch: [319/2000][1/4]; Loss: 2.3382296368471893\n",
      "Training Accuracy Epoch: [318]\tPrec@1 26.562 (26.562)\tPrec@2 39.844 (39.844)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (29.688)\n",
      "Training epoch: [320/2000][1/4]; Loss: 2.350769314277386\n",
      "Training Accuracy Epoch: [319]\tPrec@1 25.000 (25.000)\tPrec@2 35.938 (35.938)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.906)\n",
      "Training epoch: [321/2000][1/4]; Loss: 2.36003078352624\n",
      "Training Accuracy Epoch: [320]\tPrec@1 24.219 (24.219)\tPrec@2 35.156 (35.156)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (24.219)\n",
      "Training epoch: [322/2000][1/4]; Loss: 2.3295527195166827\n",
      "Training Accuracy Epoch: [321]\tPrec@1 29.688 (29.688)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (26.562)\n",
      "Training epoch: [323/2000][1/4]; Loss: 2.336877651769494\n",
      "Training Accuracy Epoch: [322]\tPrec@1 26.562 (26.562)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (26.562)\n",
      "Training epoch: [324/2000][1/4]; Loss: 2.352940255905011\n",
      "Training Accuracy Epoch: [323]\tPrec@1 25.781 (25.781)\tPrec@2 36.719 (36.719)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (28.125)\n",
      "Training epoch: [325/2000][1/4]; Loss: 2.356853455367372\n",
      "Training Accuracy Epoch: [324]\tPrec@1 27.344 (27.344)\tPrec@2 39.844 (39.844)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (27.344)\n",
      "Training epoch: [326/2000][1/4]; Loss: 2.3493920250412734\n",
      "Training Accuracy Epoch: [325]\tPrec@1 30.469 (30.469)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (27.344)\n",
      "Training epoch: [327/2000][1/4]; Loss: 2.3532951782173224\n",
      "Training Accuracy Epoch: [326]\tPrec@1 25.781 (25.781)\tPrec@2 36.719 (36.719)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (25.000)\n",
      "Training epoch: [328/2000][1/4]; Loss: 2.334380750751754\n",
      "Training Accuracy Epoch: [327]\tPrec@1 30.469 (30.469)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (26.562)\n",
      "Training epoch: [329/2000][1/4]; Loss: 2.344160979897508\n",
      "Training Accuracy Epoch: [328]\tPrec@1 24.219 (24.219)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (29.688)\n",
      "Training epoch: [330/2000][1/4]; Loss: 2.319425849252661\n",
      "Training Accuracy Epoch: [329]\tPrec@1 32.812 (32.812)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (25.781)\n",
      "Training epoch: [331/2000][1/4]; Loss: 2.345658117556979\n",
      "Training Accuracy Epoch: [330]\tPrec@1 26.562 (26.562)\tPrec@2 38.281 (38.281)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (25.000)\n",
      "Training epoch: [332/2000][1/4]; Loss: 2.3386078855529724\n",
      "Training Accuracy Epoch: [331]\tPrec@1 32.031 (32.031)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (31.250)\n",
      "Training epoch: [333/2000][1/4]; Loss: 2.3587303700258637\n",
      "Training Accuracy Epoch: [332]\tPrec@1 26.562 (26.562)\tPrec@2 35.156 (35.156)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (26.562)\n",
      "Training epoch: [334/2000][1/4]; Loss: 2.3340210014506213\n",
      "Training Accuracy Epoch: [333]\tPrec@1 25.781 (25.781)\tPrec@2 39.844 (39.844)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.906)\n",
      "Training epoch: [335/2000][1/4]; Loss: 2.34916594059311\n",
      "Training Accuracy Epoch: [334]\tPrec@1 25.000 (25.000)\tPrec@2 33.594 (33.594)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>>>\tPrec@1 (19.531)\tPrec@2 (26.562)\n",
      "Training epoch: [336/2000][1/4]; Loss: 2.3346334256239922\n",
      "Training Accuracy Epoch: [335]\tPrec@1 30.469 (30.469)\tPrec@2 39.844 (39.844)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (28.906)\n",
      "Training epoch: [337/2000][1/4]; Loss: 2.315022463855808\n",
      "Training Accuracy Epoch: [336]\tPrec@1 31.250 (31.250)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (33.594)\n",
      "Training epoch: [338/2000][1/4]; Loss: 2.3358752603839865\n",
      "Training Accuracy Epoch: [337]\tPrec@1 28.906 (28.906)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (26.562)\n",
      "Training epoch: [339/2000][1/4]; Loss: 2.3456631229612803\n",
      "Training Accuracy Epoch: [338]\tPrec@1 27.344 (27.344)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (27.344)\n",
      "Training epoch: [340/2000][1/4]; Loss: 2.3270346114404004\n",
      "Training Accuracy Epoch: [339]\tPrec@1 30.469 (30.469)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (27.344)\n",
      "Training epoch: [341/2000][1/4]; Loss: 2.326263288696218\n",
      "Training Accuracy Epoch: [340]\tPrec@1 28.906 (28.906)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (27.344)\n",
      "Training epoch: [342/2000][1/4]; Loss: 2.327065969006265\n",
      "Training Accuracy Epoch: [341]\tPrec@1 28.125 (28.125)\tPrec@2 35.156 (35.156)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (26.562)\n",
      "Training epoch: [343/2000][1/4]; Loss: 2.3390494710994303\n",
      "Training Accuracy Epoch: [342]\tPrec@1 26.562 (26.562)\tPrec@2 38.281 (38.281)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (25.781)\n",
      "Training epoch: [344/2000][1/4]; Loss: 2.3133293279815272\n",
      "Training Accuracy Epoch: [343]\tPrec@1 34.375 (34.375)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (26.562)\n",
      "Training epoch: [345/2000][1/4]; Loss: 2.3363913210582243\n",
      "Training Accuracy Epoch: [344]\tPrec@1 25.000 (25.000)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (26.562)\n",
      "Training epoch: [346/2000][1/4]; Loss: 2.3572954472454377\n",
      "Training Accuracy Epoch: [345]\tPrec@1 25.000 (25.000)\tPrec@2 33.594 (33.594)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (29.688)\n",
      "Training epoch: [347/2000][1/4]; Loss: 2.3197796137455\n",
      "Training Accuracy Epoch: [346]\tPrec@1 29.688 (29.688)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.125)\n",
      "Training epoch: [348/2000][1/4]; Loss: 2.332032383663792\n",
      "Training Accuracy Epoch: [347]\tPrec@1 28.906 (28.906)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (25.781)\n",
      "Training epoch: [349/2000][1/4]; Loss: 2.32968531598569\n",
      "Training Accuracy Epoch: [348]\tPrec@1 27.344 (27.344)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (27.344)\n",
      "Training epoch: [350/2000][1/4]; Loss: 2.3460833000194614\n",
      "Training Accuracy Epoch: [349]\tPrec@1 28.906 (28.906)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (26.562)\n",
      "Training epoch: [351/2000][1/4]; Loss: 2.3205129608514206\n",
      "Training Accuracy Epoch: [350]\tPrec@1 28.906 (28.906)\tPrec@2 39.844 (39.844)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (28.906)\n",
      "Training epoch: [352/2000][1/4]; Loss: 2.3128204190486428\n",
      "Training Accuracy Epoch: [351]\tPrec@1 29.688 (29.688)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (31.250)\n",
      "Training epoch: [353/2000][1/4]; Loss: 2.327433390032883\n",
      "Training Accuracy Epoch: [352]\tPrec@1 31.250 (31.250)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (30.469)\n",
      "Training epoch: [354/2000][1/4]; Loss: 2.326697386981583\n",
      "Training Accuracy Epoch: [353]\tPrec@1 25.781 (25.781)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (29.688)\n",
      "Training epoch: [355/2000][1/4]; Loss: 2.3073175144900215\n",
      "Training Accuracy Epoch: [354]\tPrec@1 30.469 (30.469)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (12.500)\tPrec@2 (21.875)\n",
      "Training epoch: [356/2000][1/4]; Loss: 2.3180235571073204\n",
      "Training Accuracy Epoch: [355]\tPrec@1 32.031 (32.031)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (25.781)\n",
      "Training epoch: [357/2000][1/4]; Loss: 2.325308036216069\n",
      "Training Accuracy Epoch: [356]\tPrec@1 27.344 (27.344)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (25.781)\n",
      "Training epoch: [358/2000][1/4]; Loss: 2.34165866842882\n",
      "Training Accuracy Epoch: [357]\tPrec@1 25.781 (25.781)\tPrec@2 38.281 (38.281)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.125)\n",
      "Training epoch: [359/2000][1/4]; Loss: 2.3305831399982644\n",
      "Training Accuracy Epoch: [358]\tPrec@1 28.906 (28.906)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (24.219)\n",
      "Training epoch: [360/2000][1/4]; Loss: 2.297487766153633\n",
      "Training Accuracy Epoch: [359]\tPrec@1 36.719 (36.719)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (25.000)\n",
      "Training epoch: [361/2000][1/4]; Loss: 2.328334489006108\n",
      "Training Accuracy Epoch: [360]\tPrec@1 31.250 (31.250)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.906)\n",
      "Training epoch: [362/2000][1/4]; Loss: 2.3198418444786335\n",
      "Training Accuracy Epoch: [361]\tPrec@1 28.906 (28.906)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (27.344)\n",
      "Training epoch: [363/2000][1/4]; Loss: 2.300606656654373\n",
      "Training Accuracy Epoch: [362]\tPrec@1 32.812 (32.812)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (28.906)\n",
      "Training epoch: [364/2000][1/4]; Loss: 2.325723201009436\n",
      "Training Accuracy Epoch: [363]\tPrec@1 28.125 (28.125)\tPrec@2 38.281 (38.281)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (27.344)\n",
      "Training epoch: [365/2000][1/4]; Loss: 2.3474291727022045\n",
      "Training Accuracy Epoch: [364]\tPrec@1 25.781 (25.781)\tPrec@2 34.375 (34.375)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (24.219)\n",
      "Training epoch: [366/2000][1/4]; Loss: 2.335120652338468\n",
      "Training Accuracy Epoch: [365]\tPrec@1 27.344 (27.344)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (30.469)\n",
      "Training epoch: [367/2000][1/4]; Loss: 2.3353762507804494\n",
      "Training Accuracy Epoch: [366]\tPrec@1 26.562 (26.562)\tPrec@2 35.938 (35.938)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (27.344)\n",
      "Training epoch: [368/2000][1/4]; Loss: 2.3260539447022746\n",
      "Training Accuracy Epoch: [367]\tPrec@1 23.438 (23.438)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (28.906)\n",
      "Training epoch: [369/2000][1/4]; Loss: 2.336362465158846\n",
      "Training Accuracy Epoch: [368]\tPrec@1 32.812 (32.812)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.125)\n",
      "Training epoch: [370/2000][1/4]; Loss: 2.3242850275641027\n",
      "Training Accuracy Epoch: [369]\tPrec@1 25.781 (25.781)\tPrec@2 39.844 (39.844)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (27.344)\n",
      "Training epoch: [371/2000][1/4]; Loss: 2.3339540091503994\n",
      "Training Accuracy Epoch: [370]\tPrec@1 25.781 (25.781)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (26.562)\n",
      "Training epoch: [372/2000][1/4]; Loss: 2.314484542765867\n",
      "Training Accuracy Epoch: [371]\tPrec@1 33.594 (33.594)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.906)\n",
      "Training epoch: [373/2000][1/4]; Loss: 2.3199139507429236\n",
      "Training Accuracy Epoch: [372]\tPrec@1 32.812 (32.812)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (29.688)\n",
      "Training epoch: [374/2000][1/4]; Loss: 2.315018397257702\n",
      "Training Accuracy Epoch: [373]\tPrec@1 30.469 (30.469)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.906)\n",
      "Training epoch: [375/2000][1/4]; Loss: 2.3097197826915927\n",
      "Training Accuracy Epoch: [374]\tPrec@1 31.250 (31.250)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (25.781)\n",
      "Training epoch: [376/2000][1/4]; Loss: 2.3116853254594747\n",
      "Training Accuracy Epoch: [375]\tPrec@1 34.375 (34.375)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (26.562)\n",
      "Training epoch: [377/2000][1/4]; Loss: 2.307740643515824\n",
      "Training Accuracy Epoch: [376]\tPrec@1 35.938 (35.938)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (28.125)\n",
      "Training epoch: [378/2000][1/4]; Loss: 2.3127064988987733\n",
      "Training Accuracy Epoch: [377]\tPrec@1 29.688 (29.688)\tPrec@2 39.844 (39.844)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (24.219)\n",
      "Training epoch: [379/2000][1/4]; Loss: 2.3285693238261236\n",
      "Training Accuracy Epoch: [378]\tPrec@1 28.906 (28.906)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.031)\n",
      "Training epoch: [380/2000][1/4]; Loss: 2.313786667144386\n",
      "Training Accuracy Epoch: [379]\tPrec@1 34.375 (34.375)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (29.688)\n",
      "Training epoch: [381/2000][1/4]; Loss: 2.3313302349803093\n",
      "Training Accuracy Epoch: [380]\tPrec@1 28.906 (28.906)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (31.250)\n",
      "Training epoch: [382/2000][1/4]; Loss: 2.3245199030078254\n",
      "Training Accuracy Epoch: [381]\tPrec@1 30.469 (30.469)\tPrec@2 39.844 (39.844)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.906)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: [383/2000][1/4]; Loss: 2.3008254794427487\n",
      "Training Accuracy Epoch: [382]\tPrec@1 31.250 (31.250)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.906)\n",
      "Training epoch: [384/2000][1/4]; Loss: 2.301598374097062\n",
      "Training Accuracy Epoch: [383]\tPrec@1 29.688 (29.688)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (28.125)\n",
      "Training epoch: [385/2000][1/4]; Loss: 2.315857407733225\n",
      "Training Accuracy Epoch: [384]\tPrec@1 31.250 (31.250)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (26.562)\n",
      "Training epoch: [386/2000][1/4]; Loss: 2.3344304686216395\n",
      "Training Accuracy Epoch: [385]\tPrec@1 25.000 (25.000)\tPrec@2 39.844 (39.844)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (32.031)\n",
      "Training epoch: [387/2000][1/4]; Loss: 2.315571417701945\n",
      "Training Accuracy Epoch: [386]\tPrec@1 28.906 (28.906)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (28.906)\n",
      "Training epoch: [388/2000][1/4]; Loss: 2.3165572850396887\n",
      "Training Accuracy Epoch: [387]\tPrec@1 32.031 (32.031)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (26.562)\n",
      "Training epoch: [389/2000][1/4]; Loss: 2.3237136815233983\n",
      "Training Accuracy Epoch: [388]\tPrec@1 28.125 (28.125)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (27.344)\n",
      "Training epoch: [390/2000][1/4]; Loss: 2.3308324322690495\n",
      "Training Accuracy Epoch: [389]\tPrec@1 27.344 (27.344)\tPrec@2 36.719 (36.719)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.906)\n",
      "Training epoch: [391/2000][1/4]; Loss: 2.314977188834808\n",
      "Training Accuracy Epoch: [390]\tPrec@1 32.812 (32.812)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (23.438)\n",
      "Training epoch: [392/2000][1/4]; Loss: 2.3375402853469103\n",
      "Training Accuracy Epoch: [391]\tPrec@1 22.656 (22.656)\tPrec@2 35.156 (35.156)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (22.656)\n",
      "Training epoch: [393/2000][1/4]; Loss: 2.3380578313711813\n",
      "Training Accuracy Epoch: [392]\tPrec@1 28.125 (28.125)\tPrec@2 38.281 (38.281)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (31.250)\n",
      "Training epoch: [394/2000][1/4]; Loss: 2.302433415070074\n",
      "Training Accuracy Epoch: [393]\tPrec@1 32.812 (32.812)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (28.906)\n",
      "Training epoch: [395/2000][1/4]; Loss: 2.31865192342266\n",
      "Training Accuracy Epoch: [394]\tPrec@1 32.031 (32.031)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (33.594)\n",
      "Training epoch: [396/2000][1/4]; Loss: 2.319211610979361\n",
      "Training Accuracy Epoch: [395]\tPrec@1 30.469 (30.469)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (27.344)\n",
      "Training epoch: [397/2000][1/4]; Loss: 2.3392780020832467\n",
      "Training Accuracy Epoch: [396]\tPrec@1 25.000 (25.000)\tPrec@2 36.719 (36.719)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (25.781)\n",
      "Training epoch: [398/2000][1/4]; Loss: 2.2940656837176934\n",
      "Training Accuracy Epoch: [397]\tPrec@1 32.812 (32.812)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (28.906)\n",
      "Training epoch: [399/2000][1/4]; Loss: 2.3388966814984635\n",
      "Training Accuracy Epoch: [398]\tPrec@1 27.344 (27.344)\tPrec@2 36.719 (36.719)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (31.250)\n",
      "Training epoch: [400/2000][1/4]; Loss: 2.297528062940099\n",
      "Training Accuracy Epoch: [399]\tPrec@1 30.469 (30.469)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (28.125)\n",
      "Training epoch: [401/2000][1/4]; Loss: 2.304756105900822\n",
      "Training Accuracy Epoch: [400]\tPrec@1 30.469 (30.469)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (30.469)\n",
      "Training epoch: [402/2000][1/4]; Loss: 2.2998220672695\n",
      "Training Accuracy Epoch: [401]\tPrec@1 34.375 (34.375)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (27.344)\n",
      "Training epoch: [403/2000][1/4]; Loss: 2.3101505796827255\n",
      "Training Accuracy Epoch: [402]\tPrec@1 30.469 (30.469)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (29.688)\n",
      "Training epoch: [404/2000][1/4]; Loss: 2.3149503895193426\n",
      "Training Accuracy Epoch: [403]\tPrec@1 29.688 (29.688)\tPrec@2 39.844 (39.844)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (29.688)\n",
      "Training epoch: [405/2000][1/4]; Loss: 2.3066149072873743\n",
      "Training Accuracy Epoch: [404]\tPrec@1 31.250 (31.250)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (28.906)\n",
      "Training epoch: [406/2000][1/4]; Loss: 2.3181018585234243\n",
      "Training Accuracy Epoch: [405]\tPrec@1 29.688 (29.688)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (29.688)\n",
      "Training epoch: [407/2000][1/4]; Loss: 2.30385081332089\n",
      "Training Accuracy Epoch: [406]\tPrec@1 32.812 (32.812)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (30.469)\n",
      "Training epoch: [408/2000][1/4]; Loss: 2.3156118225461704\n",
      "Training Accuracy Epoch: [407]\tPrec@1 28.906 (28.906)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (29.688)\n",
      "Training epoch: [409/2000][1/4]; Loss: 2.328178488768073\n",
      "Training Accuracy Epoch: [408]\tPrec@1 28.125 (28.125)\tPrec@2 38.281 (38.281)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.125)\n",
      "Training epoch: [410/2000][1/4]; Loss: 2.317240666616494\n",
      "Training Accuracy Epoch: [409]\tPrec@1 28.906 (28.906)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (26.562)\n",
      "Training epoch: [411/2000][1/4]; Loss: 2.30955284912586\n",
      "Training Accuracy Epoch: [410]\tPrec@1 31.250 (31.250)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (26.562)\n",
      "Training epoch: [412/2000][1/4]; Loss: 2.3121457420147524\n",
      "Training Accuracy Epoch: [411]\tPrec@1 27.344 (27.344)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (25.000)\n",
      "Training epoch: [413/2000][1/4]; Loss: 2.3372681878676427\n",
      "Training Accuracy Epoch: [412]\tPrec@1 25.000 (25.000)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (27.344)\n",
      "Training epoch: [414/2000][1/4]; Loss: 2.3006001182152733\n",
      "Training Accuracy Epoch: [413]\tPrec@1 30.469 (30.469)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (26.562)\n",
      "Training epoch: [415/2000][1/4]; Loss: 2.3058284244417733\n",
      "Training Accuracy Epoch: [414]\tPrec@1 32.812 (32.812)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (26.562)\n",
      "Training epoch: [416/2000][1/4]; Loss: 2.2866427774529225\n",
      "Training Accuracy Epoch: [415]\tPrec@1 36.719 (36.719)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (31.250)\n",
      "Training epoch: [417/2000][1/4]; Loss: 2.297233279954517\n",
      "Training Accuracy Epoch: [416]\tPrec@1 34.375 (34.375)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.906)\n",
      "Training epoch: [418/2000][1/4]; Loss: 2.3031556366290147\n",
      "Training Accuracy Epoch: [417]\tPrec@1 35.938 (35.938)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (29.688)\n",
      "Training epoch: [419/2000][1/4]; Loss: 2.301084350755655\n",
      "Training Accuracy Epoch: [418]\tPrec@1 27.344 (27.344)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (25.781)\n",
      "Training epoch: [420/2000][1/4]; Loss: 2.2993660357971892\n",
      "Training Accuracy Epoch: [419]\tPrec@1 28.906 (28.906)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (26.562)\n",
      "Training epoch: [421/2000][1/4]; Loss: 2.269484409677946\n",
      "Training Accuracy Epoch: [420]\tPrec@1 36.719 (36.719)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.906)\n",
      "Training epoch: [422/2000][1/4]; Loss: 2.3318471614000353\n",
      "Training Accuracy Epoch: [421]\tPrec@1 26.562 (26.562)\tPrec@2 34.375 (34.375)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (32.031)\n",
      "Training epoch: [423/2000][1/4]; Loss: 2.303105684767405\n",
      "Training Accuracy Epoch: [422]\tPrec@1 32.031 (32.031)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (27.344)\n",
      "Training epoch: [424/2000][1/4]; Loss: 2.340062086930145\n",
      "Training Accuracy Epoch: [423]\tPrec@1 27.344 (27.344)\tPrec@2 34.375 (34.375)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (29.688)\n",
      "Training epoch: [425/2000][1/4]; Loss: 2.289366050373856\n",
      "Training Accuracy Epoch: [424]\tPrec@1 36.719 (36.719)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (25.781)\n",
      "Training epoch: [426/2000][1/4]; Loss: 2.3112384092665232\n",
      "Training Accuracy Epoch: [425]\tPrec@1 29.688 (29.688)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.906)\n",
      "Training epoch: [427/2000][1/4]; Loss: 2.345514133544868\n",
      "Training Accuracy Epoch: [426]\tPrec@1 25.781 (25.781)\tPrec@2 32.031 (32.031)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (28.906)\n",
      "Training epoch: [428/2000][1/4]; Loss: 2.2894843557538334\n",
      "Training Accuracy Epoch: [427]\tPrec@1 34.375 (34.375)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.125)\n",
      "Training epoch: [429/2000][1/4]; Loss: 2.309100272575427\n",
      "Training Accuracy Epoch: [428]\tPrec@1 32.031 (32.031)\tPrec@2 39.844 (39.844)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (28.906)\n",
      "Training epoch: [430/2000][1/4]; Loss: 2.3312728003458334\n",
      "Training Accuracy Epoch: [429]\tPrec@1 25.000 (25.000)\tPrec@2 33.594 (33.594)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>>>\tPrec@1 (14.844)\tPrec@2 (27.344)\n",
      "Training epoch: [431/2000][1/4]; Loss: 2.3119597314430917\n",
      "Training Accuracy Epoch: [430]\tPrec@1 32.031 (32.031)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (27.344)\n",
      "Training epoch: [432/2000][1/4]; Loss: 2.304569411103271\n",
      "Training Accuracy Epoch: [431]\tPrec@1 32.812 (32.812)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (29.688)\n",
      "Training epoch: [433/2000][1/4]; Loss: 2.3232498060104336\n",
      "Training Accuracy Epoch: [432]\tPrec@1 29.688 (29.688)\tPrec@2 38.281 (38.281)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (30.469)\n",
      "Training epoch: [434/2000][1/4]; Loss: 2.3030287832054643\n",
      "Training Accuracy Epoch: [433]\tPrec@1 31.250 (31.250)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (28.125)\n",
      "Training epoch: [435/2000][1/4]; Loss: 2.311534363808246\n",
      "Training Accuracy Epoch: [434]\tPrec@1 26.562 (26.562)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (29.688)\n",
      "Training epoch: [436/2000][1/4]; Loss: 2.318764471093626\n",
      "Training Accuracy Epoch: [435]\tPrec@1 29.688 (29.688)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (27.344)\n",
      "Training epoch: [437/2000][1/4]; Loss: 2.3232460021812025\n",
      "Training Accuracy Epoch: [436]\tPrec@1 28.125 (28.125)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.906)\n",
      "Training epoch: [438/2000][1/4]; Loss: 2.3036474084171528\n",
      "Training Accuracy Epoch: [437]\tPrec@1 28.125 (28.125)\tPrec@2 38.281 (38.281)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (28.125)\n",
      "Training epoch: [439/2000][1/4]; Loss: 2.2859562426296653\n",
      "Training Accuracy Epoch: [438]\tPrec@1 34.375 (34.375)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (26.562)\n",
      "Training epoch: [440/2000][1/4]; Loss: 2.300465003504045\n",
      "Training Accuracy Epoch: [439]\tPrec@1 33.594 (33.594)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (31.250)\n",
      "Training epoch: [441/2000][1/4]; Loss: 2.277857359845759\n",
      "Training Accuracy Epoch: [440]\tPrec@1 35.156 (35.156)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.125)\n",
      "Training epoch: [442/2000][1/4]; Loss: 2.331518766860362\n",
      "Training Accuracy Epoch: [441]\tPrec@1 25.781 (25.781)\tPrec@2 35.156 (35.156)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (30.469)\n",
      "Training epoch: [443/2000][1/4]; Loss: 2.3110535958754164\n",
      "Training Accuracy Epoch: [442]\tPrec@1 26.562 (26.562)\tPrec@2 39.844 (39.844)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (25.000)\n",
      "Training epoch: [444/2000][1/4]; Loss: 2.3290447124714597\n",
      "Training Accuracy Epoch: [443]\tPrec@1 29.688 (29.688)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (30.469)\n",
      "Training epoch: [445/2000][1/4]; Loss: 2.288987240435867\n",
      "Training Accuracy Epoch: [444]\tPrec@1 31.250 (31.250)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.125)\n",
      "Training epoch: [446/2000][1/4]; Loss: 2.277275736360817\n",
      "Training Accuracy Epoch: [445]\tPrec@1 35.938 (35.938)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (23.438)\n",
      "Training epoch: [447/2000][1/4]; Loss: 2.2635115575679845\n",
      "Training Accuracy Epoch: [446]\tPrec@1 36.719 (36.719)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (28.125)\n",
      "Training epoch: [448/2000][1/4]; Loss: 2.274134142951549\n",
      "Training Accuracy Epoch: [447]\tPrec@1 35.938 (35.938)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (26.562)\n",
      "Training epoch: [449/2000][1/4]; Loss: 2.2761084125276425\n",
      "Training Accuracy Epoch: [448]\tPrec@1 33.594 (33.594)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (14.062)\tPrec@2 (27.344)\n",
      "Training epoch: [450/2000][1/4]; Loss: 2.273213021824102\n",
      "Training Accuracy Epoch: [449]\tPrec@1 33.594 (33.594)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.125)\n",
      "Training epoch: [451/2000][1/4]; Loss: 2.2775992050120664\n",
      "Training Accuracy Epoch: [450]\tPrec@1 32.812 (32.812)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (25.781)\n",
      "Training epoch: [452/2000][1/4]; Loss: 2.2875511297229263\n",
      "Training Accuracy Epoch: [451]\tPrec@1 35.156 (35.156)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (26.562)\n",
      "Training epoch: [453/2000][1/4]; Loss: 2.2828105006250885\n",
      "Training Accuracy Epoch: [452]\tPrec@1 32.812 (32.812)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (30.469)\n",
      "Training epoch: [454/2000][1/4]; Loss: 2.2761280019803425\n",
      "Training Accuracy Epoch: [453]\tPrec@1 38.281 (38.281)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (30.469)\n",
      "Training epoch: [455/2000][1/4]; Loss: 2.3011834350349716\n",
      "Training Accuracy Epoch: [454]\tPrec@1 30.469 (30.469)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (28.906)\n",
      "Training epoch: [456/2000][1/4]; Loss: 2.2961996792634816\n",
      "Training Accuracy Epoch: [455]\tPrec@1 28.906 (28.906)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (26.562)\n",
      "Training epoch: [457/2000][1/4]; Loss: 2.27645483227457\n",
      "Training Accuracy Epoch: [456]\tPrec@1 32.031 (32.031)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (30.469)\n",
      "Training epoch: [458/2000][1/4]; Loss: 2.297303138677286\n",
      "Training Accuracy Epoch: [457]\tPrec@1 30.469 (30.469)\tPrec@2 39.844 (39.844)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (27.344)\n",
      "Training epoch: [459/2000][1/4]; Loss: 2.29090947599196\n",
      "Training Accuracy Epoch: [458]\tPrec@1 32.031 (32.031)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (29.688)\n",
      "Training epoch: [460/2000][1/4]; Loss: 2.30567441624976\n",
      "Training Accuracy Epoch: [459]\tPrec@1 28.125 (28.125)\tPrec@2 35.156 (35.156)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.906)\n",
      "Training epoch: [461/2000][1/4]; Loss: 2.2888428858306202\n",
      "Training Accuracy Epoch: [460]\tPrec@1 33.594 (33.594)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (28.125)\n",
      "Training epoch: [462/2000][1/4]; Loss: 2.279750870132683\n",
      "Training Accuracy Epoch: [461]\tPrec@1 35.938 (35.938)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (25.781)\n",
      "Training epoch: [463/2000][1/4]; Loss: 2.3238306972664087\n",
      "Training Accuracy Epoch: [462]\tPrec@1 22.656 (22.656)\tPrec@2 34.375 (34.375)\n",
      "===>>>\tPrec@1 (25.000)\tPrec@2 (29.688)\n",
      "Training epoch: [464/2000][1/4]; Loss: 2.2819470611684944\n",
      "Training Accuracy Epoch: [463]\tPrec@1 32.031 (32.031)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (26.562)\n",
      "Training epoch: [465/2000][1/4]; Loss: 2.26936905104989\n",
      "Training Accuracy Epoch: [464]\tPrec@1 34.375 (34.375)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (28.906)\n",
      "Training epoch: [466/2000][1/4]; Loss: 2.281805916227175\n",
      "Training Accuracy Epoch: [465]\tPrec@1 30.469 (30.469)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (13.281)\tPrec@2 (25.000)\n",
      "Training epoch: [467/2000][1/4]; Loss: 2.272745937462498\n",
      "Training Accuracy Epoch: [466]\tPrec@1 35.938 (35.938)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (30.469)\n",
      "Training epoch: [468/2000][1/4]; Loss: 2.2634763102821625\n",
      "Training Accuracy Epoch: [467]\tPrec@1 37.500 (37.500)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (30.469)\n",
      "Training epoch: [469/2000][1/4]; Loss: 2.271302683763791\n",
      "Training Accuracy Epoch: [468]\tPrec@1 31.250 (31.250)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (13.281)\tPrec@2 (21.875)\n",
      "Training epoch: [470/2000][1/4]; Loss: 2.2819834725208024\n",
      "Training Accuracy Epoch: [469]\tPrec@1 33.594 (33.594)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (27.344)\n",
      "Training epoch: [471/2000][1/4]; Loss: 2.279843492107675\n",
      "Training Accuracy Epoch: [470]\tPrec@1 34.375 (34.375)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (29.688)\n",
      "Training epoch: [472/2000][1/4]; Loss: 2.3037943419485467\n",
      "Training Accuracy Epoch: [471]\tPrec@1 31.250 (31.250)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (31.250)\n",
      "Training epoch: [473/2000][1/4]; Loss: 2.2942392260145135\n",
      "Training Accuracy Epoch: [472]\tPrec@1 28.906 (28.906)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (29.688)\n",
      "Training epoch: [474/2000][1/4]; Loss: 2.2815501862439147\n",
      "Training Accuracy Epoch: [473]\tPrec@1 34.375 (34.375)\tPrec@2 38.281 (38.281)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (29.688)\n",
      "Training epoch: [475/2000][1/4]; Loss: 2.2590015090364925\n",
      "Training Accuracy Epoch: [474]\tPrec@1 39.844 (39.844)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (27.344)\n",
      "Training epoch: [476/2000][1/4]; Loss: 2.301713270609006\n",
      "Training Accuracy Epoch: [475]\tPrec@1 29.688 (29.688)\tPrec@2 38.281 (38.281)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (32.031)\n",
      "Training epoch: [477/2000][1/4]; Loss: 2.2960080244494843\n",
      "Training Accuracy Epoch: [476]\tPrec@1 28.125 (28.125)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (30.469)\n",
      "Training epoch: [478/2000][1/4]; Loss: 2.296063818492751\n",
      "Training Accuracy Epoch: [477]\tPrec@1 31.250 (31.250)\tPrec@2 36.719 (36.719)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>>>\tPrec@1 (20.312)\tPrec@2 (27.344)\n",
      "Training epoch: [479/2000][1/4]; Loss: 2.2795537872269027\n",
      "Training Accuracy Epoch: [478]\tPrec@1 31.250 (31.250)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (27.344)\n",
      "Training epoch: [480/2000][1/4]; Loss: 2.282159910047246\n",
      "Training Accuracy Epoch: [479]\tPrec@1 33.594 (33.594)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (25.000)\n",
      "Training epoch: [481/2000][1/4]; Loss: 2.293487229649253\n",
      "Training Accuracy Epoch: [480]\tPrec@1 28.906 (28.906)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (30.469)\n",
      "Training epoch: [482/2000][1/4]; Loss: 2.263726035138321\n",
      "Training Accuracy Epoch: [481]\tPrec@1 34.375 (34.375)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (22.656)\n",
      "Training epoch: [483/2000][1/4]; Loss: 2.285022514912512\n",
      "Training Accuracy Epoch: [482]\tPrec@1 30.469 (30.469)\tPrec@2 39.844 (39.844)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (28.125)\n",
      "Training epoch: [484/2000][1/4]; Loss: 2.2909085904430584\n",
      "Training Accuracy Epoch: [483]\tPrec@1 25.000 (25.000)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (29.688)\n",
      "Training epoch: [485/2000][1/4]; Loss: 2.2806798505722203\n",
      "Training Accuracy Epoch: [484]\tPrec@1 33.594 (33.594)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (29.688)\n",
      "Training epoch: [486/2000][1/4]; Loss: 2.3103184668719914\n",
      "Training Accuracy Epoch: [485]\tPrec@1 25.000 (25.000)\tPrec@2 32.812 (32.812)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (28.906)\n",
      "Training epoch: [487/2000][1/4]; Loss: 2.2746862973944504\n",
      "Training Accuracy Epoch: [486]\tPrec@1 28.906 (28.906)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (26.562)\n",
      "Training epoch: [488/2000][1/4]; Loss: 2.2877169728498727\n",
      "Training Accuracy Epoch: [487]\tPrec@1 29.688 (29.688)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.031)\n",
      "Training epoch: [489/2000][1/4]; Loss: 2.263434810216786\n",
      "Training Accuracy Epoch: [488]\tPrec@1 35.156 (35.156)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (32.812)\n",
      "Training epoch: [490/2000][1/4]; Loss: 2.2681426830734424\n",
      "Training Accuracy Epoch: [489]\tPrec@1 33.594 (33.594)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (30.469)\n",
      "Training epoch: [491/2000][1/4]; Loss: 2.2782272561891954\n",
      "Training Accuracy Epoch: [490]\tPrec@1 31.250 (31.250)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (29.688)\n",
      "Training epoch: [492/2000][1/4]; Loss: 2.2546469421089252\n",
      "Training Accuracy Epoch: [491]\tPrec@1 36.719 (36.719)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (28.906)\n",
      "Training epoch: [493/2000][1/4]; Loss: 2.2849288673339916\n",
      "Training Accuracy Epoch: [492]\tPrec@1 29.688 (29.688)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (31.250)\n",
      "Training epoch: [494/2000][1/4]; Loss: 2.264991412372145\n",
      "Training Accuracy Epoch: [493]\tPrec@1 34.375 (34.375)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (27.344)\n",
      "Training epoch: [495/2000][1/4]; Loss: 2.2871177592045533\n",
      "Training Accuracy Epoch: [494]\tPrec@1 32.031 (32.031)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (27.344)\n",
      "Training epoch: [496/2000][1/4]; Loss: 2.2503933339100097\n",
      "Training Accuracy Epoch: [495]\tPrec@1 39.844 (39.844)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (30.469)\n",
      "Training epoch: [497/2000][1/4]; Loss: 2.2503871178602743\n",
      "Training Accuracy Epoch: [496]\tPrec@1 41.406 (41.406)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.906)\n",
      "Training epoch: [498/2000][1/4]; Loss: 2.274881406745344\n",
      "Training Accuracy Epoch: [497]\tPrec@1 31.250 (31.250)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (28.906)\n",
      "Training epoch: [499/2000][1/4]; Loss: 2.2565815847989974\n",
      "Training Accuracy Epoch: [498]\tPrec@1 36.719 (36.719)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.125)\n",
      "Training epoch: [500/2000][1/4]; Loss: 2.24785426921879\n",
      "Training Accuracy Epoch: [499]\tPrec@1 37.500 (37.500)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (26.562)\n",
      "Training epoch: [501/2000][1/4]; Loss: 2.2812460333388964\n",
      "Training Accuracy Epoch: [500]\tPrec@1 34.375 (34.375)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (29.688)\n",
      "Training epoch: [502/2000][1/4]; Loss: 2.2955376587257903\n",
      "Training Accuracy Epoch: [501]\tPrec@1 28.906 (28.906)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (35.156)\n",
      "Training epoch: [503/2000][1/4]; Loss: 2.303915417444911\n",
      "Training Accuracy Epoch: [502]\tPrec@1 25.781 (25.781)\tPrec@2 35.938 (35.938)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (29.688)\n",
      "Training epoch: [504/2000][1/4]; Loss: 2.2823590954832143\n",
      "Training Accuracy Epoch: [503]\tPrec@1 30.469 (30.469)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (23.438)\n",
      "Training epoch: [505/2000][1/4]; Loss: 2.259997082145127\n",
      "Training Accuracy Epoch: [504]\tPrec@1 37.500 (37.500)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (28.125)\n",
      "Training epoch: [506/2000][1/4]; Loss: 2.2615815102909087\n",
      "Training Accuracy Epoch: [505]\tPrec@1 32.031 (32.031)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (22.656)\n",
      "Training epoch: [507/2000][1/4]; Loss: 2.267377713930194\n",
      "Training Accuracy Epoch: [506]\tPrec@1 29.688 (29.688)\tPrec@2 38.281 (38.281)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (28.125)\n",
      "Training epoch: [508/2000][1/4]; Loss: 2.2818935859047174\n",
      "Training Accuracy Epoch: [507]\tPrec@1 31.250 (31.250)\tPrec@2 35.938 (35.938)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (25.781)\n",
      "Training epoch: [509/2000][1/4]; Loss: 2.28002583343698\n",
      "Training Accuracy Epoch: [508]\tPrec@1 32.031 (32.031)\tPrec@2 39.844 (39.844)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (28.125)\n",
      "Training epoch: [510/2000][1/4]; Loss: 2.2584229970325294\n",
      "Training Accuracy Epoch: [509]\tPrec@1 37.500 (37.500)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (29.688)\n",
      "Training epoch: [511/2000][1/4]; Loss: 2.285450010922201\n",
      "Training Accuracy Epoch: [510]\tPrec@1 32.812 (32.812)\tPrec@2 38.281 (38.281)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (29.688)\n",
      "Training epoch: [512/2000][1/4]; Loss: 2.298497384319643\n",
      "Training Accuracy Epoch: [511]\tPrec@1 26.562 (26.562)\tPrec@2 32.031 (32.031)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (23.438)\n",
      "Training epoch: [513/2000][1/4]; Loss: 2.2740640324348007\n",
      "Training Accuracy Epoch: [512]\tPrec@1 34.375 (34.375)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (25.781)\n",
      "Training epoch: [514/2000][1/4]; Loss: 2.2744310098159914\n",
      "Training Accuracy Epoch: [513]\tPrec@1 32.031 (32.031)\tPrec@2 39.844 (39.844)\n",
      "===>>>\tPrec@1 (24.219)\tPrec@2 (32.812)\n",
      "Training epoch: [515/2000][1/4]; Loss: 2.288016202677979\n",
      "Training Accuracy Epoch: [514]\tPrec@1 31.250 (31.250)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (25.781)\n",
      "Training epoch: [516/2000][1/4]; Loss: 2.2525561941901135\n",
      "Training Accuracy Epoch: [515]\tPrec@1 35.938 (35.938)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (25.781)\n",
      "Training epoch: [517/2000][1/4]; Loss: 2.2759047911202024\n",
      "Training Accuracy Epoch: [516]\tPrec@1 29.688 (29.688)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (28.125)\n",
      "Training epoch: [518/2000][1/4]; Loss: 2.2762156363756962\n",
      "Training Accuracy Epoch: [517]\tPrec@1 31.250 (31.250)\tPrec@2 38.281 (38.281)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.125)\n",
      "Training epoch: [519/2000][1/4]; Loss: 2.23895076184822\n",
      "Training Accuracy Epoch: [518]\tPrec@1 42.188 (42.188)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (30.469)\n",
      "Training epoch: [520/2000][1/4]; Loss: 2.260622983172981\n",
      "Training Accuracy Epoch: [519]\tPrec@1 35.938 (35.938)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (29.688)\n",
      "Training epoch: [521/2000][1/4]; Loss: 2.2934204310257633\n",
      "Training Accuracy Epoch: [520]\tPrec@1 33.594 (33.594)\tPrec@2 39.844 (39.844)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (30.469)\n",
      "Training epoch: [522/2000][1/4]; Loss: 2.312839081709213\n",
      "Training Accuracy Epoch: [521]\tPrec@1 30.469 (30.469)\tPrec@2 39.844 (39.844)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.906)\n",
      "Training epoch: [523/2000][1/4]; Loss: 2.284505007317795\n",
      "Training Accuracy Epoch: [522]\tPrec@1 32.031 (32.031)\tPrec@2 38.281 (38.281)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (25.781)\n",
      "Training epoch: [524/2000][1/4]; Loss: 2.2821693647342443\n",
      "Training Accuracy Epoch: [523]\tPrec@1 29.688 (29.688)\tPrec@2 35.938 (35.938)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (29.688)\n",
      "Training epoch: [525/2000][1/4]; Loss: 2.281599510240478\n",
      "Training Accuracy Epoch: [524]\tPrec@1 32.812 (32.812)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (25.781)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: [526/2000][1/4]; Loss: 2.2507196266440515\n",
      "Training Accuracy Epoch: [525]\tPrec@1 33.594 (33.594)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (31.250)\n",
      "Training epoch: [527/2000][1/4]; Loss: 2.2507746369320785\n",
      "Training Accuracy Epoch: [526]\tPrec@1 35.156 (35.156)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (30.469)\n",
      "Training epoch: [528/2000][1/4]; Loss: 2.2604616685895076\n",
      "Training Accuracy Epoch: [527]\tPrec@1 33.594 (33.594)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (32.812)\n",
      "Training epoch: [529/2000][1/4]; Loss: 2.2702532256289425\n",
      "Training Accuracy Epoch: [528]\tPrec@1 30.469 (30.469)\tPrec@2 39.844 (39.844)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (31.250)\n",
      "Training epoch: [530/2000][1/4]; Loss: 2.294575387068475\n",
      "Training Accuracy Epoch: [529]\tPrec@1 25.000 (25.000)\tPrec@2 32.812 (32.812)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (31.250)\n",
      "Training epoch: [531/2000][1/4]; Loss: 2.2771333603647723\n",
      "Training Accuracy Epoch: [530]\tPrec@1 31.250 (31.250)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (31.250)\n",
      "Training epoch: [532/2000][1/4]; Loss: 2.256122820013516\n",
      "Training Accuracy Epoch: [531]\tPrec@1 33.594 (33.594)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (32.812)\n",
      "Training epoch: [533/2000][1/4]; Loss: 2.2563286784372556\n",
      "Training Accuracy Epoch: [532]\tPrec@1 37.500 (37.500)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (25.781)\n",
      "Training epoch: [534/2000][1/4]; Loss: 2.278536999590968\n",
      "Training Accuracy Epoch: [533]\tPrec@1 32.031 (32.031)\tPrec@2 38.281 (38.281)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (23.438)\n",
      "Training epoch: [535/2000][1/4]; Loss: 2.266939508150265\n",
      "Training Accuracy Epoch: [534]\tPrec@1 34.375 (34.375)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (24.219)\n",
      "Training epoch: [536/2000][1/4]; Loss: 2.2660859733708856\n",
      "Training Accuracy Epoch: [535]\tPrec@1 34.375 (34.375)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.906)\n",
      "Training epoch: [537/2000][1/4]; Loss: 2.2429411568543403\n",
      "Training Accuracy Epoch: [536]\tPrec@1 38.281 (38.281)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (31.250)\n",
      "Training epoch: [538/2000][1/4]; Loss: 2.2500050996932477\n",
      "Training Accuracy Epoch: [537]\tPrec@1 38.281 (38.281)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (31.250)\n",
      "Training epoch: [539/2000][1/4]; Loss: 2.2538584107812514\n",
      "Training Accuracy Epoch: [538]\tPrec@1 36.719 (36.719)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (26.562)\n",
      "Training epoch: [540/2000][1/4]; Loss: 2.260598070904155\n",
      "Training Accuracy Epoch: [539]\tPrec@1 33.594 (33.594)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (26.562)\n",
      "Training epoch: [541/2000][1/4]; Loss: 2.2590757479929926\n",
      "Training Accuracy Epoch: [540]\tPrec@1 31.250 (31.250)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (28.125)\n",
      "Training epoch: [542/2000][1/4]; Loss: 2.261193069497139\n",
      "Training Accuracy Epoch: [541]\tPrec@1 32.031 (32.031)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.125)\n",
      "Training epoch: [543/2000][1/4]; Loss: 2.2798238882438597\n",
      "Training Accuracy Epoch: [542]\tPrec@1 28.125 (28.125)\tPrec@2 36.719 (36.719)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (29.688)\n",
      "Training epoch: [544/2000][1/4]; Loss: 2.257164925622121\n",
      "Training Accuracy Epoch: [543]\tPrec@1 32.812 (32.812)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (28.125)\n",
      "Training epoch: [545/2000][1/4]; Loss: 2.2679997457962804\n",
      "Training Accuracy Epoch: [544]\tPrec@1 34.375 (34.375)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (28.906)\n",
      "Training epoch: [546/2000][1/4]; Loss: 2.2171417802360334\n",
      "Training Accuracy Epoch: [545]\tPrec@1 41.406 (41.406)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (28.906)\n",
      "Training epoch: [547/2000][1/4]; Loss: 2.2617402487288616\n",
      "Training Accuracy Epoch: [546]\tPrec@1 30.469 (30.469)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (22.656)\n",
      "Training epoch: [548/2000][1/4]; Loss: 2.259872218201038\n",
      "Training Accuracy Epoch: [547]\tPrec@1 35.156 (35.156)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (32.812)\n",
      "Training epoch: [549/2000][1/4]; Loss: 2.260126582692491\n",
      "Training Accuracy Epoch: [548]\tPrec@1 32.812 (32.812)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (27.344)\n",
      "Training epoch: [550/2000][1/4]; Loss: 2.2370693275814317\n",
      "Training Accuracy Epoch: [549]\tPrec@1 38.281 (38.281)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (32.031)\n",
      "Training epoch: [551/2000][1/4]; Loss: 2.251639520773505\n",
      "Training Accuracy Epoch: [550]\tPrec@1 35.938 (35.938)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (28.906)\n",
      "Training epoch: [552/2000][1/4]; Loss: 2.274181755120661\n",
      "Training Accuracy Epoch: [551]\tPrec@1 28.125 (28.125)\tPrec@2 35.938 (35.938)\n",
      "===>>>\tPrec@1 (13.281)\tPrec@2 (32.812)\n",
      "Training epoch: [553/2000][1/4]; Loss: 2.216182905316553\n",
      "Training Accuracy Epoch: [552]\tPrec@1 40.625 (40.625)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (27.344)\n",
      "Training epoch: [554/2000][1/4]; Loss: 2.2437054462694213\n",
      "Training Accuracy Epoch: [553]\tPrec@1 31.250 (31.250)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (28.125)\n",
      "Training epoch: [555/2000][1/4]; Loss: 2.256257911650665\n",
      "Training Accuracy Epoch: [554]\tPrec@1 33.594 (33.594)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (24.219)\n",
      "Training epoch: [556/2000][1/4]; Loss: 2.2782957174210945\n",
      "Training Accuracy Epoch: [555]\tPrec@1 30.469 (30.469)\tPrec@2 38.281 (38.281)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (29.688)\n",
      "Training epoch: [557/2000][1/4]; Loss: 2.2311567885048906\n",
      "Training Accuracy Epoch: [556]\tPrec@1 40.625 (40.625)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (28.125)\n",
      "Training epoch: [558/2000][1/4]; Loss: 2.2369963084944584\n",
      "Training Accuracy Epoch: [557]\tPrec@1 38.281 (38.281)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.125)\n",
      "Training epoch: [559/2000][1/4]; Loss: 2.2404873386015254\n",
      "Training Accuracy Epoch: [558]\tPrec@1 34.375 (34.375)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (29.688)\n",
      "Training epoch: [560/2000][1/4]; Loss: 2.2465692580607532\n",
      "Training Accuracy Epoch: [559]\tPrec@1 35.156 (35.156)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (31.250)\n",
      "Training epoch: [561/2000][1/4]; Loss: 2.2280501577998537\n",
      "Training Accuracy Epoch: [560]\tPrec@1 39.844 (39.844)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (29.688)\n",
      "Training epoch: [562/2000][1/4]; Loss: 2.239316623584126\n",
      "Training Accuracy Epoch: [561]\tPrec@1 33.594 (33.594)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (29.688)\n",
      "Training epoch: [563/2000][1/4]; Loss: 2.2448656102851854\n",
      "Training Accuracy Epoch: [562]\tPrec@1 33.594 (33.594)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (28.906)\n",
      "Training epoch: [564/2000][1/4]; Loss: 2.2514013421681427\n",
      "Training Accuracy Epoch: [563]\tPrec@1 32.031 (32.031)\tPrec@2 38.281 (38.281)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (25.000)\n",
      "Training epoch: [565/2000][1/4]; Loss: 2.2420161866870236\n",
      "Training Accuracy Epoch: [564]\tPrec@1 36.719 (36.719)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (28.125)\n",
      "Training epoch: [566/2000][1/4]; Loss: 2.2598380505893103\n",
      "Training Accuracy Epoch: [565]\tPrec@1 30.469 (30.469)\tPrec@2 33.594 (33.594)\n",
      "===>>>\tPrec@1 (26.562)\tPrec@2 (34.375)\n",
      "Training epoch: [567/2000][1/4]; Loss: 2.219569844836827\n",
      "Training Accuracy Epoch: [566]\tPrec@1 40.625 (40.625)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (28.906)\n",
      "Training epoch: [568/2000][1/4]; Loss: 2.2850155959924834\n",
      "Training Accuracy Epoch: [567]\tPrec@1 23.438 (23.438)\tPrec@2 33.594 (33.594)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (29.688)\n",
      "Training epoch: [569/2000][1/4]; Loss: 2.236702891901673\n",
      "Training Accuracy Epoch: [568]\tPrec@1 37.500 (37.500)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (30.469)\n",
      "Training epoch: [570/2000][1/4]; Loss: 2.272651172140873\n",
      "Training Accuracy Epoch: [569]\tPrec@1 32.812 (32.812)\tPrec@2 36.719 (36.719)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (29.688)\n",
      "Training epoch: [571/2000][1/4]; Loss: 2.2555203987729597\n",
      "Training Accuracy Epoch: [570]\tPrec@1 35.156 (35.156)\tPrec@2 38.281 (38.281)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (29.688)\n",
      "Training epoch: [572/2000][1/4]; Loss: 2.2312040659686385\n",
      "Training Accuracy Epoch: [571]\tPrec@1 39.062 (39.062)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (30.469)\n",
      "Training epoch: [573/2000][1/4]; Loss: 2.2622373430783838\n",
      "Training Accuracy Epoch: [572]\tPrec@1 28.906 (28.906)\tPrec@2 35.938 (35.938)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>>>\tPrec@1 (17.969)\tPrec@2 (23.438)\n",
      "Training epoch: [574/2000][1/4]; Loss: 2.243433479870813\n",
      "Training Accuracy Epoch: [573]\tPrec@1 36.719 (36.719)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (31.250)\n",
      "Training epoch: [575/2000][1/4]; Loss: 2.24041176224681\n",
      "Training Accuracy Epoch: [574]\tPrec@1 32.812 (32.812)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (30.469)\n",
      "Training epoch: [576/2000][1/4]; Loss: 2.2376501610365382\n",
      "Training Accuracy Epoch: [575]\tPrec@1 35.156 (35.156)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (32.812)\n",
      "Training epoch: [577/2000][1/4]; Loss: 2.230891708698564\n",
      "Training Accuracy Epoch: [576]\tPrec@1 38.281 (38.281)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (29.688)\n",
      "Training epoch: [578/2000][1/4]; Loss: 2.241732402400816\n",
      "Training Accuracy Epoch: [577]\tPrec@1 31.250 (31.250)\tPrec@2 38.281 (38.281)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (29.688)\n",
      "Training epoch: [579/2000][1/4]; Loss: 2.245311786701266\n",
      "Training Accuracy Epoch: [578]\tPrec@1 34.375 (34.375)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (28.125)\n",
      "Training epoch: [580/2000][1/4]; Loss: 2.2156718138250917\n",
      "Training Accuracy Epoch: [579]\tPrec@1 39.062 (39.062)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (28.906)\n",
      "Training epoch: [581/2000][1/4]; Loss: 2.223867125315074\n",
      "Training Accuracy Epoch: [580]\tPrec@1 38.281 (38.281)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.906)\n",
      "Training epoch: [582/2000][1/4]; Loss: 2.2933197577020676\n",
      "Training Accuracy Epoch: [581]\tPrec@1 25.781 (25.781)\tPrec@2 36.719 (36.719)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (27.344)\n",
      "Training epoch: [583/2000][1/4]; Loss: 2.2213350726232104\n",
      "Training Accuracy Epoch: [582]\tPrec@1 37.500 (37.500)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (28.906)\n",
      "Training epoch: [584/2000][1/4]; Loss: 2.2495874771470765\n",
      "Training Accuracy Epoch: [583]\tPrec@1 32.812 (32.812)\tPrec@2 39.844 (39.844)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (28.906)\n",
      "Training epoch: [585/2000][1/4]; Loss: 2.284958613147482\n",
      "Training Accuracy Epoch: [584]\tPrec@1 28.125 (28.125)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (25.781)\n",
      "Training epoch: [586/2000][1/4]; Loss: 2.2427674340648953\n",
      "Training Accuracy Epoch: [585]\tPrec@1 34.375 (34.375)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (25.781)\n",
      "Training epoch: [587/2000][1/4]; Loss: 2.2321722905484767\n",
      "Training Accuracy Epoch: [586]\tPrec@1 34.375 (34.375)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (26.562)\n",
      "Training epoch: [588/2000][1/4]; Loss: 2.24858727564159\n",
      "Training Accuracy Epoch: [587]\tPrec@1 34.375 (34.375)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (22.656)\n",
      "Training epoch: [589/2000][1/4]; Loss: 2.2553230223376186\n",
      "Training Accuracy Epoch: [588]\tPrec@1 30.469 (30.469)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (28.906)\n",
      "Training epoch: [590/2000][1/4]; Loss: 2.2184393262267323\n",
      "Training Accuracy Epoch: [589]\tPrec@1 35.938 (35.938)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (25.781)\n",
      "Training epoch: [591/2000][1/4]; Loss: 2.252143289537869\n",
      "Training Accuracy Epoch: [590]\tPrec@1 32.812 (32.812)\tPrec@2 36.719 (36.719)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (28.125)\n",
      "Training epoch: [592/2000][1/4]; Loss: 2.2507048962687373\n",
      "Training Accuracy Epoch: [591]\tPrec@1 32.812 (32.812)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (31.250)\n",
      "Training epoch: [593/2000][1/4]; Loss: 2.2362950377407924\n",
      "Training Accuracy Epoch: [592]\tPrec@1 34.375 (34.375)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (25.781)\n",
      "Training epoch: [594/2000][1/4]; Loss: 2.2395440940963387\n",
      "Training Accuracy Epoch: [593]\tPrec@1 36.719 (36.719)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (27.344)\n",
      "Training epoch: [595/2000][1/4]; Loss: 2.2551946050194087\n",
      "Training Accuracy Epoch: [594]\tPrec@1 28.906 (28.906)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (28.906)\n",
      "Training epoch: [596/2000][1/4]; Loss: 2.2524838045445312\n",
      "Training Accuracy Epoch: [595]\tPrec@1 32.812 (32.812)\tPrec@2 39.844 (39.844)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (28.125)\n",
      "Training epoch: [597/2000][1/4]; Loss: 2.282567295136741\n",
      "Training Accuracy Epoch: [596]\tPrec@1 25.781 (25.781)\tPrec@2 35.156 (35.156)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (25.781)\n",
      "Training epoch: [598/2000][1/4]; Loss: 2.2378760838510448\n",
      "Training Accuracy Epoch: [597]\tPrec@1 34.375 (34.375)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (31.250)\n",
      "Training epoch: [599/2000][1/4]; Loss: 2.2448004661539644\n",
      "Training Accuracy Epoch: [598]\tPrec@1 35.156 (35.156)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (27.344)\n",
      "Training epoch: [600/2000][1/4]; Loss: 2.237926378328663\n",
      "Training Accuracy Epoch: [599]\tPrec@1 33.594 (33.594)\tPrec@2 38.281 (38.281)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (28.906)\n",
      "Training epoch: [601/2000][1/4]; Loss: 2.2416311587410713\n",
      "Training Accuracy Epoch: [600]\tPrec@1 33.594 (33.594)\tPrec@2 38.281 (38.281)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (28.906)\n",
      "Training epoch: [602/2000][1/4]; Loss: 2.205358769094182\n",
      "Training Accuracy Epoch: [601]\tPrec@1 38.281 (38.281)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (30.469)\n",
      "Training epoch: [603/2000][1/4]; Loss: 2.266297480802984\n",
      "Training Accuracy Epoch: [602]\tPrec@1 30.469 (30.469)\tPrec@2 34.375 (34.375)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (28.906)\n",
      "Training epoch: [604/2000][1/4]; Loss: 2.256908235006502\n",
      "Training Accuracy Epoch: [603]\tPrec@1 28.125 (28.125)\tPrec@2 35.156 (35.156)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.125)\n",
      "Training epoch: [605/2000][1/4]; Loss: 2.2454858932695325\n",
      "Training Accuracy Epoch: [604]\tPrec@1 31.250 (31.250)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (30.469)\n",
      "Training epoch: [606/2000][1/4]; Loss: 2.203265743449106\n",
      "Training Accuracy Epoch: [605]\tPrec@1 36.719 (36.719)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (24.219)\n",
      "Training epoch: [607/2000][1/4]; Loss: 2.223488562762868\n",
      "Training Accuracy Epoch: [606]\tPrec@1 38.281 (38.281)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (27.344)\n",
      "Training epoch: [608/2000][1/4]; Loss: 2.200956754482933\n",
      "Training Accuracy Epoch: [607]\tPrec@1 41.406 (41.406)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (28.906)\n",
      "Training epoch: [609/2000][1/4]; Loss: 2.2317602135222336\n",
      "Training Accuracy Epoch: [608]\tPrec@1 34.375 (34.375)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (24.219)\n",
      "Training epoch: [610/2000][1/4]; Loss: 2.21672490744368\n",
      "Training Accuracy Epoch: [609]\tPrec@1 37.500 (37.500)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (28.125)\n",
      "Training epoch: [611/2000][1/4]; Loss: 2.2327558502261824\n",
      "Training Accuracy Epoch: [610]\tPrec@1 33.594 (33.594)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (25.781)\n",
      "Training epoch: [612/2000][1/4]; Loss: 2.20903708673199\n",
      "Training Accuracy Epoch: [611]\tPrec@1 35.156 (35.156)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (29.688)\n",
      "Training epoch: [613/2000][1/4]; Loss: 2.228415835120459\n",
      "Training Accuracy Epoch: [612]\tPrec@1 30.469 (30.469)\tPrec@2 35.156 (35.156)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (30.469)\n",
      "Training epoch: [614/2000][1/4]; Loss: 2.2087137235222736\n",
      "Training Accuracy Epoch: [613]\tPrec@1 36.719 (36.719)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (32.031)\n",
      "Training epoch: [615/2000][1/4]; Loss: 2.202899954304412\n",
      "Training Accuracy Epoch: [614]\tPrec@1 38.281 (38.281)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (25.781)\n",
      "Training epoch: [616/2000][1/4]; Loss: 2.245975799828086\n",
      "Training Accuracy Epoch: [615]\tPrec@1 27.344 (27.344)\tPrec@2 34.375 (34.375)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (24.219)\n",
      "Training epoch: [617/2000][1/4]; Loss: 2.232151983267505\n",
      "Training Accuracy Epoch: [616]\tPrec@1 30.469 (30.469)\tPrec@2 39.844 (39.844)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (26.562)\n",
      "Training epoch: [618/2000][1/4]; Loss: 2.196783932350679\n",
      "Training Accuracy Epoch: [617]\tPrec@1 39.062 (39.062)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (26.562)\n",
      "Training epoch: [619/2000][1/4]; Loss: 2.1924245637080295\n",
      "Training Accuracy Epoch: [618]\tPrec@1 42.188 (42.188)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (28.125)\n",
      "Training epoch: [620/2000][1/4]; Loss: 2.226507046870513\n",
      "Training Accuracy Epoch: [619]\tPrec@1 35.156 (35.156)\tPrec@2 39.844 (39.844)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (29.688)\n",
      "Training epoch: [621/2000][1/4]; Loss: 2.208659917745467\n",
      "Training Accuracy Epoch: [620]\tPrec@1 36.719 (36.719)\tPrec@2 46.094 (46.094)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>>>\tPrec@1 (25.781)\tPrec@2 (31.250)\n",
      "Training epoch: [622/2000][1/4]; Loss: 2.219517618887277\n",
      "Training Accuracy Epoch: [621]\tPrec@1 35.938 (35.938)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (30.469)\n",
      "Training epoch: [623/2000][1/4]; Loss: 2.2127289564205648\n",
      "Training Accuracy Epoch: [622]\tPrec@1 38.281 (38.281)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (25.781)\n",
      "Training epoch: [624/2000][1/4]; Loss: 2.2160722905911685\n",
      "Training Accuracy Epoch: [623]\tPrec@1 37.500 (37.500)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (26.562)\n",
      "Training epoch: [625/2000][1/4]; Loss: 2.2200377659788098\n",
      "Training Accuracy Epoch: [624]\tPrec@1 35.156 (35.156)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (30.469)\n",
      "Training epoch: [626/2000][1/4]; Loss: 2.230659438683122\n",
      "Training Accuracy Epoch: [625]\tPrec@1 32.812 (32.812)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.125)\n",
      "Training epoch: [627/2000][1/4]; Loss: 2.232006190859231\n",
      "Training Accuracy Epoch: [626]\tPrec@1 32.812 (32.812)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (28.125)\n",
      "Training epoch: [628/2000][1/4]; Loss: 2.2224415899947307\n",
      "Training Accuracy Epoch: [627]\tPrec@1 34.375 (34.375)\tPrec@2 39.844 (39.844)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (28.125)\n",
      "Training epoch: [629/2000][1/4]; Loss: 2.2475104524360745\n",
      "Training Accuracy Epoch: [628]\tPrec@1 30.469 (30.469)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (27.344)\n",
      "Training epoch: [630/2000][1/4]; Loss: 2.2116999547280654\n",
      "Training Accuracy Epoch: [629]\tPrec@1 39.062 (39.062)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (28.906)\n",
      "Training epoch: [631/2000][1/4]; Loss: 2.2384763616833414\n",
      "Training Accuracy Epoch: [630]\tPrec@1 31.250 (31.250)\tPrec@2 35.938 (35.938)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (28.125)\n",
      "Training epoch: [632/2000][1/4]; Loss: 2.2076793875968366\n",
      "Training Accuracy Epoch: [631]\tPrec@1 35.938 (35.938)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (28.906)\n",
      "Training epoch: [633/2000][1/4]; Loss: 2.2138518931310083\n",
      "Training Accuracy Epoch: [632]\tPrec@1 36.719 (36.719)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.906)\n",
      "Training epoch: [634/2000][1/4]; Loss: 2.208347222086113\n",
      "Training Accuracy Epoch: [633]\tPrec@1 40.625 (40.625)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (30.469)\n",
      "Training epoch: [635/2000][1/4]; Loss: 2.224494203978703\n",
      "Training Accuracy Epoch: [634]\tPrec@1 32.812 (32.812)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (24.219)\tPrec@2 (29.688)\n",
      "Training epoch: [636/2000][1/4]; Loss: 2.21312466468505\n",
      "Training Accuracy Epoch: [635]\tPrec@1 34.375 (34.375)\tPrec@2 39.844 (39.844)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (29.688)\n",
      "Training epoch: [637/2000][1/4]; Loss: 2.2332797762611984\n",
      "Training Accuracy Epoch: [636]\tPrec@1 33.594 (33.594)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (25.781)\n",
      "Training epoch: [638/2000][1/4]; Loss: 2.2415482852714756\n",
      "Training Accuracy Epoch: [637]\tPrec@1 31.250 (31.250)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (30.469)\n",
      "Training epoch: [639/2000][1/4]; Loss: 2.2154352224003366\n",
      "Training Accuracy Epoch: [638]\tPrec@1 32.031 (32.031)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (28.906)\n",
      "Training epoch: [640/2000][1/4]; Loss: 2.222788161692651\n",
      "Training Accuracy Epoch: [639]\tPrec@1 36.719 (36.719)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (22.656)\n",
      "Training epoch: [641/2000][1/4]; Loss: 2.1854548006322805\n",
      "Training Accuracy Epoch: [640]\tPrec@1 39.844 (39.844)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (28.906)\n",
      "Training epoch: [642/2000][1/4]; Loss: 2.221230339604572\n",
      "Training Accuracy Epoch: [641]\tPrec@1 32.031 (32.031)\tPrec@2 39.844 (39.844)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (29.688)\n",
      "Training epoch: [643/2000][1/4]; Loss: 2.20287119777991\n",
      "Training Accuracy Epoch: [642]\tPrec@1 40.625 (40.625)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (25.000)\tPrec@2 (28.125)\n",
      "Training epoch: [644/2000][1/4]; Loss: 2.2010395622694037\n",
      "Training Accuracy Epoch: [643]\tPrec@1 35.156 (35.156)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (27.344)\n",
      "Training epoch: [645/2000][1/4]; Loss: 2.248377593854406\n",
      "Training Accuracy Epoch: [644]\tPrec@1 28.906 (28.906)\tPrec@2 36.719 (36.719)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (26.562)\n",
      "Training epoch: [646/2000][1/4]; Loss: 2.189627654676174\n",
      "Training Accuracy Epoch: [645]\tPrec@1 41.406 (41.406)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (27.344)\n",
      "Training epoch: [647/2000][1/4]; Loss: 2.2359815883595444\n",
      "Training Accuracy Epoch: [646]\tPrec@1 31.250 (31.250)\tPrec@2 35.938 (35.938)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (28.906)\n",
      "Training epoch: [648/2000][1/4]; Loss: 2.260169279572006\n",
      "Training Accuracy Epoch: [647]\tPrec@1 29.688 (29.688)\tPrec@2 33.594 (33.594)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (28.906)\n",
      "Training epoch: [649/2000][1/4]; Loss: 2.2070080647820767\n",
      "Training Accuracy Epoch: [648]\tPrec@1 36.719 (36.719)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (25.781)\n",
      "Training epoch: [650/2000][1/4]; Loss: 2.1736673572177723\n",
      "Training Accuracy Epoch: [649]\tPrec@1 43.750 (43.750)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (31.250)\n",
      "Training epoch: [651/2000][1/4]; Loss: 2.212039505424421\n",
      "Training Accuracy Epoch: [650]\tPrec@1 36.719 (36.719)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (25.000)\n",
      "Training epoch: [652/2000][1/4]; Loss: 2.205072760881872\n",
      "Training Accuracy Epoch: [651]\tPrec@1 35.156 (35.156)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (27.344)\n",
      "Training epoch: [653/2000][1/4]; Loss: 2.2219750665475977\n",
      "Training Accuracy Epoch: [652]\tPrec@1 33.594 (33.594)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (33.594)\n",
      "Training epoch: [654/2000][1/4]; Loss: 2.2406175457198403\n",
      "Training Accuracy Epoch: [653]\tPrec@1 26.562 (26.562)\tPrec@2 33.594 (33.594)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (30.469)\n",
      "Training epoch: [655/2000][1/4]; Loss: 2.22283565831662\n",
      "Training Accuracy Epoch: [654]\tPrec@1 32.031 (32.031)\tPrec@2 39.844 (39.844)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (26.562)\n",
      "Training epoch: [656/2000][1/4]; Loss: 2.207755252408057\n",
      "Training Accuracy Epoch: [655]\tPrec@1 33.594 (33.594)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (25.000)\n",
      "Training epoch: [657/2000][1/4]; Loss: 2.2117113286458974\n",
      "Training Accuracy Epoch: [656]\tPrec@1 34.375 (34.375)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (28.906)\n",
      "Training epoch: [658/2000][1/4]; Loss: 2.2283704192611733\n",
      "Training Accuracy Epoch: [657]\tPrec@1 32.031 (32.031)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (26.562)\n",
      "Training epoch: [659/2000][1/4]; Loss: 2.1854054213118523\n",
      "Training Accuracy Epoch: [658]\tPrec@1 40.625 (40.625)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (25.000)\tPrec@2 (29.688)\n",
      "Training epoch: [660/2000][1/4]; Loss: 2.1767517074197986\n",
      "Training Accuracy Epoch: [659]\tPrec@1 39.844 (39.844)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.906)\n",
      "Training epoch: [661/2000][1/4]; Loss: 2.1894909740420823\n",
      "Training Accuracy Epoch: [660]\tPrec@1 36.719 (36.719)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (25.781)\n",
      "Training epoch: [662/2000][1/4]; Loss: 2.2060851684587863\n",
      "Training Accuracy Epoch: [661]\tPrec@1 35.156 (35.156)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (29.688)\n",
      "Training epoch: [663/2000][1/4]; Loss: 2.2084211968340903\n",
      "Training Accuracy Epoch: [662]\tPrec@1 34.375 (34.375)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (34.375)\n",
      "Training epoch: [664/2000][1/4]; Loss: 2.2124964369526854\n",
      "Training Accuracy Epoch: [663]\tPrec@1 32.031 (32.031)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (30.469)\n",
      "Training epoch: [665/2000][1/4]; Loss: 2.21147418079146\n",
      "Training Accuracy Epoch: [664]\tPrec@1 32.812 (32.812)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (29.688)\n",
      "Training epoch: [666/2000][1/4]; Loss: 2.216016960239446\n",
      "Training Accuracy Epoch: [665]\tPrec@1 34.375 (34.375)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (30.469)\n",
      "Training epoch: [667/2000][1/4]; Loss: 2.226582435045004\n",
      "Training Accuracy Epoch: [666]\tPrec@1 32.812 (32.812)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (31.250)\n",
      "Training epoch: [668/2000][1/4]; Loss: 2.219506655989062\n",
      "Training Accuracy Epoch: [667]\tPrec@1 29.688 (29.688)\tPrec@2 36.719 (36.719)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (25.000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: [669/2000][1/4]; Loss: 2.207932491953867\n",
      "Training Accuracy Epoch: [668]\tPrec@1 33.594 (33.594)\tPrec@2 39.844 (39.844)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (27.344)\n",
      "Training epoch: [670/2000][1/4]; Loss: 2.2024709070340696\n",
      "Training Accuracy Epoch: [669]\tPrec@1 30.469 (30.469)\tPrec@2 34.375 (34.375)\n",
      "===>>>\tPrec@1 (14.062)\tPrec@2 (25.781)\n",
      "Training epoch: [671/2000][1/4]; Loss: 2.183969063709144\n",
      "Training Accuracy Epoch: [670]\tPrec@1 38.281 (38.281)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (27.344)\n",
      "Training epoch: [672/2000][1/4]; Loss: 2.183484048426348\n",
      "Training Accuracy Epoch: [671]\tPrec@1 36.719 (36.719)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (29.688)\n",
      "Training epoch: [673/2000][1/4]; Loss: 2.1648827331996765\n",
      "Training Accuracy Epoch: [672]\tPrec@1 39.844 (39.844)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (29.688)\n",
      "Training epoch: [674/2000][1/4]; Loss: 2.195104996036863\n",
      "Training Accuracy Epoch: [673]\tPrec@1 35.938 (35.938)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (27.344)\n",
      "Training epoch: [675/2000][1/4]; Loss: 2.228613656678456\n",
      "Training Accuracy Epoch: [674]\tPrec@1 27.344 (27.344)\tPrec@2 32.812 (32.812)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (25.781)\n",
      "Training epoch: [676/2000][1/4]; Loss: 2.224115330460939\n",
      "Training Accuracy Epoch: [675]\tPrec@1 33.594 (33.594)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (27.344)\n",
      "Training epoch: [677/2000][1/4]; Loss: 2.1908749595659502\n",
      "Training Accuracy Epoch: [676]\tPrec@1 36.719 (36.719)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (29.688)\n",
      "Training epoch: [678/2000][1/4]; Loss: 2.2259263745296787\n",
      "Training Accuracy Epoch: [677]\tPrec@1 28.906 (28.906)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.031)\n",
      "Training epoch: [679/2000][1/4]; Loss: 2.2595644416251743\n",
      "Training Accuracy Epoch: [678]\tPrec@1 27.344 (27.344)\tPrec@2 35.938 (35.938)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (32.031)\n",
      "Training epoch: [680/2000][1/4]; Loss: 2.2303286792320316\n",
      "Training Accuracy Epoch: [679]\tPrec@1 28.125 (28.125)\tPrec@2 35.156 (35.156)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (31.250)\n",
      "Training epoch: [681/2000][1/4]; Loss: 2.192918499551258\n",
      "Training Accuracy Epoch: [680]\tPrec@1 32.031 (32.031)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (25.000)\tPrec@2 (32.812)\n",
      "Training epoch: [682/2000][1/4]; Loss: 2.2176212950190175\n",
      "Training Accuracy Epoch: [681]\tPrec@1 30.469 (30.469)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (26.562)\n",
      "Training epoch: [683/2000][1/4]; Loss: 2.193108636882826\n",
      "Training Accuracy Epoch: [682]\tPrec@1 32.031 (32.031)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (29.688)\n",
      "Training epoch: [684/2000][1/4]; Loss: 2.1768496415864593\n",
      "Training Accuracy Epoch: [683]\tPrec@1 36.719 (36.719)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (29.688)\n",
      "Training epoch: [685/2000][1/4]; Loss: 2.205735625787283\n",
      "Training Accuracy Epoch: [684]\tPrec@1 35.156 (35.156)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (32.812)\n",
      "Training epoch: [686/2000][1/4]; Loss: 2.1762092679069736\n",
      "Training Accuracy Epoch: [685]\tPrec@1 37.500 (37.500)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (14.062)\tPrec@2 (25.000)\n",
      "Training epoch: [687/2000][1/4]; Loss: 2.1864373377908013\n",
      "Training Accuracy Epoch: [686]\tPrec@1 35.938 (35.938)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (27.344)\n",
      "Training epoch: [688/2000][1/4]; Loss: 2.1851669218572902\n",
      "Training Accuracy Epoch: [687]\tPrec@1 35.156 (35.156)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (29.688)\n",
      "Training epoch: [689/2000][1/4]; Loss: 2.2092227900715353\n",
      "Training Accuracy Epoch: [688]\tPrec@1 34.375 (34.375)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (27.344)\n",
      "Training epoch: [690/2000][1/4]; Loss: 2.2141656280151327\n",
      "Training Accuracy Epoch: [689]\tPrec@1 28.125 (28.125)\tPrec@2 35.938 (35.938)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (29.688)\n",
      "Training epoch: [691/2000][1/4]; Loss: 2.20585709022403\n",
      "Training Accuracy Epoch: [690]\tPrec@1 36.719 (36.719)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (14.062)\tPrec@2 (27.344)\n",
      "Training epoch: [692/2000][1/4]; Loss: 2.1925391920958406\n",
      "Training Accuracy Epoch: [691]\tPrec@1 32.812 (32.812)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (31.250)\n",
      "Training epoch: [693/2000][1/4]; Loss: 2.1717976903057443\n",
      "Training Accuracy Epoch: [692]\tPrec@1 40.625 (40.625)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (24.219)\n",
      "Training epoch: [694/2000][1/4]; Loss: 2.2227227753322\n",
      "Training Accuracy Epoch: [693]\tPrec@1 27.344 (27.344)\tPrec@2 36.719 (36.719)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (31.250)\n",
      "Training epoch: [695/2000][1/4]; Loss: 2.204715453185345\n",
      "Training Accuracy Epoch: [694]\tPrec@1 28.906 (28.906)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (26.562)\n",
      "Training epoch: [696/2000][1/4]; Loss: 2.1753247754609575\n",
      "Training Accuracy Epoch: [695]\tPrec@1 37.500 (37.500)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (26.562)\tPrec@2 (32.812)\n",
      "Training epoch: [697/2000][1/4]; Loss: 2.179260483468481\n",
      "Training Accuracy Epoch: [696]\tPrec@1 37.500 (37.500)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (25.000)\n",
      "Training epoch: [698/2000][1/4]; Loss: 2.173648952391621\n",
      "Training Accuracy Epoch: [697]\tPrec@1 42.969 (42.969)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (28.125)\n",
      "Training epoch: [699/2000][1/4]; Loss: 2.1835571663406625\n",
      "Training Accuracy Epoch: [698]\tPrec@1 37.500 (37.500)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (27.344)\n",
      "Training epoch: [700/2000][1/4]; Loss: 2.187331875741223\n",
      "Training Accuracy Epoch: [699]\tPrec@1 38.281 (38.281)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (28.906)\n",
      "Training epoch: [701/2000][1/4]; Loss: 2.1895959006608705\n",
      "Training Accuracy Epoch: [700]\tPrec@1 32.031 (32.031)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (25.781)\n",
      "Training epoch: [702/2000][1/4]; Loss: 2.2078279701490806\n",
      "Training Accuracy Epoch: [701]\tPrec@1 28.906 (28.906)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (25.000)\tPrec@2 (32.031)\n",
      "Training epoch: [703/2000][1/4]; Loss: 2.2044219168261963\n",
      "Training Accuracy Epoch: [702]\tPrec@1 32.031 (32.031)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (26.562)\n",
      "Training epoch: [704/2000][1/4]; Loss: 2.1960727703293763\n",
      "Training Accuracy Epoch: [703]\tPrec@1 35.156 (35.156)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (26.562)\n",
      "Training epoch: [705/2000][1/4]; Loss: 2.1857282482824663\n",
      "Training Accuracy Epoch: [704]\tPrec@1 32.031 (32.031)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (29.688)\n",
      "Training epoch: [706/2000][1/4]; Loss: 2.2088702895742296\n",
      "Training Accuracy Epoch: [705]\tPrec@1 28.125 (28.125)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (28.125)\n",
      "Training epoch: [707/2000][1/4]; Loss: 2.1950297099228875\n",
      "Training Accuracy Epoch: [706]\tPrec@1 30.469 (30.469)\tPrec@2 39.844 (39.844)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (28.906)\n",
      "Training epoch: [708/2000][1/4]; Loss: 2.1617702668569416\n",
      "Training Accuracy Epoch: [707]\tPrec@1 39.844 (39.844)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (32.812)\n",
      "Training epoch: [709/2000][1/4]; Loss: 2.1818474453610412\n",
      "Training Accuracy Epoch: [708]\tPrec@1 35.156 (35.156)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (25.781)\n",
      "Training epoch: [710/2000][1/4]; Loss: 2.223931350429782\n",
      "Training Accuracy Epoch: [709]\tPrec@1 33.594 (33.594)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (24.219)\n",
      "Training epoch: [711/2000][1/4]; Loss: 2.1507353948770676\n",
      "Training Accuracy Epoch: [710]\tPrec@1 39.062 (39.062)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.125)\n",
      "Training epoch: [712/2000][1/4]; Loss: 2.2119881619755772\n",
      "Training Accuracy Epoch: [711]\tPrec@1 27.344 (27.344)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (28.125)\n",
      "Training epoch: [713/2000][1/4]; Loss: 2.184555300822075\n",
      "Training Accuracy Epoch: [712]\tPrec@1 32.812 (32.812)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.906)\n",
      "Training epoch: [714/2000][1/4]; Loss: 2.214639171055529\n",
      "Training Accuracy Epoch: [713]\tPrec@1 30.469 (30.469)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (28.906)\n",
      "Training epoch: [715/2000][1/4]; Loss: 2.187503158341036\n",
      "Training Accuracy Epoch: [714]\tPrec@1 31.250 (31.250)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.906)\n",
      "Training epoch: [716/2000][1/4]; Loss: 2.191742805856596\n",
      "Training Accuracy Epoch: [715]\tPrec@1 33.594 (33.594)\tPrec@2 41.406 (41.406)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>>>\tPrec@1 (21.875)\tPrec@2 (32.812)\n",
      "Training epoch: [717/2000][1/4]; Loss: 2.2078479520817322\n",
      "Training Accuracy Epoch: [716]\tPrec@1 30.469 (30.469)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (28.125)\n",
      "Training epoch: [718/2000][1/4]; Loss: 2.1848195528189622\n",
      "Training Accuracy Epoch: [717]\tPrec@1 31.250 (31.250)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (28.125)\n",
      "Training epoch: [719/2000][1/4]; Loss: 2.1647132389678108\n",
      "Training Accuracy Epoch: [718]\tPrec@1 35.156 (35.156)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (30.469)\n",
      "Training epoch: [720/2000][1/4]; Loss: 2.1898337320581205\n",
      "Training Accuracy Epoch: [719]\tPrec@1 35.156 (35.156)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (30.469)\n",
      "Training epoch: [721/2000][1/4]; Loss: 2.206229998824183\n",
      "Training Accuracy Epoch: [720]\tPrec@1 29.688 (29.688)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (26.562)\n",
      "Training epoch: [722/2000][1/4]; Loss: 2.1987548749527814\n",
      "Training Accuracy Epoch: [721]\tPrec@1 29.688 (29.688)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (25.781)\n",
      "Training epoch: [723/2000][1/4]; Loss: 2.175488884774759\n",
      "Training Accuracy Epoch: [722]\tPrec@1 36.719 (36.719)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (27.344)\n",
      "Training epoch: [724/2000][1/4]; Loss: 2.1963170323202235\n",
      "Training Accuracy Epoch: [723]\tPrec@1 32.031 (32.031)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.125)\n",
      "Training epoch: [725/2000][1/4]; Loss: 2.153898003569707\n",
      "Training Accuracy Epoch: [724]\tPrec@1 40.625 (40.625)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (31.250)\n",
      "Training epoch: [726/2000][1/4]; Loss: 2.208752135989176\n",
      "Training Accuracy Epoch: [725]\tPrec@1 28.906 (28.906)\tPrec@2 36.719 (36.719)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (26.562)\n",
      "Training epoch: [727/2000][1/4]; Loss: 2.1796540274811687\n",
      "Training Accuracy Epoch: [726]\tPrec@1 32.031 (32.031)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (26.562)\n",
      "Training epoch: [728/2000][1/4]; Loss: 2.209597280592341\n",
      "Training Accuracy Epoch: [727]\tPrec@1 25.781 (25.781)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (25.781)\n",
      "Training epoch: [729/2000][1/4]; Loss: 2.230127584959184\n",
      "Training Accuracy Epoch: [728]\tPrec@1 26.562 (26.562)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (30.469)\n",
      "Training epoch: [730/2000][1/4]; Loss: 2.1643455447211997\n",
      "Training Accuracy Epoch: [729]\tPrec@1 35.156 (35.156)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (28.906)\n",
      "Training epoch: [731/2000][1/4]; Loss: 2.2039707660767\n",
      "Training Accuracy Epoch: [730]\tPrec@1 34.375 (34.375)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (30.469)\n",
      "Training epoch: [732/2000][1/4]; Loss: 2.157317674632393\n",
      "Training Accuracy Epoch: [731]\tPrec@1 39.844 (39.844)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (28.125)\n",
      "Training epoch: [733/2000][1/4]; Loss: 2.1702212582882434\n",
      "Training Accuracy Epoch: [732]\tPrec@1 37.500 (37.500)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (34.375)\n",
      "Training epoch: [734/2000][1/4]; Loss: 2.1703339099775616\n",
      "Training Accuracy Epoch: [733]\tPrec@1 34.375 (34.375)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (28.906)\n",
      "Training epoch: [735/2000][1/4]; Loss: 2.2036613721124416\n",
      "Training Accuracy Epoch: [734]\tPrec@1 32.812 (32.812)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.125)\n",
      "Training epoch: [736/2000][1/4]; Loss: 2.187136484612394\n",
      "Training Accuracy Epoch: [735]\tPrec@1 26.562 (26.562)\tPrec@2 39.844 (39.844)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (32.031)\n",
      "Training epoch: [737/2000][1/4]; Loss: 2.205149638753485\n",
      "Training Accuracy Epoch: [736]\tPrec@1 28.906 (28.906)\tPrec@2 35.938 (35.938)\n",
      "===>>>\tPrec@1 (24.219)\tPrec@2 (32.812)\n",
      "Training epoch: [738/2000][1/4]; Loss: 2.1502363576960954\n",
      "Training Accuracy Epoch: [737]\tPrec@1 39.062 (39.062)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (28.906)\n",
      "Training epoch: [739/2000][1/4]; Loss: 2.1487246495509105\n",
      "Training Accuracy Epoch: [738]\tPrec@1 36.719 (36.719)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (28.906)\n",
      "Training epoch: [740/2000][1/4]; Loss: 2.10400810096712\n",
      "Training Accuracy Epoch: [739]\tPrec@1 50.000 (50.000)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (31.250)\n",
      "Training epoch: [741/2000][1/4]; Loss: 2.1311797699314567\n",
      "Training Accuracy Epoch: [740]\tPrec@1 41.406 (41.406)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (27.344)\n",
      "Training epoch: [742/2000][1/4]; Loss: 2.1663936514632933\n",
      "Training Accuracy Epoch: [741]\tPrec@1 31.250 (31.250)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (29.688)\n",
      "Training epoch: [743/2000][1/4]; Loss: 2.164433270763934\n",
      "Training Accuracy Epoch: [742]\tPrec@1 35.156 (35.156)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (27.344)\n",
      "Training epoch: [744/2000][1/4]; Loss: 2.1465884120999412\n",
      "Training Accuracy Epoch: [743]\tPrec@1 35.938 (35.938)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (29.688)\n",
      "Training epoch: [745/2000][1/4]; Loss: 2.204883577054884\n",
      "Training Accuracy Epoch: [744]\tPrec@1 27.344 (27.344)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.125)\n",
      "Training epoch: [746/2000][1/4]; Loss: 2.196248273606388\n",
      "Training Accuracy Epoch: [745]\tPrec@1 25.000 (25.000)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (13.281)\tPrec@2 (26.562)\n",
      "Training epoch: [747/2000][1/4]; Loss: 2.173633779932879\n",
      "Training Accuracy Epoch: [746]\tPrec@1 31.250 (31.250)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (30.469)\n",
      "Training epoch: [748/2000][1/4]; Loss: 2.1603820374008342\n",
      "Training Accuracy Epoch: [747]\tPrec@1 33.594 (33.594)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (30.469)\n",
      "Training epoch: [749/2000][1/4]; Loss: 2.1618876384288495\n",
      "Training Accuracy Epoch: [748]\tPrec@1 37.500 (37.500)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (30.469)\n",
      "Training epoch: [750/2000][1/4]; Loss: 2.187832384822756\n",
      "Training Accuracy Epoch: [749]\tPrec@1 31.250 (31.250)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (28.125)\n",
      "Training epoch: [751/2000][1/4]; Loss: 2.151190519699447\n",
      "Training Accuracy Epoch: [750]\tPrec@1 35.938 (35.938)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (32.031)\n",
      "Training epoch: [752/2000][1/4]; Loss: 2.2170563192642914\n",
      "Training Accuracy Epoch: [751]\tPrec@1 28.906 (28.906)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (30.469)\n",
      "Training epoch: [753/2000][1/4]; Loss: 2.1549861298559048\n",
      "Training Accuracy Epoch: [752]\tPrec@1 33.594 (33.594)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.125)\n",
      "Training epoch: [754/2000][1/4]; Loss: 2.1578701102646116\n",
      "Training Accuracy Epoch: [753]\tPrec@1 31.250 (31.250)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (27.344)\n",
      "Training epoch: [755/2000][1/4]; Loss: 2.163549855532618\n",
      "Training Accuracy Epoch: [754]\tPrec@1 34.375 (34.375)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (30.469)\n",
      "Training epoch: [756/2000][1/4]; Loss: 2.2165487289518087\n",
      "Training Accuracy Epoch: [755]\tPrec@1 28.906 (28.906)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (25.781)\n",
      "Training epoch: [757/2000][1/4]; Loss: 2.1602493075623856\n",
      "Training Accuracy Epoch: [756]\tPrec@1 31.250 (31.250)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (28.906)\n",
      "Training epoch: [758/2000][1/4]; Loss: 2.1595559485885856\n",
      "Training Accuracy Epoch: [757]\tPrec@1 35.938 (35.938)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (31.250)\n",
      "Training epoch: [759/2000][1/4]; Loss: 2.2426493766518427\n",
      "Training Accuracy Epoch: [758]\tPrec@1 21.094 (21.094)\tPrec@2 33.594 (33.594)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.906)\n",
      "Training epoch: [760/2000][1/4]; Loss: 2.1928142100070884\n",
      "Training Accuracy Epoch: [759]\tPrec@1 25.781 (25.781)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (29.688)\n",
      "Training epoch: [761/2000][1/4]; Loss: 2.167378574695098\n",
      "Training Accuracy Epoch: [760]\tPrec@1 27.344 (27.344)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.125)\n",
      "Training epoch: [762/2000][1/4]; Loss: 2.151818930447599\n",
      "Training Accuracy Epoch: [761]\tPrec@1 34.375 (34.375)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (30.469)\n",
      "Training epoch: [763/2000][1/4]; Loss: 2.142756541719284\n",
      "Training Accuracy Epoch: [762]\tPrec@1 36.719 (36.719)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (32.031)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: [764/2000][1/4]; Loss: 2.167902241311564\n",
      "Training Accuracy Epoch: [763]\tPrec@1 30.469 (30.469)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.906)\n",
      "Training epoch: [765/2000][1/4]; Loss: 2.158922860919146\n",
      "Training Accuracy Epoch: [764]\tPrec@1 35.938 (35.938)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (30.469)\n",
      "Training epoch: [766/2000][1/4]; Loss: 2.1653142337397653\n",
      "Training Accuracy Epoch: [765]\tPrec@1 34.375 (34.375)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (28.906)\n",
      "Training epoch: [767/2000][1/4]; Loss: 2.178076898243628\n",
      "Training Accuracy Epoch: [766]\tPrec@1 30.469 (30.469)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (29.688)\n",
      "Training epoch: [768/2000][1/4]; Loss: 2.168491305886517\n",
      "Training Accuracy Epoch: [767]\tPrec@1 32.031 (32.031)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (25.000)\n",
      "Training epoch: [769/2000][1/4]; Loss: 2.1556798645954784\n",
      "Training Accuracy Epoch: [768]\tPrec@1 35.938 (35.938)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (27.344)\n",
      "Training epoch: [770/2000][1/4]; Loss: 2.148957007689056\n",
      "Training Accuracy Epoch: [769]\tPrec@1 35.938 (35.938)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (30.469)\n",
      "Training epoch: [771/2000][1/4]; Loss: 2.1547220499304096\n",
      "Training Accuracy Epoch: [770]\tPrec@1 35.938 (35.938)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (28.125)\n",
      "Training epoch: [772/2000][1/4]; Loss: 2.1802209791663514\n",
      "Training Accuracy Epoch: [771]\tPrec@1 30.469 (30.469)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (27.344)\n",
      "Training epoch: [773/2000][1/4]; Loss: 2.133827590422139\n",
      "Training Accuracy Epoch: [772]\tPrec@1 35.156 (35.156)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (26.562)\n",
      "Training epoch: [774/2000][1/4]; Loss: 2.165786751663018\n",
      "Training Accuracy Epoch: [773]\tPrec@1 31.250 (31.250)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (27.344)\n",
      "Training epoch: [775/2000][1/4]; Loss: 2.1539752402184345\n",
      "Training Accuracy Epoch: [774]\tPrec@1 30.469 (30.469)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (31.250)\n",
      "Training epoch: [776/2000][1/4]; Loss: 2.1572274091041366\n",
      "Training Accuracy Epoch: [775]\tPrec@1 36.719 (36.719)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (27.344)\n",
      "Training epoch: [777/2000][1/4]; Loss: 2.2036337612657824\n",
      "Training Accuracy Epoch: [776]\tPrec@1 25.781 (25.781)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (32.031)\n",
      "Training epoch: [778/2000][1/4]; Loss: 2.1562404077001456\n",
      "Training Accuracy Epoch: [777]\tPrec@1 30.469 (30.469)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (28.906)\n",
      "Training epoch: [779/2000][1/4]; Loss: 2.168223548809466\n",
      "Training Accuracy Epoch: [778]\tPrec@1 32.031 (32.031)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (32.812)\n",
      "Training epoch: [780/2000][1/4]; Loss: 2.1459439653758183\n",
      "Training Accuracy Epoch: [779]\tPrec@1 34.375 (34.375)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (31.250)\n",
      "Training epoch: [781/2000][1/4]; Loss: 2.176615306919381\n",
      "Training Accuracy Epoch: [780]\tPrec@1 36.719 (36.719)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (29.688)\n",
      "Training epoch: [782/2000][1/4]; Loss: 2.1833643317333653\n",
      "Training Accuracy Epoch: [781]\tPrec@1 26.562 (26.562)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (28.906)\n",
      "Training epoch: [783/2000][1/4]; Loss: 2.1553811061336106\n",
      "Training Accuracy Epoch: [782]\tPrec@1 35.156 (35.156)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (30.469)\n",
      "Training epoch: [784/2000][1/4]; Loss: 2.1848557373029824\n",
      "Training Accuracy Epoch: [783]\tPrec@1 29.688 (29.688)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (28.125)\n",
      "Training epoch: [785/2000][1/4]; Loss: 2.1646038511564525\n",
      "Training Accuracy Epoch: [784]\tPrec@1 33.594 (33.594)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (28.906)\n",
      "Training epoch: [786/2000][1/4]; Loss: 2.157429746885996\n",
      "Training Accuracy Epoch: [785]\tPrec@1 29.688 (29.688)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (35.156)\n",
      "Training epoch: [787/2000][1/4]; Loss: 2.1119335585804073\n",
      "Training Accuracy Epoch: [786]\tPrec@1 36.719 (36.719)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (26.562)\n",
      "Training epoch: [788/2000][1/4]; Loss: 2.182644796722645\n",
      "Training Accuracy Epoch: [787]\tPrec@1 26.562 (26.562)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.906)\n",
      "Training epoch: [789/2000][1/4]; Loss: 2.1190253076062726\n",
      "Training Accuracy Epoch: [788]\tPrec@1 33.594 (33.594)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (27.344)\n",
      "Training epoch: [790/2000][1/4]; Loss: 2.1305343895871585\n",
      "Training Accuracy Epoch: [789]\tPrec@1 41.406 (41.406)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (27.344)\n",
      "Training epoch: [791/2000][1/4]; Loss: 2.1368541035338486\n",
      "Training Accuracy Epoch: [790]\tPrec@1 35.156 (35.156)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.812)\n",
      "Training epoch: [792/2000][1/4]; Loss: 2.180235874659663\n",
      "Training Accuracy Epoch: [791]\tPrec@1 27.344 (27.344)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (24.219)\n",
      "Training epoch: [793/2000][1/4]; Loss: 2.1557348362199775\n",
      "Training Accuracy Epoch: [792]\tPrec@1 29.688 (29.688)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (25.781)\n",
      "Training epoch: [794/2000][1/4]; Loss: 2.1315467302283713\n",
      "Training Accuracy Epoch: [793]\tPrec@1 33.594 (33.594)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (28.906)\n",
      "Training epoch: [795/2000][1/4]; Loss: 2.1643548290720265\n",
      "Training Accuracy Epoch: [794]\tPrec@1 31.250 (31.250)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (26.562)\n",
      "Training epoch: [796/2000][1/4]; Loss: 2.1332280220173683\n",
      "Training Accuracy Epoch: [795]\tPrec@1 40.625 (40.625)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (29.688)\n",
      "Training epoch: [797/2000][1/4]; Loss: 2.140045783375691\n",
      "Training Accuracy Epoch: [796]\tPrec@1 31.250 (31.250)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.031)\n",
      "Training epoch: [798/2000][1/4]; Loss: 2.1675227459495545\n",
      "Training Accuracy Epoch: [797]\tPrec@1 27.344 (27.344)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (34.375)\n",
      "Training epoch: [799/2000][1/4]; Loss: 2.1787896601180194\n",
      "Training Accuracy Epoch: [798]\tPrec@1 23.438 (23.438)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (26.562)\n",
      "Training epoch: [800/2000][1/4]; Loss: 2.1896082032846724\n",
      "Training Accuracy Epoch: [799]\tPrec@1 28.906 (28.906)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (28.906)\n",
      "Training epoch: [801/2000][1/4]; Loss: 2.137608606369705\n",
      "Training Accuracy Epoch: [800]\tPrec@1 31.250 (31.250)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (35.156)\n",
      "Training epoch: [802/2000][1/4]; Loss: 2.1755133361245376\n",
      "Training Accuracy Epoch: [801]\tPrec@1 28.125 (28.125)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (32.031)\n",
      "Training epoch: [803/2000][1/4]; Loss: 2.127890158539037\n",
      "Training Accuracy Epoch: [802]\tPrec@1 36.719 (36.719)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (32.031)\n",
      "Training epoch: [804/2000][1/4]; Loss: 2.146870170019933\n",
      "Training Accuracy Epoch: [803]\tPrec@1 29.688 (29.688)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (26.562)\n",
      "Training epoch: [805/2000][1/4]; Loss: 2.1798181815693276\n",
      "Training Accuracy Epoch: [804]\tPrec@1 29.688 (29.688)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (29.688)\n",
      "Training epoch: [806/2000][1/4]; Loss: 2.1049649040553606\n",
      "Training Accuracy Epoch: [805]\tPrec@1 42.969 (42.969)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (28.906)\n",
      "Training epoch: [807/2000][1/4]; Loss: 2.213130738010687\n",
      "Training Accuracy Epoch: [806]\tPrec@1 25.781 (25.781)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (29.688)\n",
      "Training epoch: [808/2000][1/4]; Loss: 2.1412882906515134\n",
      "Training Accuracy Epoch: [807]\tPrec@1 31.250 (31.250)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (28.125)\n",
      "Training epoch: [809/2000][1/4]; Loss: 2.1348392870490795\n",
      "Training Accuracy Epoch: [808]\tPrec@1 33.594 (33.594)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.031)\n",
      "Training epoch: [810/2000][1/4]; Loss: 2.1543780159813353\n",
      "Training Accuracy Epoch: [809]\tPrec@1 32.031 (32.031)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (28.906)\n",
      "Training epoch: [811/2000][1/4]; Loss: 2.145847506769666\n",
      "Training Accuracy Epoch: [810]\tPrec@1 28.125 (28.125)\tPrec@2 48.438 (48.438)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>>>\tPrec@1 (17.969)\tPrec@2 (33.594)\n",
      "Training epoch: [812/2000][1/4]; Loss: 2.1275694726778966\n",
      "Training Accuracy Epoch: [811]\tPrec@1 36.719 (36.719)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (32.031)\n",
      "Training epoch: [813/2000][1/4]; Loss: 2.1724170861988727\n",
      "Training Accuracy Epoch: [812]\tPrec@1 29.688 (29.688)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (30.469)\n",
      "Training epoch: [814/2000][1/4]; Loss: 2.151599252354725\n",
      "Training Accuracy Epoch: [813]\tPrec@1 33.594 (33.594)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (25.000)\n",
      "Training epoch: [815/2000][1/4]; Loss: 2.1580833413887097\n",
      "Training Accuracy Epoch: [814]\tPrec@1 30.469 (30.469)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (31.250)\n",
      "Training epoch: [816/2000][1/4]; Loss: 2.1254794472929173\n",
      "Training Accuracy Epoch: [815]\tPrec@1 41.406 (41.406)\tPrec@2 56.250 (56.250)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (29.688)\n",
      "Training epoch: [817/2000][1/4]; Loss: 2.1159390760279377\n",
      "Training Accuracy Epoch: [816]\tPrec@1 32.031 (32.031)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.906)\n",
      "Training epoch: [818/2000][1/4]; Loss: 2.121923017584965\n",
      "Training Accuracy Epoch: [817]\tPrec@1 31.250 (31.250)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (26.562)\n",
      "Training epoch: [819/2000][1/4]; Loss: 2.123781603273416\n",
      "Training Accuracy Epoch: [818]\tPrec@1 40.625 (40.625)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (32.812)\n",
      "Training epoch: [820/2000][1/4]; Loss: 2.1487072093998307\n",
      "Training Accuracy Epoch: [819]\tPrec@1 35.938 (35.938)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (30.469)\n",
      "Training epoch: [821/2000][1/4]; Loss: 2.156958747222245\n",
      "Training Accuracy Epoch: [820]\tPrec@1 29.688 (29.688)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.812)\n",
      "Training epoch: [822/2000][1/4]; Loss: 2.1842084487012468\n",
      "Training Accuracy Epoch: [821]\tPrec@1 29.688 (29.688)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (31.250)\n",
      "Training epoch: [823/2000][1/4]; Loss: 2.2073602520679296\n",
      "Training Accuracy Epoch: [822]\tPrec@1 21.094 (21.094)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (30.469)\n",
      "Training epoch: [824/2000][1/4]; Loss: 2.1370528458546194\n",
      "Training Accuracy Epoch: [823]\tPrec@1 46.875 (46.875)\tPrec@2 61.719 (61.719)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (28.906)\n",
      "Training epoch: [825/2000][1/4]; Loss: 2.174646594428427\n",
      "Training Accuracy Epoch: [824]\tPrec@1 26.562 (26.562)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (31.250)\n",
      "Training epoch: [826/2000][1/4]; Loss: 2.1627788679869218\n",
      "Training Accuracy Epoch: [825]\tPrec@1 32.031 (32.031)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (28.125)\n",
      "Training epoch: [827/2000][1/4]; Loss: 2.188057253746358\n",
      "Training Accuracy Epoch: [826]\tPrec@1 27.344 (27.344)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (27.344)\n",
      "Training epoch: [828/2000][1/4]; Loss: 2.1863664636826274\n",
      "Training Accuracy Epoch: [827]\tPrec@1 28.906 (28.906)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (39.844)\n",
      "Training epoch: [829/2000][1/4]; Loss: 2.2086313945339158\n",
      "Training Accuracy Epoch: [828]\tPrec@1 20.312 (20.312)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (30.469)\n",
      "Training epoch: [830/2000][1/4]; Loss: 2.141053212196197\n",
      "Training Accuracy Epoch: [829]\tPrec@1 30.469 (30.469)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (33.594)\n",
      "Training epoch: [831/2000][1/4]; Loss: 2.171413451454245\n",
      "Training Accuracy Epoch: [830]\tPrec@1 29.688 (29.688)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (31.250)\n",
      "Training epoch: [832/2000][1/4]; Loss: 2.1901493712216493\n",
      "Training Accuracy Epoch: [831]\tPrec@1 27.344 (27.344)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (28.125)\n",
      "Training epoch: [833/2000][1/4]; Loss: 2.1376229050846867\n",
      "Training Accuracy Epoch: [832]\tPrec@1 35.156 (35.156)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (26.562)\n",
      "Training epoch: [834/2000][1/4]; Loss: 2.1696208342703835\n",
      "Training Accuracy Epoch: [833]\tPrec@1 29.688 (29.688)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (31.250)\n",
      "Training epoch: [835/2000][1/4]; Loss: 2.0762097719192183\n",
      "Training Accuracy Epoch: [834]\tPrec@1 49.219 (49.219)\tPrec@2 60.938 (60.938)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (28.906)\n",
      "Training epoch: [836/2000][1/4]; Loss: 2.1416221680025194\n",
      "Training Accuracy Epoch: [835]\tPrec@1 33.594 (33.594)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.125)\n",
      "Training epoch: [837/2000][1/4]; Loss: 2.154865411877609\n",
      "Training Accuracy Epoch: [836]\tPrec@1 32.812 (32.812)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (26.562)\n",
      "Training epoch: [838/2000][1/4]; Loss: 2.122434716882936\n",
      "Training Accuracy Epoch: [837]\tPrec@1 36.719 (36.719)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (28.906)\n",
      "Training epoch: [839/2000][1/4]; Loss: 2.1740295627192787\n",
      "Training Accuracy Epoch: [838]\tPrec@1 30.469 (30.469)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (25.000)\n",
      "Training epoch: [840/2000][1/4]; Loss: 2.1374068627931533\n",
      "Training Accuracy Epoch: [839]\tPrec@1 30.469 (30.469)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (28.906)\n",
      "Training epoch: [841/2000][1/4]; Loss: 2.1697396870738457\n",
      "Training Accuracy Epoch: [840]\tPrec@1 26.562 (26.562)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (13.281)\tPrec@2 (28.906)\n",
      "Training epoch: [842/2000][1/4]; Loss: 2.1126310701755178\n",
      "Training Accuracy Epoch: [841]\tPrec@1 32.812 (32.812)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (26.562)\n",
      "Training epoch: [843/2000][1/4]; Loss: 2.1131258037510396\n",
      "Training Accuracy Epoch: [842]\tPrec@1 36.719 (36.719)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.031)\n",
      "Training epoch: [844/2000][1/4]; Loss: 2.144321653024876\n",
      "Training Accuracy Epoch: [843]\tPrec@1 30.469 (30.469)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (32.812)\n",
      "Training epoch: [845/2000][1/4]; Loss: 2.130918339476848\n",
      "Training Accuracy Epoch: [844]\tPrec@1 33.594 (33.594)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (25.000)\n",
      "Training epoch: [846/2000][1/4]; Loss: 2.127116957354164\n",
      "Training Accuracy Epoch: [845]\tPrec@1 38.281 (38.281)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (29.688)\n",
      "Training epoch: [847/2000][1/4]; Loss: 2.140539062580908\n",
      "Training Accuracy Epoch: [846]\tPrec@1 33.594 (33.594)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.031)\n",
      "Training epoch: [848/2000][1/4]; Loss: 2.144377519494401\n",
      "Training Accuracy Epoch: [847]\tPrec@1 33.594 (33.594)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (29.688)\n",
      "Training epoch: [849/2000][1/4]; Loss: 2.172881140818274\n",
      "Training Accuracy Epoch: [848]\tPrec@1 29.688 (29.688)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (25.781)\n",
      "Training epoch: [850/2000][1/4]; Loss: 2.1538750617047455\n",
      "Training Accuracy Epoch: [849]\tPrec@1 28.125 (28.125)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.812)\n",
      "Training epoch: [851/2000][1/4]; Loss: 2.1241514002628006\n",
      "Training Accuracy Epoch: [850]\tPrec@1 33.594 (33.594)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (31.250)\n",
      "Training epoch: [852/2000][1/4]; Loss: 2.1436878691846797\n",
      "Training Accuracy Epoch: [851]\tPrec@1 30.469 (30.469)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.031)\n",
      "Training epoch: [853/2000][1/4]; Loss: 2.1189773255686664\n",
      "Training Accuracy Epoch: [852]\tPrec@1 35.938 (35.938)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (30.469)\n",
      "Training epoch: [854/2000][1/4]; Loss: 2.1147993840024237\n",
      "Training Accuracy Epoch: [853]\tPrec@1 39.062 (39.062)\tPrec@2 56.250 (56.250)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (26.562)\n",
      "Training epoch: [855/2000][1/4]; Loss: 2.0845367317909083\n",
      "Training Accuracy Epoch: [854]\tPrec@1 41.406 (41.406)\tPrec@2 57.031 (57.031)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (27.344)\n",
      "Training epoch: [856/2000][1/4]; Loss: 2.1626338711207445\n",
      "Training Accuracy Epoch: [855]\tPrec@1 28.906 (28.906)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (37.500)\n",
      "Training epoch: [857/2000][1/4]; Loss: 2.098264665033898\n",
      "Training Accuracy Epoch: [856]\tPrec@1 38.281 (38.281)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (29.688)\n",
      "Training epoch: [858/2000][1/4]; Loss: 2.111616917255877\n",
      "Training Accuracy Epoch: [857]\tPrec@1 35.156 (35.156)\tPrec@2 57.812 (57.812)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (32.812)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: [859/2000][1/4]; Loss: 2.1361900191357783\n",
      "Training Accuracy Epoch: [858]\tPrec@1 38.281 (38.281)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.031)\n",
      "Training epoch: [860/2000][1/4]; Loss: 2.1259272813217103\n",
      "Training Accuracy Epoch: [859]\tPrec@1 37.500 (37.500)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (31.250)\n",
      "Training epoch: [861/2000][1/4]; Loss: 2.1708149795647125\n",
      "Training Accuracy Epoch: [860]\tPrec@1 28.906 (28.906)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (28.125)\n",
      "Training epoch: [862/2000][1/4]; Loss: 2.114190572812526\n",
      "Training Accuracy Epoch: [861]\tPrec@1 39.844 (39.844)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (31.250)\n",
      "Training epoch: [863/2000][1/4]; Loss: 2.129290895801652\n",
      "Training Accuracy Epoch: [862]\tPrec@1 36.719 (36.719)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (31.250)\n",
      "Training epoch: [864/2000][1/4]; Loss: 2.1458156317862453\n",
      "Training Accuracy Epoch: [863]\tPrec@1 32.812 (32.812)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (28.125)\n",
      "Training epoch: [865/2000][1/4]; Loss: 2.095248089579307\n",
      "Training Accuracy Epoch: [864]\tPrec@1 42.969 (42.969)\tPrec@2 57.812 (57.812)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (31.250)\n",
      "Training epoch: [866/2000][1/4]; Loss: 2.1328905320020195\n",
      "Training Accuracy Epoch: [865]\tPrec@1 34.375 (34.375)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (32.812)\n",
      "Training epoch: [867/2000][1/4]; Loss: 2.160883051278622\n",
      "Training Accuracy Epoch: [866]\tPrec@1 28.125 (28.125)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (27.344)\n",
      "Training epoch: [868/2000][1/4]; Loss: 2.1098674317085204\n",
      "Training Accuracy Epoch: [867]\tPrec@1 34.375 (34.375)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (28.906)\n",
      "Training epoch: [869/2000][1/4]; Loss: 2.139413767295983\n",
      "Training Accuracy Epoch: [868]\tPrec@1 34.375 (34.375)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (14.062)\tPrec@2 (26.562)\n",
      "Training epoch: [870/2000][1/4]; Loss: 2.141045139620698\n",
      "Training Accuracy Epoch: [869]\tPrec@1 29.688 (29.688)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (28.906)\n",
      "Training epoch: [871/2000][1/4]; Loss: 2.1633016050706493\n",
      "Training Accuracy Epoch: [870]\tPrec@1 26.562 (26.562)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (34.375)\n",
      "Training epoch: [872/2000][1/4]; Loss: 2.15955202253436\n",
      "Training Accuracy Epoch: [871]\tPrec@1 29.688 (29.688)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (30.469)\n",
      "Training epoch: [873/2000][1/4]; Loss: 2.09063960829992\n",
      "Training Accuracy Epoch: [872]\tPrec@1 37.500 (37.500)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.031)\n",
      "Training epoch: [874/2000][1/4]; Loss: 2.1198786157046947\n",
      "Training Accuracy Epoch: [873]\tPrec@1 36.719 (36.719)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (31.250)\n",
      "Training epoch: [875/2000][1/4]; Loss: 2.106482555395588\n",
      "Training Accuracy Epoch: [874]\tPrec@1 39.062 (39.062)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.812)\n",
      "Training epoch: [876/2000][1/4]; Loss: 2.0906584486358826\n",
      "Training Accuracy Epoch: [875]\tPrec@1 38.281 (38.281)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (31.250)\n",
      "Training epoch: [877/2000][1/4]; Loss: 2.0819694482994713\n",
      "Training Accuracy Epoch: [876]\tPrec@1 45.312 (45.312)\tPrec@2 56.250 (56.250)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (26.562)\n",
      "Training epoch: [878/2000][1/4]; Loss: 2.1170163166118776\n",
      "Training Accuracy Epoch: [877]\tPrec@1 35.938 (35.938)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (27.344)\n",
      "Training epoch: [879/2000][1/4]; Loss: 2.121516410959399\n",
      "Training Accuracy Epoch: [878]\tPrec@1 39.844 (39.844)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (30.469)\n",
      "Training epoch: [880/2000][1/4]; Loss: 2.1594073284144404\n",
      "Training Accuracy Epoch: [879]\tPrec@1 25.000 (25.000)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (28.125)\n",
      "Training epoch: [881/2000][1/4]; Loss: 2.0892094569141286\n",
      "Training Accuracy Epoch: [880]\tPrec@1 39.062 (39.062)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (31.250)\n",
      "Training epoch: [882/2000][1/4]; Loss: 2.1424986960026886\n",
      "Training Accuracy Epoch: [881]\tPrec@1 28.906 (28.906)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (29.688)\n",
      "Training epoch: [883/2000][1/4]; Loss: 2.097966011481175\n",
      "Training Accuracy Epoch: [882]\tPrec@1 37.500 (37.500)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (33.594)\n",
      "Training epoch: [884/2000][1/4]; Loss: 2.1231390390358325\n",
      "Training Accuracy Epoch: [883]\tPrec@1 32.812 (32.812)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (30.469)\n",
      "Training epoch: [885/2000][1/4]; Loss: 2.1286528557117306\n",
      "Training Accuracy Epoch: [884]\tPrec@1 33.594 (33.594)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (14.062)\tPrec@2 (25.781)\n",
      "Training epoch: [886/2000][1/4]; Loss: 2.11684325475691\n",
      "Training Accuracy Epoch: [885]\tPrec@1 41.406 (41.406)\tPrec@2 56.250 (56.250)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (28.125)\n",
      "Training epoch: [887/2000][1/4]; Loss: 2.074982841534745\n",
      "Training Accuracy Epoch: [886]\tPrec@1 42.188 (42.188)\tPrec@2 57.031 (57.031)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.125)\n",
      "Training epoch: [888/2000][1/4]; Loss: 2.104275674432595\n",
      "Training Accuracy Epoch: [887]\tPrec@1 39.844 (39.844)\tPrec@2 57.812 (57.812)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (29.688)\n",
      "Training epoch: [889/2000][1/4]; Loss: 2.117893987005064\n",
      "Training Accuracy Epoch: [888]\tPrec@1 34.375 (34.375)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (33.594)\n",
      "Training epoch: [890/2000][1/4]; Loss: 2.102994574915015\n",
      "Training Accuracy Epoch: [889]\tPrec@1 35.156 (35.156)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (32.812)\n",
      "Training epoch: [891/2000][1/4]; Loss: 2.131313293183571\n",
      "Training Accuracy Epoch: [890]\tPrec@1 26.562 (26.562)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (33.594)\n",
      "Training epoch: [892/2000][1/4]; Loss: 2.1238794525879023\n",
      "Training Accuracy Epoch: [891]\tPrec@1 34.375 (34.375)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (28.906)\n",
      "Training epoch: [893/2000][1/4]; Loss: 2.1269071764169065\n",
      "Training Accuracy Epoch: [892]\tPrec@1 35.938 (35.938)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (31.250)\n",
      "Training epoch: [894/2000][1/4]; Loss: 2.110330299826796\n",
      "Training Accuracy Epoch: [893]\tPrec@1 37.500 (37.500)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (35.938)\n",
      "Training epoch: [895/2000][1/4]; Loss: 2.14990941504135\n",
      "Training Accuracy Epoch: [894]\tPrec@1 33.594 (33.594)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (26.562)\n",
      "Training epoch: [896/2000][1/4]; Loss: 2.0972537738974744\n",
      "Training Accuracy Epoch: [895]\tPrec@1 34.375 (34.375)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (31.250)\n",
      "Training epoch: [897/2000][1/4]; Loss: 2.138176416241032\n",
      "Training Accuracy Epoch: [896]\tPrec@1 32.031 (32.031)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (26.562)\n",
      "Training epoch: [898/2000][1/4]; Loss: 2.1407004586643965\n",
      "Training Accuracy Epoch: [897]\tPrec@1 32.812 (32.812)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (31.250)\n",
      "Training epoch: [899/2000][1/4]; Loss: 2.1101682562336816\n",
      "Training Accuracy Epoch: [898]\tPrec@1 32.812 (32.812)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (31.250)\n",
      "Training epoch: [900/2000][1/4]; Loss: 2.0909897230318757\n",
      "Training Accuracy Epoch: [899]\tPrec@1 35.938 (35.938)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (33.594)\n",
      "Training epoch: [901/2000][1/4]; Loss: 2.0918091492920707\n",
      "Training Accuracy Epoch: [900]\tPrec@1 39.844 (39.844)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (25.000)\n",
      "Training epoch: [902/2000][1/4]; Loss: 2.118884891510331\n",
      "Training Accuracy Epoch: [901]\tPrec@1 30.469 (30.469)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (30.469)\n",
      "Training epoch: [903/2000][1/4]; Loss: 2.079092372284445\n",
      "Training Accuracy Epoch: [902]\tPrec@1 41.406 (41.406)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (27.344)\n",
      "Training epoch: [904/2000][1/4]; Loss: 2.120555643767695\n",
      "Training Accuracy Epoch: [903]\tPrec@1 31.250 (31.250)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (30.469)\n",
      "Training epoch: [905/2000][1/4]; Loss: 2.117695061121438\n",
      "Training Accuracy Epoch: [904]\tPrec@1 32.812 (32.812)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (30.469)\n",
      "Training epoch: [906/2000][1/4]; Loss: 2.1435053231494514\n",
      "Training Accuracy Epoch: [905]\tPrec@1 30.469 (30.469)\tPrec@2 44.531 (44.531)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>>>\tPrec@1 (14.844)\tPrec@2 (30.469)\n",
      "Training epoch: [907/2000][1/4]; Loss: 2.1284401231243297\n",
      "Training Accuracy Epoch: [906]\tPrec@1 32.812 (32.812)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (25.781)\tPrec@2 (29.688)\n",
      "Training epoch: [908/2000][1/4]; Loss: 2.0916588082361756\n",
      "Training Accuracy Epoch: [907]\tPrec@1 38.281 (38.281)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (14.062)\tPrec@2 (28.906)\n",
      "Training epoch: [909/2000][1/4]; Loss: 2.1071874272376165\n",
      "Training Accuracy Epoch: [908]\tPrec@1 36.719 (36.719)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (25.000)\tPrec@2 (32.031)\n",
      "Training epoch: [910/2000][1/4]; Loss: 2.1398103648915274\n",
      "Training Accuracy Epoch: [909]\tPrec@1 34.375 (34.375)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.031)\n",
      "Training epoch: [911/2000][1/4]; Loss: 2.2117288210952886\n",
      "Training Accuracy Epoch: [910]\tPrec@1 25.781 (25.781)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (26.562)\tPrec@2 (37.500)\n",
      "Training epoch: [912/2000][1/4]; Loss: 2.1411902232915594\n",
      "Training Accuracy Epoch: [911]\tPrec@1 29.688 (29.688)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (30.469)\n",
      "Training epoch: [913/2000][1/4]; Loss: 2.1103500053458877\n",
      "Training Accuracy Epoch: [912]\tPrec@1 32.812 (32.812)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (28.125)\n",
      "Training epoch: [914/2000][1/4]; Loss: 2.072232486082307\n",
      "Training Accuracy Epoch: [913]\tPrec@1 39.844 (39.844)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (33.594)\n",
      "Training epoch: [915/2000][1/4]; Loss: 2.1144726854745306\n",
      "Training Accuracy Epoch: [914]\tPrec@1 33.594 (33.594)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (13.281)\tPrec@2 (23.438)\n",
      "Training epoch: [916/2000][1/4]; Loss: 2.12095130224431\n",
      "Training Accuracy Epoch: [915]\tPrec@1 31.250 (31.250)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (34.375)\n",
      "Training epoch: [917/2000][1/4]; Loss: 2.1143966235037843\n",
      "Training Accuracy Epoch: [916]\tPrec@1 35.156 (35.156)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (30.469)\n",
      "Training epoch: [918/2000][1/4]; Loss: 2.122468426054983\n",
      "Training Accuracy Epoch: [917]\tPrec@1 31.250 (31.250)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (30.469)\n",
      "Training epoch: [919/2000][1/4]; Loss: 2.0630907698991607\n",
      "Training Accuracy Epoch: [918]\tPrec@1 35.938 (35.938)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (32.031)\n",
      "Training epoch: [920/2000][1/4]; Loss: 2.1037406637327862\n",
      "Training Accuracy Epoch: [919]\tPrec@1 32.812 (32.812)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (32.812)\n",
      "Training epoch: [921/2000][1/4]; Loss: 2.1261311971349164\n",
      "Training Accuracy Epoch: [920]\tPrec@1 32.812 (32.812)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (31.250)\n",
      "Training epoch: [922/2000][1/4]; Loss: 2.1232144079697988\n",
      "Training Accuracy Epoch: [921]\tPrec@1 35.156 (35.156)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (14.062)\tPrec@2 (28.125)\n",
      "Training epoch: [923/2000][1/4]; Loss: 2.1353613919696213\n",
      "Training Accuracy Epoch: [922]\tPrec@1 28.906 (28.906)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (14.062)\tPrec@2 (28.906)\n",
      "Training epoch: [924/2000][1/4]; Loss: 2.102383406946957\n",
      "Training Accuracy Epoch: [923]\tPrec@1 32.031 (32.031)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (29.688)\n",
      "Training epoch: [925/2000][1/4]; Loss: 2.1534904171949725\n",
      "Training Accuracy Epoch: [924]\tPrec@1 31.250 (31.250)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (14.062)\tPrec@2 (32.031)\n",
      "Training epoch: [926/2000][1/4]; Loss: 2.0969728043728524\n",
      "Training Accuracy Epoch: [925]\tPrec@1 37.500 (37.500)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (28.125)\n",
      "Training epoch: [927/2000][1/4]; Loss: 2.0835457807346107\n",
      "Training Accuracy Epoch: [926]\tPrec@1 36.719 (36.719)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (33.594)\n",
      "Training epoch: [928/2000][1/4]; Loss: 2.112565479719825\n",
      "Training Accuracy Epoch: [927]\tPrec@1 32.812 (32.812)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (31.250)\n",
      "Training epoch: [929/2000][1/4]; Loss: 2.133355459970271\n",
      "Training Accuracy Epoch: [928]\tPrec@1 35.938 (35.938)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (31.250)\n",
      "Training epoch: [930/2000][1/4]; Loss: 2.075794953181367\n",
      "Training Accuracy Epoch: [929]\tPrec@1 37.500 (37.500)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (32.031)\n",
      "Training epoch: [931/2000][1/4]; Loss: 2.1017604315418317\n",
      "Training Accuracy Epoch: [930]\tPrec@1 37.500 (37.500)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (26.562)\n",
      "Training epoch: [932/2000][1/4]; Loss: 2.1207033316466393\n",
      "Training Accuracy Epoch: [931]\tPrec@1 30.469 (30.469)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (30.469)\n",
      "Training epoch: [933/2000][1/4]; Loss: 2.0623762098993463\n",
      "Training Accuracy Epoch: [932]\tPrec@1 44.531 (44.531)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (25.000)\tPrec@2 (32.031)\n",
      "Training epoch: [934/2000][1/4]; Loss: 2.1386921351378434\n",
      "Training Accuracy Epoch: [933]\tPrec@1 25.781 (25.781)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (25.000)\n",
      "Training epoch: [935/2000][1/4]; Loss: 2.1054168215927223\n",
      "Training Accuracy Epoch: [934]\tPrec@1 34.375 (34.375)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (30.469)\n",
      "Training epoch: [936/2000][1/4]; Loss: 2.1237253317050904\n",
      "Training Accuracy Epoch: [935]\tPrec@1 24.219 (24.219)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (29.688)\n",
      "Training epoch: [937/2000][1/4]; Loss: 2.07349917365018\n",
      "Training Accuracy Epoch: [936]\tPrec@1 37.500 (37.500)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (32.031)\n",
      "Training epoch: [938/2000][1/4]; Loss: 2.133688249849674\n",
      "Training Accuracy Epoch: [937]\tPrec@1 33.594 (33.594)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (14.062)\tPrec@2 (26.562)\n",
      "Training epoch: [939/2000][1/4]; Loss: 2.1284522315170533\n",
      "Training Accuracy Epoch: [938]\tPrec@1 31.250 (31.250)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (32.031)\n",
      "Training epoch: [940/2000][1/4]; Loss: 2.097598860774248\n",
      "Training Accuracy Epoch: [939]\tPrec@1 34.375 (34.375)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.906)\n",
      "Training epoch: [941/2000][1/4]; Loss: 2.1440458508921547\n",
      "Training Accuracy Epoch: [940]\tPrec@1 30.469 (30.469)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (27.344)\n",
      "Training epoch: [942/2000][1/4]; Loss: 2.091806987215541\n",
      "Training Accuracy Epoch: [941]\tPrec@1 35.156 (35.156)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (32.031)\n",
      "Training epoch: [943/2000][1/4]; Loss: 2.1067955409163637\n",
      "Training Accuracy Epoch: [942]\tPrec@1 35.156 (35.156)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (29.688)\n",
      "Training epoch: [944/2000][1/4]; Loss: 2.1344910799837415\n",
      "Training Accuracy Epoch: [943]\tPrec@1 31.250 (31.250)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (29.688)\n",
      "Training epoch: [945/2000][1/4]; Loss: 2.11045152017005\n",
      "Training Accuracy Epoch: [944]\tPrec@1 33.594 (33.594)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (32.031)\n",
      "Training epoch: [946/2000][1/4]; Loss: 2.070314769272197\n",
      "Training Accuracy Epoch: [945]\tPrec@1 39.062 (39.062)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (25.000)\tPrec@2 (35.156)\n",
      "Training epoch: [947/2000][1/4]; Loss: 2.126321879602297\n",
      "Training Accuracy Epoch: [946]\tPrec@1 29.688 (29.688)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (14.062)\tPrec@2 (25.781)\n",
      "Training epoch: [948/2000][1/4]; Loss: 2.1039270455674637\n",
      "Training Accuracy Epoch: [947]\tPrec@1 37.500 (37.500)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (33.594)\n",
      "Training epoch: [949/2000][1/4]; Loss: 2.0968888748726306\n",
      "Training Accuracy Epoch: [948]\tPrec@1 33.594 (33.594)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (32.812)\n",
      "Training epoch: [950/2000][1/4]; Loss: 2.0621237172192934\n",
      "Training Accuracy Epoch: [949]\tPrec@1 39.062 (39.062)\tPrec@2 57.812 (57.812)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (33.594)\n",
      "Training epoch: [951/2000][1/4]; Loss: 2.1360364181628926\n",
      "Training Accuracy Epoch: [950]\tPrec@1 31.250 (31.250)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (29.688)\n",
      "Training epoch: [952/2000][1/4]; Loss: 2.1006231082680586\n",
      "Training Accuracy Epoch: [951]\tPrec@1 36.719 (36.719)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (28.906)\n",
      "Training epoch: [953/2000][1/4]; Loss: 2.0736074801149824\n",
      "Training Accuracy Epoch: [952]\tPrec@1 41.406 (41.406)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (29.688)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: [954/2000][1/4]; Loss: 2.1032842942565164\n",
      "Training Accuracy Epoch: [953]\tPrec@1 33.594 (33.594)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (32.031)\n",
      "Training epoch: [955/2000][1/4]; Loss: 2.1286556744906298\n",
      "Training Accuracy Epoch: [954]\tPrec@1 31.250 (31.250)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (28.125)\n",
      "Training epoch: [956/2000][1/4]; Loss: 2.1058779113449893\n",
      "Training Accuracy Epoch: [955]\tPrec@1 32.812 (32.812)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (30.469)\n",
      "Training epoch: [957/2000][1/4]; Loss: 2.1165592082994085\n",
      "Training Accuracy Epoch: [956]\tPrec@1 39.062 (39.062)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.125)\n",
      "Training epoch: [958/2000][1/4]; Loss: 2.1025764709165045\n",
      "Training Accuracy Epoch: [957]\tPrec@1 32.031 (32.031)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (30.469)\n",
      "Training epoch: [959/2000][1/4]; Loss: 2.129350463922383\n",
      "Training Accuracy Epoch: [958]\tPrec@1 31.250 (31.250)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (34.375)\n",
      "Training epoch: [960/2000][1/4]; Loss: 2.1144969277233274\n",
      "Training Accuracy Epoch: [959]\tPrec@1 31.250 (31.250)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.125)\n",
      "Training epoch: [961/2000][1/4]; Loss: 2.10441745424843\n",
      "Training Accuracy Epoch: [960]\tPrec@1 36.719 (36.719)\tPrec@2 57.031 (57.031)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (28.906)\n",
      "Training epoch: [962/2000][1/4]; Loss: 2.0882532798220708\n",
      "Training Accuracy Epoch: [961]\tPrec@1 34.375 (34.375)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (29.688)\n",
      "Training epoch: [963/2000][1/4]; Loss: 2.114260824852101\n",
      "Training Accuracy Epoch: [962]\tPrec@1 32.812 (32.812)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (28.906)\n",
      "Training epoch: [964/2000][1/4]; Loss: 2.060644621818267\n",
      "Training Accuracy Epoch: [963]\tPrec@1 37.500 (37.500)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (30.469)\n",
      "Training epoch: [965/2000][1/4]; Loss: 2.1033830523118926\n",
      "Training Accuracy Epoch: [964]\tPrec@1 32.812 (32.812)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (28.906)\n",
      "Training epoch: [966/2000][1/4]; Loss: 2.0872450499750133\n",
      "Training Accuracy Epoch: [965]\tPrec@1 36.719 (36.719)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (31.250)\n",
      "Training epoch: [967/2000][1/4]; Loss: 2.090831413214338\n",
      "Training Accuracy Epoch: [966]\tPrec@1 35.156 (35.156)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (31.250)\n",
      "Training epoch: [968/2000][1/4]; Loss: 2.093851949342817\n",
      "Training Accuracy Epoch: [967]\tPrec@1 32.031 (32.031)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.906)\n",
      "Training epoch: [969/2000][1/4]; Loss: 2.0731958880649963\n",
      "Training Accuracy Epoch: [968]\tPrec@1 35.938 (35.938)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.812)\n",
      "Training epoch: [970/2000][1/4]; Loss: 2.066424763405952\n",
      "Training Accuracy Epoch: [969]\tPrec@1 38.281 (38.281)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (28.906)\n",
      "Training epoch: [971/2000][1/4]; Loss: 2.1618589914723927\n",
      "Training Accuracy Epoch: [970]\tPrec@1 22.656 (22.656)\tPrec@2 34.375 (34.375)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (33.594)\n",
      "Training epoch: [972/2000][1/4]; Loss: 2.0994906499548125\n",
      "Training Accuracy Epoch: [971]\tPrec@1 36.719 (36.719)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (25.781)\n",
      "Training epoch: [973/2000][1/4]; Loss: 2.084283813164119\n",
      "Training Accuracy Epoch: [972]\tPrec@1 32.812 (32.812)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (34.375)\n",
      "Training epoch: [974/2000][1/4]; Loss: 2.0963719958133162\n",
      "Training Accuracy Epoch: [973]\tPrec@1 34.375 (34.375)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (28.906)\n",
      "Training epoch: [975/2000][1/4]; Loss: 2.117061988807943\n",
      "Training Accuracy Epoch: [974]\tPrec@1 34.375 (34.375)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (29.688)\n",
      "Training epoch: [976/2000][1/4]; Loss: 2.1234639509190036\n",
      "Training Accuracy Epoch: [975]\tPrec@1 32.812 (32.812)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (35.938)\n",
      "Training epoch: [977/2000][1/4]; Loss: 2.076046483234077\n",
      "Training Accuracy Epoch: [976]\tPrec@1 35.156 (35.156)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (27.344)\n",
      "Training epoch: [978/2000][1/4]; Loss: 2.1026143245748763\n",
      "Training Accuracy Epoch: [977]\tPrec@1 29.688 (29.688)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (32.031)\n",
      "Training epoch: [979/2000][1/4]; Loss: 2.0671017103952987\n",
      "Training Accuracy Epoch: [978]\tPrec@1 35.156 (35.156)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (28.906)\n",
      "Training epoch: [980/2000][1/4]; Loss: 2.1135355402821108\n",
      "Training Accuracy Epoch: [979]\tPrec@1 30.469 (30.469)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.906)\n",
      "Training epoch: [981/2000][1/4]; Loss: 2.0811889754909303\n",
      "Training Accuracy Epoch: [980]\tPrec@1 38.281 (38.281)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (29.688)\n",
      "Training epoch: [982/2000][1/4]; Loss: 2.0465454904941587\n",
      "Training Accuracy Epoch: [981]\tPrec@1 39.062 (39.062)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (36.719)\n",
      "Training epoch: [983/2000][1/4]; Loss: 2.061435160979831\n",
      "Training Accuracy Epoch: [982]\tPrec@1 38.281 (38.281)\tPrec@2 57.031 (57.031)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (28.125)\n",
      "Training epoch: [984/2000][1/4]; Loss: 2.1290802204338766\n",
      "Training Accuracy Epoch: [983]\tPrec@1 29.688 (29.688)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (30.469)\n",
      "Training epoch: [985/2000][1/4]; Loss: 2.0491465757658895\n",
      "Training Accuracy Epoch: [984]\tPrec@1 39.062 (39.062)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (30.469)\n",
      "Training epoch: [986/2000][1/4]; Loss: 2.07407610190333\n",
      "Training Accuracy Epoch: [985]\tPrec@1 35.938 (35.938)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (35.938)\n",
      "Training epoch: [987/2000][1/4]; Loss: 2.026879769305465\n",
      "Training Accuracy Epoch: [986]\tPrec@1 39.844 (39.844)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.906)\n",
      "Training epoch: [988/2000][1/4]; Loss: 2.0614704704454057\n",
      "Training Accuracy Epoch: [987]\tPrec@1 36.719 (36.719)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (26.562)\n",
      "Training epoch: [989/2000][1/4]; Loss: 2.0855695983609044\n",
      "Training Accuracy Epoch: [988]\tPrec@1 32.031 (32.031)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (31.250)\n",
      "Training epoch: [990/2000][1/4]; Loss: 2.082877285373721\n",
      "Training Accuracy Epoch: [989]\tPrec@1 36.719 (36.719)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.906)\n",
      "Training epoch: [991/2000][1/4]; Loss: 2.051988794470478\n",
      "Training Accuracy Epoch: [990]\tPrec@1 35.156 (35.156)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (29.688)\n",
      "Training epoch: [992/2000][1/4]; Loss: 2.0695857752649194\n",
      "Training Accuracy Epoch: [991]\tPrec@1 41.406 (41.406)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (28.125)\n",
      "Training epoch: [993/2000][1/4]; Loss: 2.0440082990728223\n",
      "Training Accuracy Epoch: [992]\tPrec@1 39.844 (39.844)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (32.812)\n",
      "Training epoch: [994/2000][1/4]; Loss: 2.0963637528501966\n",
      "Training Accuracy Epoch: [993]\tPrec@1 34.375 (34.375)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.812)\n",
      "Training epoch: [995/2000][1/4]; Loss: 2.152511471505091\n",
      "Training Accuracy Epoch: [994]\tPrec@1 26.562 (26.562)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (26.562)\n",
      "Training epoch: [996/2000][1/4]; Loss: 2.102092668614692\n",
      "Training Accuracy Epoch: [995]\tPrec@1 33.594 (33.594)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (28.906)\n",
      "Training epoch: [997/2000][1/4]; Loss: 2.085047847343016\n",
      "Training Accuracy Epoch: [996]\tPrec@1 33.594 (33.594)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (35.938)\n",
      "Training epoch: [998/2000][1/4]; Loss: 2.098002104933439\n",
      "Training Accuracy Epoch: [997]\tPrec@1 33.594 (33.594)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (32.812)\n",
      "Training epoch: [999/2000][1/4]; Loss: 2.0743299217421174\n",
      "Training Accuracy Epoch: [998]\tPrec@1 31.250 (31.250)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (32.812)\n",
      "Training epoch: [1000/2000][1/4]; Loss: 2.0509973655117095\n",
      "Training Accuracy Epoch: [999]\tPrec@1 34.375 (34.375)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (35.156)\n",
      "Training epoch: [1001/2000][1/4]; Loss: 2.071831518965277\n",
      "Training Accuracy Epoch: [1000]\tPrec@1 35.938 (35.938)\tPrec@2 52.344 (52.344)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>>>\tPrec@1 (15.625)\tPrec@2 (29.688)\n",
      "Training epoch: [1002/2000][1/4]; Loss: 2.162743864212728\n",
      "Training Accuracy Epoch: [1001]\tPrec@1 26.562 (26.562)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (25.000)\n",
      "Training epoch: [1003/2000][1/4]; Loss: 2.0162694159879817\n",
      "Training Accuracy Epoch: [1002]\tPrec@1 38.281 (38.281)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (31.250)\n",
      "Training epoch: [1004/2000][1/4]; Loss: 2.1033819371916525\n",
      "Training Accuracy Epoch: [1003]\tPrec@1 35.938 (35.938)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (30.469)\n",
      "Training epoch: [1005/2000][1/4]; Loss: 2.108580606024466\n",
      "Training Accuracy Epoch: [1004]\tPrec@1 35.938 (35.938)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (30.469)\n",
      "Training epoch: [1006/2000][1/4]; Loss: 2.095937912941195\n",
      "Training Accuracy Epoch: [1005]\tPrec@1 32.812 (32.812)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (32.031)\n",
      "Training epoch: [1007/2000][1/4]; Loss: 2.0642265954733157\n",
      "Training Accuracy Epoch: [1006]\tPrec@1 36.719 (36.719)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (30.469)\n",
      "Training epoch: [1008/2000][1/4]; Loss: 2.0671330495781053\n",
      "Training Accuracy Epoch: [1007]\tPrec@1 37.500 (37.500)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (28.125)\n",
      "Training epoch: [1009/2000][1/4]; Loss: 2.0795649348147487\n",
      "Training Accuracy Epoch: [1008]\tPrec@1 30.469 (30.469)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (26.562)\n",
      "Training epoch: [1010/2000][1/4]; Loss: 2.086564385680635\n",
      "Training Accuracy Epoch: [1009]\tPrec@1 28.906 (28.906)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (31.250)\n",
      "Training epoch: [1011/2000][1/4]; Loss: 2.0963233162655612\n",
      "Training Accuracy Epoch: [1010]\tPrec@1 27.344 (27.344)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (37.500)\n",
      "Training epoch: [1012/2000][1/4]; Loss: 2.0717645287287945\n",
      "Training Accuracy Epoch: [1011]\tPrec@1 40.625 (40.625)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (32.031)\n",
      "Training epoch: [1013/2000][1/4]; Loss: 2.017753349536025\n",
      "Training Accuracy Epoch: [1012]\tPrec@1 42.969 (42.969)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (29.688)\n",
      "Training epoch: [1014/2000][1/4]; Loss: 2.117530014012585\n",
      "Training Accuracy Epoch: [1013]\tPrec@1 35.938 (35.938)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (28.906)\n",
      "Training epoch: [1015/2000][1/4]; Loss: 2.0248350876266206\n",
      "Training Accuracy Epoch: [1014]\tPrec@1 36.719 (36.719)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (31.250)\n",
      "Training epoch: [1016/2000][1/4]; Loss: 2.073124752204877\n",
      "Training Accuracy Epoch: [1015]\tPrec@1 34.375 (34.375)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (30.469)\n",
      "Training epoch: [1017/2000][1/4]; Loss: 2.050612163702146\n",
      "Training Accuracy Epoch: [1016]\tPrec@1 35.156 (35.156)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.812)\n",
      "Training epoch: [1018/2000][1/4]; Loss: 2.103191798505332\n",
      "Training Accuracy Epoch: [1017]\tPrec@1 34.375 (34.375)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (31.250)\n",
      "Training epoch: [1019/2000][1/4]; Loss: 2.0336387298546468\n",
      "Training Accuracy Epoch: [1018]\tPrec@1 39.062 (39.062)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (31.250)\n",
      "Training epoch: [1020/2000][1/4]; Loss: 2.0686640654415958\n",
      "Training Accuracy Epoch: [1019]\tPrec@1 42.969 (42.969)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.812)\n",
      "Training epoch: [1021/2000][1/4]; Loss: 2.0847889984249175\n",
      "Training Accuracy Epoch: [1020]\tPrec@1 32.031 (32.031)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (30.469)\n",
      "Training epoch: [1022/2000][1/4]; Loss: 2.1071669350431717\n",
      "Training Accuracy Epoch: [1021]\tPrec@1 36.719 (36.719)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (36.719)\n",
      "Training epoch: [1023/2000][1/4]; Loss: 2.1056962172007676\n",
      "Training Accuracy Epoch: [1022]\tPrec@1 30.469 (30.469)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (35.156)\n",
      "Training epoch: [1024/2000][1/4]; Loss: 2.0641455759711405\n",
      "Training Accuracy Epoch: [1023]\tPrec@1 35.156 (35.156)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (34.375)\n",
      "Training epoch: [1025/2000][1/4]; Loss: 2.1131501025550903\n",
      "Training Accuracy Epoch: [1024]\tPrec@1 31.250 (31.250)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (29.688)\n",
      "Training epoch: [1026/2000][1/4]; Loss: 2.0460106010456065\n",
      "Training Accuracy Epoch: [1025]\tPrec@1 38.281 (38.281)\tPrec@2 56.250 (56.250)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (32.031)\n",
      "Training epoch: [1027/2000][1/4]; Loss: 2.0758770454894098\n",
      "Training Accuracy Epoch: [1026]\tPrec@1 27.344 (27.344)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (35.938)\n",
      "Training epoch: [1028/2000][1/4]; Loss: 2.108057692700386\n",
      "Training Accuracy Epoch: [1027]\tPrec@1 35.156 (35.156)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (31.250)\n",
      "Training epoch: [1029/2000][1/4]; Loss: 2.0789088907800912\n",
      "Training Accuracy Epoch: [1028]\tPrec@1 35.938 (35.938)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (29.688)\n",
      "Training epoch: [1030/2000][1/4]; Loss: 2.089495827552712\n",
      "Training Accuracy Epoch: [1029]\tPrec@1 33.594 (33.594)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (29.688)\n",
      "Training epoch: [1031/2000][1/4]; Loss: 2.096946977492405\n",
      "Training Accuracy Epoch: [1030]\tPrec@1 31.250 (31.250)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (29.688)\n",
      "Training epoch: [1032/2000][1/4]; Loss: 2.122329001807998\n",
      "Training Accuracy Epoch: [1031]\tPrec@1 28.125 (28.125)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (14.062)\tPrec@2 (25.781)\n",
      "Training epoch: [1033/2000][1/4]; Loss: 2.0541817278286096\n",
      "Training Accuracy Epoch: [1032]\tPrec@1 39.062 (39.062)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (33.594)\n",
      "Training epoch: [1034/2000][1/4]; Loss: 2.0703770952595812\n",
      "Training Accuracy Epoch: [1033]\tPrec@1 32.812 (32.812)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (27.344)\n",
      "Training epoch: [1035/2000][1/4]; Loss: 2.0812179774678006\n",
      "Training Accuracy Epoch: [1034]\tPrec@1 32.031 (32.031)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (32.031)\n",
      "Training epoch: [1036/2000][1/4]; Loss: 2.0683384873618547\n",
      "Training Accuracy Epoch: [1035]\tPrec@1 40.625 (40.625)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (29.688)\n",
      "Training epoch: [1037/2000][1/4]; Loss: 2.074249012012374\n",
      "Training Accuracy Epoch: [1036]\tPrec@1 35.938 (35.938)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (32.031)\n",
      "Training epoch: [1038/2000][1/4]; Loss: 2.117374476781812\n",
      "Training Accuracy Epoch: [1037]\tPrec@1 25.781 (25.781)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (29.688)\n",
      "Training epoch: [1039/2000][1/4]; Loss: 2.0700903382861644\n",
      "Training Accuracy Epoch: [1038]\tPrec@1 33.594 (33.594)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (28.906)\n",
      "Training epoch: [1040/2000][1/4]; Loss: 2.0467493744637437\n",
      "Training Accuracy Epoch: [1039]\tPrec@1 36.719 (36.719)\tPrec@2 56.250 (56.250)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (28.906)\n",
      "Training epoch: [1041/2000][1/4]; Loss: 2.046488576303162\n",
      "Training Accuracy Epoch: [1040]\tPrec@1 37.500 (37.500)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (32.031)\n",
      "Training epoch: [1042/2000][1/4]; Loss: 2.0602725579981023\n",
      "Training Accuracy Epoch: [1041]\tPrec@1 35.938 (35.938)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (32.031)\n",
      "Training epoch: [1043/2000][1/4]; Loss: 2.021451596715054\n",
      "Training Accuracy Epoch: [1042]\tPrec@1 45.312 (45.312)\tPrec@2 57.812 (57.812)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (29.688)\n",
      "Training epoch: [1044/2000][1/4]; Loss: 2.0958709176008443\n",
      "Training Accuracy Epoch: [1043]\tPrec@1 29.688 (29.688)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (29.688)\n",
      "Training epoch: [1045/2000][1/4]; Loss: 2.085088307043167\n",
      "Training Accuracy Epoch: [1044]\tPrec@1 32.812 (32.812)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (27.344)\n",
      "Training epoch: [1046/2000][1/4]; Loss: 2.1130714254923344\n",
      "Training Accuracy Epoch: [1045]\tPrec@1 25.000 (25.000)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (31.250)\n",
      "Training epoch: [1047/2000][1/4]; Loss: 2.060964240296491\n",
      "Training Accuracy Epoch: [1046]\tPrec@1 36.719 (36.719)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (32.031)\n",
      "Training epoch: [1048/2000][1/4]; Loss: 2.077772081642965\n",
      "Training Accuracy Epoch: [1047]\tPrec@1 35.156 (35.156)\tPrec@2 49.219 (49.219)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>>>\tPrec@1 (18.750)\tPrec@2 (30.469)\n",
      "Training epoch: [1049/2000][1/4]; Loss: 2.072355313433875\n",
      "Training Accuracy Epoch: [1048]\tPrec@1 34.375 (34.375)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (28.125)\n",
      "Training epoch: [1050/2000][1/4]; Loss: 2.0642493133386677\n",
      "Training Accuracy Epoch: [1049]\tPrec@1 32.031 (32.031)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (31.250)\n",
      "Training epoch: [1051/2000][1/4]; Loss: 2.051576768569508\n",
      "Training Accuracy Epoch: [1050]\tPrec@1 42.188 (42.188)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (13.281)\tPrec@2 (25.781)\n",
      "Training epoch: [1052/2000][1/4]; Loss: 2.0624813595364357\n",
      "Training Accuracy Epoch: [1051]\tPrec@1 32.812 (32.812)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (28.125)\n",
      "Training epoch: [1053/2000][1/4]; Loss: 2.064026399742457\n",
      "Training Accuracy Epoch: [1052]\tPrec@1 35.156 (35.156)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (30.469)\n",
      "Training epoch: [1054/2000][1/4]; Loss: 2.0936077344716812\n",
      "Training Accuracy Epoch: [1053]\tPrec@1 31.250 (31.250)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (31.250)\n",
      "Training epoch: [1055/2000][1/4]; Loss: 2.0730684664330403\n",
      "Training Accuracy Epoch: [1054]\tPrec@1 39.062 (39.062)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (29.688)\n",
      "Training epoch: [1056/2000][1/4]; Loss: 2.0848997760690304\n",
      "Training Accuracy Epoch: [1055]\tPrec@1 28.906 (28.906)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (34.375)\n",
      "Training epoch: [1057/2000][1/4]; Loss: 2.040156233354061\n",
      "Training Accuracy Epoch: [1056]\tPrec@1 39.844 (39.844)\tPrec@2 57.812 (57.812)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.906)\n",
      "Training epoch: [1058/2000][1/4]; Loss: 2.097647909336667\n",
      "Training Accuracy Epoch: [1057]\tPrec@1 36.719 (36.719)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (32.812)\n",
      "Training epoch: [1059/2000][1/4]; Loss: 2.112855972474189\n",
      "Training Accuracy Epoch: [1058]\tPrec@1 28.906 (28.906)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (31.250)\n",
      "Training epoch: [1060/2000][1/4]; Loss: 2.0710630558104155\n",
      "Training Accuracy Epoch: [1059]\tPrec@1 32.812 (32.812)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (31.250)\n",
      "Training epoch: [1061/2000][1/4]; Loss: 2.059783588578114\n",
      "Training Accuracy Epoch: [1060]\tPrec@1 36.719 (36.719)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (31.250)\n",
      "Training epoch: [1062/2000][1/4]; Loss: 2.0857216694905394\n",
      "Training Accuracy Epoch: [1061]\tPrec@1 31.250 (31.250)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (29.688)\n",
      "Training epoch: [1063/2000][1/4]; Loss: 2.1405378633975554\n",
      "Training Accuracy Epoch: [1062]\tPrec@1 21.875 (21.875)\tPrec@2 38.281 (38.281)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (32.812)\n",
      "Training epoch: [1064/2000][1/4]; Loss: 2.078845424792505\n",
      "Training Accuracy Epoch: [1063]\tPrec@1 35.156 (35.156)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (25.781)\n",
      "Training epoch: [1065/2000][1/4]; Loss: 2.0652776625439015\n",
      "Training Accuracy Epoch: [1064]\tPrec@1 42.188 (42.188)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (31.250)\n",
      "Training epoch: [1066/2000][1/4]; Loss: 2.0956354577985303\n",
      "Training Accuracy Epoch: [1065]\tPrec@1 30.469 (30.469)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (34.375)\n",
      "Training epoch: [1067/2000][1/4]; Loss: 2.0610250503812573\n",
      "Training Accuracy Epoch: [1066]\tPrec@1 39.844 (39.844)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (31.250)\n",
      "Training epoch: [1068/2000][1/4]; Loss: 2.074109414351998\n",
      "Training Accuracy Epoch: [1067]\tPrec@1 35.156 (35.156)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (30.469)\n",
      "Training epoch: [1069/2000][1/4]; Loss: 2.0874794112404644\n",
      "Training Accuracy Epoch: [1068]\tPrec@1 32.812 (32.812)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (25.781)\n",
      "Training epoch: [1070/2000][1/4]; Loss: 2.0976530323592675\n",
      "Training Accuracy Epoch: [1069]\tPrec@1 35.156 (35.156)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (28.125)\n",
      "Training epoch: [1071/2000][1/4]; Loss: 2.066200819730446\n",
      "Training Accuracy Epoch: [1070]\tPrec@1 32.812 (32.812)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (34.375)\n",
      "Training epoch: [1072/2000][1/4]; Loss: 2.1505186733814554\n",
      "Training Accuracy Epoch: [1071]\tPrec@1 28.906 (28.906)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (33.594)\n",
      "Training epoch: [1073/2000][1/4]; Loss: 2.0732655576326326\n",
      "Training Accuracy Epoch: [1072]\tPrec@1 32.812 (32.812)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (30.469)\n",
      "Training epoch: [1074/2000][1/4]; Loss: 2.067915865070483\n",
      "Training Accuracy Epoch: [1073]\tPrec@1 32.812 (32.812)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.031)\n",
      "Training epoch: [1075/2000][1/4]; Loss: 2.0582479233805135\n",
      "Training Accuracy Epoch: [1074]\tPrec@1 31.250 (31.250)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (35.156)\n",
      "Training epoch: [1076/2000][1/4]; Loss: 2.048328665837184\n",
      "Training Accuracy Epoch: [1075]\tPrec@1 39.844 (39.844)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (30.469)\n",
      "Training epoch: [1077/2000][1/4]; Loss: 2.101556078097873\n",
      "Training Accuracy Epoch: [1076]\tPrec@1 30.469 (30.469)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (33.594)\n",
      "Training epoch: [1078/2000][1/4]; Loss: 2.029432443960406\n",
      "Training Accuracy Epoch: [1077]\tPrec@1 42.969 (42.969)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (31.250)\n",
      "Training epoch: [1079/2000][1/4]; Loss: 2.078258282769177\n",
      "Training Accuracy Epoch: [1078]\tPrec@1 43.750 (43.750)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (28.125)\n",
      "Training epoch: [1080/2000][1/4]; Loss: 2.062705325276048\n",
      "Training Accuracy Epoch: [1079]\tPrec@1 35.938 (35.938)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (31.250)\n",
      "Training epoch: [1081/2000][1/4]; Loss: 2.022977266892521\n",
      "Training Accuracy Epoch: [1080]\tPrec@1 37.500 (37.500)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.906)\n",
      "Training epoch: [1082/2000][1/4]; Loss: 2.0660661251146935\n",
      "Training Accuracy Epoch: [1081]\tPrec@1 35.156 (35.156)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.031)\n",
      "Training epoch: [1083/2000][1/4]; Loss: 2.0187765826452457\n",
      "Training Accuracy Epoch: [1082]\tPrec@1 45.312 (45.312)\tPrec@2 60.156 (60.156)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (33.594)\n",
      "Training epoch: [1084/2000][1/4]; Loss: 2.0806507535784937\n",
      "Training Accuracy Epoch: [1083]\tPrec@1 35.156 (35.156)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (31.250)\n",
      "Training epoch: [1085/2000][1/4]; Loss: 2.023099721227654\n",
      "Training Accuracy Epoch: [1084]\tPrec@1 41.406 (41.406)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (29.688)\n",
      "Training epoch: [1086/2000][1/4]; Loss: 2.0986126385275465\n",
      "Training Accuracy Epoch: [1085]\tPrec@1 30.469 (30.469)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (31.250)\n",
      "Training epoch: [1087/2000][1/4]; Loss: 2.045508989222567\n",
      "Training Accuracy Epoch: [1086]\tPrec@1 41.406 (41.406)\tPrec@2 57.031 (57.031)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (30.469)\n",
      "Training epoch: [1088/2000][1/4]; Loss: 2.046129455373928\n",
      "Training Accuracy Epoch: [1087]\tPrec@1 39.062 (39.062)\tPrec@2 57.031 (57.031)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (30.469)\n",
      "Training epoch: [1089/2000][1/4]; Loss: 2.074487047008023\n",
      "Training Accuracy Epoch: [1088]\tPrec@1 34.375 (34.375)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (26.562)\n",
      "Training epoch: [1090/2000][1/4]; Loss: 2.1030269035270157\n",
      "Training Accuracy Epoch: [1089]\tPrec@1 34.375 (34.375)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (35.938)\n",
      "Training epoch: [1091/2000][1/4]; Loss: 2.06759726140558\n",
      "Training Accuracy Epoch: [1090]\tPrec@1 39.062 (39.062)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (24.219)\tPrec@2 (33.594)\n",
      "Training epoch: [1092/2000][1/4]; Loss: 2.0248119373764673\n",
      "Training Accuracy Epoch: [1091]\tPrec@1 37.500 (37.500)\tPrec@2 56.250 (56.250)\n",
      "===>>>\tPrec@1 (25.000)\tPrec@2 (38.281)\n",
      "Training epoch: [1093/2000][1/4]; Loss: 2.0925284431694275\n",
      "Training Accuracy Epoch: [1092]\tPrec@1 33.594 (33.594)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (32.031)\n",
      "Training epoch: [1094/2000][1/4]; Loss: 2.037410929071717\n",
      "Training Accuracy Epoch: [1093]\tPrec@1 35.938 (35.938)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (31.250)\n",
      "Training epoch: [1095/2000][1/4]; Loss: 2.115269295960272\n",
      "Training Accuracy Epoch: [1094]\tPrec@1 29.688 (29.688)\tPrec@2 46.875 (46.875)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.906)\n",
      "Training epoch: [1096/2000][1/4]; Loss: 2.0538527265984694\n",
      "Training Accuracy Epoch: [1095]\tPrec@1 33.594 (33.594)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.125)\n",
      "Training epoch: [1097/2000][1/4]; Loss: 2.095686449639583\n",
      "Training Accuracy Epoch: [1096]\tPrec@1 30.469 (30.469)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (33.594)\n",
      "Training epoch: [1098/2000][1/4]; Loss: 2.0756298571865854\n",
      "Training Accuracy Epoch: [1097]\tPrec@1 29.688 (29.688)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.812)\n",
      "Training epoch: [1099/2000][1/4]; Loss: 2.0651139467023083\n",
      "Training Accuracy Epoch: [1098]\tPrec@1 34.375 (34.375)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (32.031)\n",
      "Training epoch: [1100/2000][1/4]; Loss: 2.0702590134400616\n",
      "Training Accuracy Epoch: [1099]\tPrec@1 31.250 (31.250)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.906)\n",
      "Training epoch: [1101/2000][1/4]; Loss: 2.1244148735706414\n",
      "Training Accuracy Epoch: [1100]\tPrec@1 27.344 (27.344)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (14.062)\tPrec@2 (25.781)\n",
      "Training epoch: [1102/2000][1/4]; Loss: 2.128800065388294\n",
      "Training Accuracy Epoch: [1101]\tPrec@1 26.562 (26.562)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (24.219)\n",
      "Training epoch: [1103/2000][1/4]; Loss: 2.0161995742059244\n",
      "Training Accuracy Epoch: [1102]\tPrec@1 48.438 (48.438)\tPrec@2 59.375 (59.375)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.906)\n",
      "Training epoch: [1104/2000][1/4]; Loss: 2.0448333194755657\n",
      "Training Accuracy Epoch: [1103]\tPrec@1 39.844 (39.844)\tPrec@2 58.594 (58.594)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (31.250)\n",
      "Training epoch: [1105/2000][1/4]; Loss: 2.065692824165294\n",
      "Training Accuracy Epoch: [1104]\tPrec@1 30.469 (30.469)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (31.250)\n",
      "Training epoch: [1106/2000][1/4]; Loss: 2.07801841298321\n",
      "Training Accuracy Epoch: [1105]\tPrec@1 35.938 (35.938)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (33.594)\n",
      "Training epoch: [1107/2000][1/4]; Loss: 2.01127073803913\n",
      "Training Accuracy Epoch: [1106]\tPrec@1 44.531 (44.531)\tPrec@2 57.031 (57.031)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (32.812)\n",
      "Training epoch: [1108/2000][1/4]; Loss: 2.0402116427827948\n",
      "Training Accuracy Epoch: [1107]\tPrec@1 38.281 (38.281)\tPrec@2 56.250 (56.250)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (31.250)\n",
      "Training epoch: [1109/2000][1/4]; Loss: 2.0915569252221156\n",
      "Training Accuracy Epoch: [1108]\tPrec@1 35.156 (35.156)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (34.375)\n",
      "Training epoch: [1110/2000][1/4]; Loss: 2.0742922047754515\n",
      "Training Accuracy Epoch: [1109]\tPrec@1 37.500 (37.500)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (30.469)\n",
      "Training epoch: [1111/2000][1/4]; Loss: 2.0489907292783065\n",
      "Training Accuracy Epoch: [1110]\tPrec@1 42.188 (42.188)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (31.250)\n",
      "Training epoch: [1112/2000][1/4]; Loss: 2.0712328087360015\n",
      "Training Accuracy Epoch: [1111]\tPrec@1 37.500 (37.500)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (28.125)\n",
      "Training epoch: [1113/2000][1/4]; Loss: 2.0677797072648403\n",
      "Training Accuracy Epoch: [1112]\tPrec@1 32.031 (32.031)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (31.250)\n",
      "Training epoch: [1114/2000][1/4]; Loss: 2.063997416316421\n",
      "Training Accuracy Epoch: [1113]\tPrec@1 33.594 (33.594)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (28.906)\n",
      "Training epoch: [1115/2000][1/4]; Loss: 2.077347010259484\n",
      "Training Accuracy Epoch: [1114]\tPrec@1 38.281 (38.281)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (27.344)\n",
      "Training epoch: [1116/2000][1/4]; Loss: 2.043794845802499\n",
      "Training Accuracy Epoch: [1115]\tPrec@1 40.625 (40.625)\tPrec@2 59.375 (59.375)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.906)\n",
      "Training epoch: [1117/2000][1/4]; Loss: 2.0866547771449206\n",
      "Training Accuracy Epoch: [1116]\tPrec@1 32.031 (32.031)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.125)\n",
      "Training epoch: [1118/2000][1/4]; Loss: 2.0731236945971716\n",
      "Training Accuracy Epoch: [1117]\tPrec@1 36.719 (36.719)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (33.594)\n",
      "Training epoch: [1119/2000][1/4]; Loss: 2.042956211519806\n",
      "Training Accuracy Epoch: [1118]\tPrec@1 37.500 (37.500)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (34.375)\n",
      "Training epoch: [1120/2000][1/4]; Loss: 2.014052590379742\n",
      "Training Accuracy Epoch: [1119]\tPrec@1 42.969 (42.969)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (26.562)\n",
      "Training epoch: [1121/2000][1/4]; Loss: 2.0563810741820983\n",
      "Training Accuracy Epoch: [1120]\tPrec@1 31.250 (31.250)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (32.812)\n",
      "Training epoch: [1122/2000][1/4]; Loss: 2.0404740161756423\n",
      "Training Accuracy Epoch: [1121]\tPrec@1 36.719 (36.719)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (30.469)\n",
      "Training epoch: [1123/2000][1/4]; Loss: 2.071773868142055\n",
      "Training Accuracy Epoch: [1122]\tPrec@1 31.250 (31.250)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.031)\n",
      "Training epoch: [1124/2000][1/4]; Loss: 2.0664406431674402\n",
      "Training Accuracy Epoch: [1123]\tPrec@1 32.031 (32.031)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (31.250)\n",
      "Training epoch: [1125/2000][1/4]; Loss: 2.06975754503565\n",
      "Training Accuracy Epoch: [1124]\tPrec@1 35.938 (35.938)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (29.688)\n",
      "Training epoch: [1126/2000][1/4]; Loss: 2.1124810002862233\n",
      "Training Accuracy Epoch: [1125]\tPrec@1 35.938 (35.938)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (31.250)\n",
      "Training epoch: [1127/2000][1/4]; Loss: 2.0482625984987335\n",
      "Training Accuracy Epoch: [1126]\tPrec@1 36.719 (36.719)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (30.469)\n",
      "Training epoch: [1128/2000][1/4]; Loss: 2.039218035493006\n",
      "Training Accuracy Epoch: [1127]\tPrec@1 35.938 (35.938)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (31.250)\n",
      "Training epoch: [1129/2000][1/4]; Loss: 2.046399076288178\n",
      "Training Accuracy Epoch: [1128]\tPrec@1 34.375 (34.375)\tPrec@2 57.031 (57.031)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (34.375)\n",
      "Training epoch: [1130/2000][1/4]; Loss: 2.0761890487064663\n",
      "Training Accuracy Epoch: [1129]\tPrec@1 33.594 (33.594)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (32.812)\n",
      "Training epoch: [1131/2000][1/4]; Loss: 2.069395787597496\n",
      "Training Accuracy Epoch: [1130]\tPrec@1 28.906 (28.906)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (14.062)\tPrec@2 (28.125)\n",
      "Training epoch: [1132/2000][1/4]; Loss: 2.091389947762566\n",
      "Training Accuracy Epoch: [1131]\tPrec@1 39.062 (39.062)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (28.125)\n",
      "Training epoch: [1133/2000][1/4]; Loss: 2.060788151048314\n",
      "Training Accuracy Epoch: [1132]\tPrec@1 34.375 (34.375)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (24.219)\tPrec@2 (33.594)\n",
      "Training epoch: [1134/2000][1/4]; Loss: 2.0460002511393265\n",
      "Training Accuracy Epoch: [1133]\tPrec@1 40.625 (40.625)\tPrec@2 57.812 (57.812)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (32.031)\n",
      "Training epoch: [1135/2000][1/4]; Loss: 2.0602123738837306\n",
      "Training Accuracy Epoch: [1134]\tPrec@1 36.719 (36.719)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (31.250)\n",
      "Training epoch: [1136/2000][1/4]; Loss: 2.080252318294059\n",
      "Training Accuracy Epoch: [1135]\tPrec@1 29.688 (29.688)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (32.812)\n",
      "Training epoch: [1137/2000][1/4]; Loss: 2.0795696025027337\n",
      "Training Accuracy Epoch: [1136]\tPrec@1 34.375 (34.375)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (28.906)\n",
      "Training epoch: [1138/2000][1/4]; Loss: 2.018167046595928\n",
      "Training Accuracy Epoch: [1137]\tPrec@1 44.531 (44.531)\tPrec@2 59.375 (59.375)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (31.250)\n",
      "Training epoch: [1139/2000][1/4]; Loss: 2.06902138852426\n",
      "Training Accuracy Epoch: [1138]\tPrec@1 30.469 (30.469)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (35.156)\n",
      "Training epoch: [1140/2000][1/4]; Loss: 2.065633586703765\n",
      "Training Accuracy Epoch: [1139]\tPrec@1 38.281 (38.281)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (28.125)\n",
      "Training epoch: [1141/2000][1/4]; Loss: 2.0670950323212876\n",
      "Training Accuracy Epoch: [1140]\tPrec@1 35.156 (35.156)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (32.812)\n",
      "Training epoch: [1142/2000][1/4]; Loss: 2.0274462257427217\n",
      "Training Accuracy Epoch: [1141]\tPrec@1 39.062 (39.062)\tPrec@2 55.469 (55.469)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>>>\tPrec@1 (16.406)\tPrec@2 (28.125)\n",
      "Training epoch: [1143/2000][1/4]; Loss: 2.0561194923191337\n",
      "Training Accuracy Epoch: [1142]\tPrec@1 36.719 (36.719)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (31.250)\n",
      "Training epoch: [1144/2000][1/4]; Loss: 2.046260150902939\n",
      "Training Accuracy Epoch: [1143]\tPrec@1 38.281 (38.281)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (31.250)\n",
      "Training epoch: [1145/2000][1/4]; Loss: 2.063629680947636\n",
      "Training Accuracy Epoch: [1144]\tPrec@1 38.281 (38.281)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (31.250)\n",
      "Training epoch: [1146/2000][1/4]; Loss: 2.008560547608305\n",
      "Training Accuracy Epoch: [1145]\tPrec@1 37.500 (37.500)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (28.906)\n",
      "Training epoch: [1147/2000][1/4]; Loss: 2.0201581098184724\n",
      "Training Accuracy Epoch: [1146]\tPrec@1 46.875 (46.875)\tPrec@2 57.812 (57.812)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (32.031)\n",
      "Training epoch: [1148/2000][1/4]; Loss: 2.092096205964875\n",
      "Training Accuracy Epoch: [1147]\tPrec@1 27.344 (27.344)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (32.031)\n",
      "Training epoch: [1149/2000][1/4]; Loss: 2.0533462928577775\n",
      "Training Accuracy Epoch: [1148]\tPrec@1 34.375 (34.375)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.031)\n",
      "Training epoch: [1150/2000][1/4]; Loss: 2.0523758552577593\n",
      "Training Accuracy Epoch: [1149]\tPrec@1 37.500 (37.500)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (28.906)\n",
      "Training epoch: [1151/2000][1/4]; Loss: 2.0842017611098345\n",
      "Training Accuracy Epoch: [1150]\tPrec@1 35.156 (35.156)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (30.469)\n",
      "Training epoch: [1152/2000][1/4]; Loss: 2.038003622781971\n",
      "Training Accuracy Epoch: [1151]\tPrec@1 38.281 (38.281)\tPrec@2 60.156 (60.156)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (33.594)\n",
      "Training epoch: [1153/2000][1/4]; Loss: 2.0768034842839076\n",
      "Training Accuracy Epoch: [1152]\tPrec@1 38.281 (38.281)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.812)\n",
      "Training epoch: [1154/2000][1/4]; Loss: 2.09611510320436\n",
      "Training Accuracy Epoch: [1153]\tPrec@1 36.719 (36.719)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (25.781)\n",
      "Training epoch: [1155/2000][1/4]; Loss: 2.0704953222777576\n",
      "Training Accuracy Epoch: [1154]\tPrec@1 34.375 (34.375)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.906)\n",
      "Training epoch: [1156/2000][1/4]; Loss: 2.0660413052014226\n",
      "Training Accuracy Epoch: [1155]\tPrec@1 33.594 (33.594)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (33.594)\n",
      "Training epoch: [1157/2000][1/4]; Loss: 2.0616969005223957\n",
      "Training Accuracy Epoch: [1156]\tPrec@1 42.969 (42.969)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.125)\n",
      "Training epoch: [1158/2000][1/4]; Loss: 2.0969428886185275\n",
      "Training Accuracy Epoch: [1157]\tPrec@1 25.781 (25.781)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (30.469)\n",
      "Training epoch: [1159/2000][1/4]; Loss: 2.024222743190449\n",
      "Training Accuracy Epoch: [1158]\tPrec@1 40.625 (40.625)\tPrec@2 57.812 (57.812)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (32.031)\n",
      "Training epoch: [1160/2000][1/4]; Loss: 2.06126393985279\n",
      "Training Accuracy Epoch: [1159]\tPrec@1 36.719 (36.719)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.812)\n",
      "Training epoch: [1161/2000][1/4]; Loss: 2.0731150330980186\n",
      "Training Accuracy Epoch: [1160]\tPrec@1 33.594 (33.594)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (29.688)\n",
      "Training epoch: [1162/2000][1/4]; Loss: 2.037334813106355\n",
      "Training Accuracy Epoch: [1161]\tPrec@1 37.500 (37.500)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (30.469)\n",
      "Training epoch: [1163/2000][1/4]; Loss: 2.1171868684584667\n",
      "Training Accuracy Epoch: [1162]\tPrec@1 27.344 (27.344)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (31.250)\n",
      "Training epoch: [1164/2000][1/4]; Loss: 2.0905395673241136\n",
      "Training Accuracy Epoch: [1163]\tPrec@1 32.031 (32.031)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (27.344)\n",
      "Training epoch: [1165/2000][1/4]; Loss: 2.0619456736916164\n",
      "Training Accuracy Epoch: [1164]\tPrec@1 34.375 (34.375)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (28.906)\n",
      "Training epoch: [1166/2000][1/4]; Loss: 2.1023955547377557\n",
      "Training Accuracy Epoch: [1165]\tPrec@1 28.125 (28.125)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (29.688)\n",
      "Training epoch: [1167/2000][1/4]; Loss: 2.025182718632526\n",
      "Training Accuracy Epoch: [1166]\tPrec@1 34.375 (34.375)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (30.469)\n",
      "Training epoch: [1168/2000][1/4]; Loss: 2.1423531039126216\n",
      "Training Accuracy Epoch: [1167]\tPrec@1 27.344 (27.344)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (39.062)\n",
      "Training epoch: [1169/2000][1/4]; Loss: 2.0792162231935776\n",
      "Training Accuracy Epoch: [1168]\tPrec@1 28.906 (28.906)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (33.594)\n",
      "Training epoch: [1170/2000][1/4]; Loss: 2.0025365917914963\n",
      "Training Accuracy Epoch: [1169]\tPrec@1 46.875 (46.875)\tPrec@2 62.500 (62.500)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (30.469)\n",
      "Training epoch: [1171/2000][1/4]; Loss: 2.0978583876890915\n",
      "Training Accuracy Epoch: [1170]\tPrec@1 31.250 (31.250)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (28.125)\n",
      "Training epoch: [1172/2000][1/4]; Loss: 2.0471506645059354\n",
      "Training Accuracy Epoch: [1171]\tPrec@1 32.812 (32.812)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (30.469)\n",
      "Training epoch: [1173/2000][1/4]; Loss: 2.0472033614543306\n",
      "Training Accuracy Epoch: [1172]\tPrec@1 35.156 (35.156)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (28.125)\n",
      "Training epoch: [1174/2000][1/4]; Loss: 2.081706445005099\n",
      "Training Accuracy Epoch: [1173]\tPrec@1 37.500 (37.500)\tPrec@2 56.250 (56.250)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.812)\n",
      "Training epoch: [1175/2000][1/4]; Loss: 2.1616091991409987\n",
      "Training Accuracy Epoch: [1174]\tPrec@1 23.438 (23.438)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (31.250)\n",
      "Training epoch: [1176/2000][1/4]; Loss: 2.0665193828140693\n",
      "Training Accuracy Epoch: [1175]\tPrec@1 35.938 (35.938)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (29.688)\n",
      "Training epoch: [1177/2000][1/4]; Loss: 2.041934193584155\n",
      "Training Accuracy Epoch: [1176]\tPrec@1 38.281 (38.281)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (31.250)\n",
      "Training epoch: [1178/2000][1/4]; Loss: 2.0676973747179463\n",
      "Training Accuracy Epoch: [1177]\tPrec@1 35.156 (35.156)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (28.906)\n",
      "Training epoch: [1179/2000][1/4]; Loss: 2.0489828536198185\n",
      "Training Accuracy Epoch: [1178]\tPrec@1 36.719 (36.719)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (32.812)\n",
      "Training epoch: [1180/2000][1/4]; Loss: 2.0450445946577473\n",
      "Training Accuracy Epoch: [1179]\tPrec@1 32.812 (32.812)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (30.469)\n",
      "Training epoch: [1181/2000][1/4]; Loss: 2.0423405970909196\n",
      "Training Accuracy Epoch: [1180]\tPrec@1 36.719 (36.719)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (30.469)\n",
      "Training epoch: [1182/2000][1/4]; Loss: 2.0701652841174814\n",
      "Training Accuracy Epoch: [1181]\tPrec@1 34.375 (34.375)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (30.469)\n",
      "Training epoch: [1183/2000][1/4]; Loss: 2.0461012867099364\n",
      "Training Accuracy Epoch: [1182]\tPrec@1 41.406 (41.406)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (33.594)\n",
      "Training epoch: [1184/2000][1/4]; Loss: 2.1182182776324385\n",
      "Training Accuracy Epoch: [1183]\tPrec@1 30.469 (30.469)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (27.344)\n",
      "Training epoch: [1185/2000][1/4]; Loss: 2.07820918177929\n",
      "Training Accuracy Epoch: [1184]\tPrec@1 32.812 (32.812)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (33.594)\n",
      "Training epoch: [1186/2000][1/4]; Loss: 2.0213206027272563\n",
      "Training Accuracy Epoch: [1185]\tPrec@1 37.500 (37.500)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (31.250)\n",
      "Training epoch: [1187/2000][1/4]; Loss: 2.0336947989556813\n",
      "Training Accuracy Epoch: [1186]\tPrec@1 36.719 (36.719)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (29.688)\n",
      "Training epoch: [1188/2000][1/4]; Loss: 2.0624768023386735\n",
      "Training Accuracy Epoch: [1187]\tPrec@1 35.938 (35.938)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (31.250)\n",
      "Training epoch: [1189/2000][1/4]; Loss: 2.0569633289438882\n",
      "Training Accuracy Epoch: [1188]\tPrec@1 36.719 (36.719)\tPrec@2 55.469 (55.469)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>>>\tPrec@1 (18.750)\tPrec@2 (34.375)\n",
      "Training epoch: [1190/2000][1/4]; Loss: 2.0578783952581947\n",
      "Training Accuracy Epoch: [1189]\tPrec@1 33.594 (33.594)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (29.688)\n",
      "Training epoch: [1191/2000][1/4]; Loss: 2.023081852591081\n",
      "Training Accuracy Epoch: [1190]\tPrec@1 43.750 (43.750)\tPrec@2 59.375 (59.375)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (29.688)\n",
      "Training epoch: [1192/2000][1/4]; Loss: 2.0593487755131226\n",
      "Training Accuracy Epoch: [1191]\tPrec@1 32.031 (32.031)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (32.031)\n",
      "Training epoch: [1193/2000][1/4]; Loss: 2.11037099872804\n",
      "Training Accuracy Epoch: [1192]\tPrec@1 26.562 (26.562)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (30.469)\n",
      "Training epoch: [1194/2000][1/4]; Loss: 2.0942847609505626\n",
      "Training Accuracy Epoch: [1193]\tPrec@1 30.469 (30.469)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (31.250)\n",
      "Training epoch: [1195/2000][1/4]; Loss: 2.0720134305324756\n",
      "Training Accuracy Epoch: [1194]\tPrec@1 33.594 (33.594)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (32.031)\n",
      "Training epoch: [1196/2000][1/4]; Loss: 2.0350154629701\n",
      "Training Accuracy Epoch: [1195]\tPrec@1 35.938 (35.938)\tPrec@2 57.812 (57.812)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (33.594)\n",
      "Training epoch: [1197/2000][1/4]; Loss: 2.1103702076203787\n",
      "Training Accuracy Epoch: [1196]\tPrec@1 24.219 (24.219)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (31.250)\n",
      "Training epoch: [1198/2000][1/4]; Loss: 2.045742817239354\n",
      "Training Accuracy Epoch: [1197]\tPrec@1 33.594 (33.594)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (32.812)\n",
      "Training epoch: [1199/2000][1/4]; Loss: 2.0744897374673066\n",
      "Training Accuracy Epoch: [1198]\tPrec@1 32.031 (32.031)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (27.344)\n",
      "Training epoch: [1200/2000][1/4]; Loss: 2.0751163853725125\n",
      "Training Accuracy Epoch: [1199]\tPrec@1 31.250 (31.250)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (34.375)\n",
      "Training epoch: [1201/2000][1/4]; Loss: 2.111824231227908\n",
      "Training Accuracy Epoch: [1200]\tPrec@1 25.781 (25.781)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (29.688)\n",
      "Training epoch: [1202/2000][1/4]; Loss: 2.0917814010204414\n",
      "Training Accuracy Epoch: [1201]\tPrec@1 26.562 (26.562)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (28.906)\n",
      "Training epoch: [1203/2000][1/4]; Loss: 2.06085743695674\n",
      "Training Accuracy Epoch: [1202]\tPrec@1 35.156 (35.156)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (32.031)\n",
      "Training epoch: [1204/2000][1/4]; Loss: 2.064949092667055\n",
      "Training Accuracy Epoch: [1203]\tPrec@1 36.719 (36.719)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (30.469)\n",
      "Training epoch: [1205/2000][1/4]; Loss: 2.064889523654634\n",
      "Training Accuracy Epoch: [1204]\tPrec@1 35.156 (35.156)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.812)\n",
      "Training epoch: [1206/2000][1/4]; Loss: 2.058152130386314\n",
      "Training Accuracy Epoch: [1205]\tPrec@1 33.594 (33.594)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (31.250)\n",
      "Training epoch: [1207/2000][1/4]; Loss: 2.0655099764005795\n",
      "Training Accuracy Epoch: [1206]\tPrec@1 32.812 (32.812)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (25.781)\n",
      "Training epoch: [1208/2000][1/4]; Loss: 2.0504621142188495\n",
      "Training Accuracy Epoch: [1207]\tPrec@1 32.812 (32.812)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (30.469)\n",
      "Training epoch: [1209/2000][1/4]; Loss: 2.048509052038618\n",
      "Training Accuracy Epoch: [1208]\tPrec@1 39.062 (39.062)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (28.125)\n",
      "Training epoch: [1210/2000][1/4]; Loss: 2.1032183851939967\n",
      "Training Accuracy Epoch: [1209]\tPrec@1 29.688 (29.688)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.906)\n",
      "Training epoch: [1211/2000][1/4]; Loss: 2.0661582304933983\n",
      "Training Accuracy Epoch: [1210]\tPrec@1 31.250 (31.250)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.125)\n",
      "Training epoch: [1212/2000][1/4]; Loss: 2.0862292690444924\n",
      "Training Accuracy Epoch: [1211]\tPrec@1 28.906 (28.906)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (13.281)\tPrec@2 (27.344)\n",
      "Training epoch: [1213/2000][1/4]; Loss: 2.047173904224454\n",
      "Training Accuracy Epoch: [1212]\tPrec@1 34.375 (34.375)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (34.375)\n",
      "Training epoch: [1214/2000][1/4]; Loss: 2.0747869506761556\n",
      "Training Accuracy Epoch: [1213]\tPrec@1 33.594 (33.594)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (32.812)\n",
      "Training epoch: [1215/2000][1/4]; Loss: 2.0167907309107513\n",
      "Training Accuracy Epoch: [1214]\tPrec@1 39.062 (39.062)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (26.562)\n",
      "Training epoch: [1216/2000][1/4]; Loss: 2.0158670154892673\n",
      "Training Accuracy Epoch: [1215]\tPrec@1 42.969 (42.969)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (32.812)\n",
      "Training epoch: [1217/2000][1/4]; Loss: 2.0943864747034775\n",
      "Training Accuracy Epoch: [1216]\tPrec@1 28.906 (28.906)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.812)\n",
      "Training epoch: [1218/2000][1/4]; Loss: 2.083761567425913\n",
      "Training Accuracy Epoch: [1217]\tPrec@1 24.219 (24.219)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.125)\n",
      "Training epoch: [1219/2000][1/4]; Loss: 2.0534474124581017\n",
      "Training Accuracy Epoch: [1218]\tPrec@1 35.156 (35.156)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (29.688)\n",
      "Training epoch: [1220/2000][1/4]; Loss: 2.008705951407034\n",
      "Training Accuracy Epoch: [1219]\tPrec@1 41.406 (41.406)\tPrec@2 58.594 (58.594)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.031)\n",
      "Training epoch: [1221/2000][1/4]; Loss: 2.034143734018295\n",
      "Training Accuracy Epoch: [1220]\tPrec@1 37.500 (37.500)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (34.375)\n",
      "Training epoch: [1222/2000][1/4]; Loss: 2.0349778661624\n",
      "Training Accuracy Epoch: [1221]\tPrec@1 38.281 (38.281)\tPrec@2 57.812 (57.812)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (30.469)\n",
      "Training epoch: [1223/2000][1/4]; Loss: 2.085103904094912\n",
      "Training Accuracy Epoch: [1222]\tPrec@1 29.688 (29.688)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (30.469)\n",
      "Training epoch: [1224/2000][1/4]; Loss: 2.068546308302895\n",
      "Training Accuracy Epoch: [1223]\tPrec@1 31.250 (31.250)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (33.594)\n",
      "Training epoch: [1225/2000][1/4]; Loss: 2.0741005669414125\n",
      "Training Accuracy Epoch: [1224]\tPrec@1 30.469 (30.469)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (29.688)\n",
      "Training epoch: [1226/2000][1/4]; Loss: 2.0751350860672035\n",
      "Training Accuracy Epoch: [1225]\tPrec@1 32.812 (32.812)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (28.906)\n",
      "Training epoch: [1227/2000][1/4]; Loss: 2.066955478534755\n",
      "Training Accuracy Epoch: [1226]\tPrec@1 32.031 (32.031)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.906)\n",
      "Training epoch: [1228/2000][1/4]; Loss: 2.0474455140463133\n",
      "Training Accuracy Epoch: [1227]\tPrec@1 40.625 (40.625)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (34.375)\n",
      "Training epoch: [1229/2000][1/4]; Loss: 2.0676519992734637\n",
      "Training Accuracy Epoch: [1228]\tPrec@1 32.031 (32.031)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (30.469)\n",
      "Training epoch: [1230/2000][1/4]; Loss: 2.0952530745119295\n",
      "Training Accuracy Epoch: [1229]\tPrec@1 27.344 (27.344)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (30.469)\n",
      "Training epoch: [1231/2000][1/4]; Loss: 2.0299461317158025\n",
      "Training Accuracy Epoch: [1230]\tPrec@1 32.812 (32.812)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (30.469)\n",
      "Training epoch: [1232/2000][1/4]; Loss: 2.0562792883019774\n",
      "Training Accuracy Epoch: [1231]\tPrec@1 31.250 (31.250)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (31.250)\n",
      "Training epoch: [1233/2000][1/4]; Loss: 2.0622979434033404\n",
      "Training Accuracy Epoch: [1232]\tPrec@1 34.375 (34.375)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (14.062)\tPrec@2 (28.906)\n",
      "Training epoch: [1234/2000][1/4]; Loss: 2.0977981635214396\n",
      "Training Accuracy Epoch: [1233]\tPrec@1 35.156 (35.156)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (30.469)\n",
      "Training epoch: [1235/2000][1/4]; Loss: 2.0647553854760674\n",
      "Training Accuracy Epoch: [1234]\tPrec@1 32.812 (32.812)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.906)\n",
      "Training epoch: [1236/2000][1/4]; Loss: 2.0493030015529707\n",
      "Training Accuracy Epoch: [1235]\tPrec@1 37.500 (37.500)\tPrec@2 50.781 (50.781)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>>>\tPrec@1 (23.438)\tPrec@2 (35.938)\n",
      "Training epoch: [1237/2000][1/4]; Loss: 2.020179166699951\n",
      "Training Accuracy Epoch: [1236]\tPrec@1 40.625 (40.625)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (31.250)\n",
      "Training epoch: [1238/2000][1/4]; Loss: 2.078585591684982\n",
      "Training Accuracy Epoch: [1237]\tPrec@1 35.156 (35.156)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (30.469)\n",
      "Training epoch: [1239/2000][1/4]; Loss: 2.0975225153222086\n",
      "Training Accuracy Epoch: [1238]\tPrec@1 29.688 (29.688)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (29.688)\n",
      "Training epoch: [1240/2000][1/4]; Loss: 2.0047153879088113\n",
      "Training Accuracy Epoch: [1239]\tPrec@1 43.750 (43.750)\tPrec@2 58.594 (58.594)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (31.250)\n",
      "Training epoch: [1241/2000][1/4]; Loss: 2.0156016857812538\n",
      "Training Accuracy Epoch: [1240]\tPrec@1 40.625 (40.625)\tPrec@2 62.500 (62.500)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (25.781)\n",
      "Training epoch: [1242/2000][1/4]; Loss: 2.0668592662319867\n",
      "Training Accuracy Epoch: [1241]\tPrec@1 31.250 (31.250)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (29.688)\n",
      "Training epoch: [1243/2000][1/4]; Loss: 2.092409698088763\n",
      "Training Accuracy Epoch: [1242]\tPrec@1 30.469 (30.469)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (31.250)\n",
      "Training epoch: [1244/2000][1/4]; Loss: 2.0915510415862144\n",
      "Training Accuracy Epoch: [1243]\tPrec@1 32.031 (32.031)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (30.469)\n",
      "Training epoch: [1245/2000][1/4]; Loss: 2.0951387957423524\n",
      "Training Accuracy Epoch: [1244]\tPrec@1 31.250 (31.250)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (30.469)\n",
      "Training epoch: [1246/2000][1/4]; Loss: 2.063384111184076\n",
      "Training Accuracy Epoch: [1245]\tPrec@1 35.938 (35.938)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (29.688)\n",
      "Training epoch: [1247/2000][1/4]; Loss: 2.059311282078385\n",
      "Training Accuracy Epoch: [1246]\tPrec@1 37.500 (37.500)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (28.906)\n",
      "Training epoch: [1248/2000][1/4]; Loss: 2.0593966160230512\n",
      "Training Accuracy Epoch: [1247]\tPrec@1 35.938 (35.938)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (32.812)\n",
      "Training epoch: [1249/2000][1/4]; Loss: 2.072852917629566\n",
      "Training Accuracy Epoch: [1248]\tPrec@1 34.375 (34.375)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (32.031)\n",
      "Training epoch: [1250/2000][1/4]; Loss: 2.044737372738755\n",
      "Training Accuracy Epoch: [1249]\tPrec@1 35.156 (35.156)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (32.812)\n",
      "Training epoch: [1251/2000][1/4]; Loss: 2.068549494338874\n",
      "Training Accuracy Epoch: [1250]\tPrec@1 39.062 (39.062)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (32.031)\n",
      "Training epoch: [1252/2000][1/4]; Loss: 2.048261391581026\n",
      "Training Accuracy Epoch: [1251]\tPrec@1 34.375 (34.375)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (33.594)\n",
      "Training epoch: [1253/2000][1/4]; Loss: 2.0303948265462384\n",
      "Training Accuracy Epoch: [1252]\tPrec@1 36.719 (36.719)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (13.281)\tPrec@2 (27.344)\n",
      "Training epoch: [1254/2000][1/4]; Loss: 2.072817399395048\n",
      "Training Accuracy Epoch: [1253]\tPrec@1 32.031 (32.031)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (32.031)\n",
      "Training epoch: [1255/2000][1/4]; Loss: 2.0262029365969565\n",
      "Training Accuracy Epoch: [1254]\tPrec@1 32.812 (32.812)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (26.562)\n",
      "Training epoch: [1256/2000][1/4]; Loss: 2.1041154282071615\n",
      "Training Accuracy Epoch: [1255]\tPrec@1 29.688 (29.688)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (32.031)\n",
      "Training epoch: [1257/2000][1/4]; Loss: 2.0740566485216423\n",
      "Training Accuracy Epoch: [1256]\tPrec@1 33.594 (33.594)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (28.906)\n",
      "Training epoch: [1258/2000][1/4]; Loss: 2.01159124059329\n",
      "Training Accuracy Epoch: [1257]\tPrec@1 42.969 (42.969)\tPrec@2 57.812 (57.812)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.031)\n",
      "Training epoch: [1259/2000][1/4]; Loss: 2.0687739042670237\n",
      "Training Accuracy Epoch: [1258]\tPrec@1 32.812 (32.812)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (31.250)\n",
      "Training epoch: [1260/2000][1/4]; Loss: 2.0580525339721314\n",
      "Training Accuracy Epoch: [1259]\tPrec@1 32.031 (32.031)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (30.469)\n",
      "Training epoch: [1261/2000][1/4]; Loss: 2.1012387511958\n",
      "Training Accuracy Epoch: [1260]\tPrec@1 28.906 (28.906)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (30.469)\n",
      "Training epoch: [1262/2000][1/4]; Loss: 2.062613313382886\n",
      "Training Accuracy Epoch: [1261]\tPrec@1 34.375 (34.375)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (30.469)\n",
      "Training epoch: [1263/2000][1/4]; Loss: 2.010468555384985\n",
      "Training Accuracy Epoch: [1262]\tPrec@1 42.188 (42.188)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (30.469)\n",
      "Training epoch: [1264/2000][1/4]; Loss: 2.0973589855960997\n",
      "Training Accuracy Epoch: [1263]\tPrec@1 33.594 (33.594)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (29.688)\n",
      "Training epoch: [1265/2000][1/4]; Loss: 2.0504469307068063\n",
      "Training Accuracy Epoch: [1264]\tPrec@1 34.375 (34.375)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (31.250)\n",
      "Training epoch: [1266/2000][1/4]; Loss: 2.051712749741542\n",
      "Training Accuracy Epoch: [1265]\tPrec@1 33.594 (33.594)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (29.688)\n",
      "Training epoch: [1267/2000][1/4]; Loss: 2.1132603396884853\n",
      "Training Accuracy Epoch: [1266]\tPrec@1 29.688 (29.688)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (30.469)\n",
      "Training epoch: [1268/2000][1/4]; Loss: 2.078717325463844\n",
      "Training Accuracy Epoch: [1267]\tPrec@1 33.594 (33.594)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (27.344)\n",
      "Training epoch: [1269/2000][1/4]; Loss: 2.100197496323164\n",
      "Training Accuracy Epoch: [1268]\tPrec@1 32.031 (32.031)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (28.906)\n",
      "Training epoch: [1270/2000][1/4]; Loss: 2.0909617040514914\n",
      "Training Accuracy Epoch: [1269]\tPrec@1 32.031 (32.031)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (31.250)\n",
      "Training epoch: [1271/2000][1/4]; Loss: 2.073018186493583\n",
      "Training Accuracy Epoch: [1270]\tPrec@1 31.250 (31.250)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (33.594)\n",
      "Training epoch: [1272/2000][1/4]; Loss: 2.0995035854353823\n",
      "Training Accuracy Epoch: [1271]\tPrec@1 28.906 (28.906)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.812)\n",
      "Training epoch: [1273/2000][1/4]; Loss: 2.0196629862656836\n",
      "Training Accuracy Epoch: [1272]\tPrec@1 37.500 (37.500)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (33.594)\n",
      "Training epoch: [1274/2000][1/4]; Loss: 2.1897239032890856\n",
      "Training Accuracy Epoch: [1273]\tPrec@1 23.438 (23.438)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (33.594)\n",
      "Training epoch: [1275/2000][1/4]; Loss: 2.037245456229595\n",
      "Training Accuracy Epoch: [1274]\tPrec@1 36.719 (36.719)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (33.594)\n",
      "Training epoch: [1276/2000][1/4]; Loss: 2.032781696114465\n",
      "Training Accuracy Epoch: [1275]\tPrec@1 38.281 (38.281)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (29.688)\n",
      "Training epoch: [1277/2000][1/4]; Loss: 2.038556776511442\n",
      "Training Accuracy Epoch: [1276]\tPrec@1 40.625 (40.625)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (28.906)\n",
      "Training epoch: [1278/2000][1/4]; Loss: 2.0535390643479827\n",
      "Training Accuracy Epoch: [1277]\tPrec@1 34.375 (34.375)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (30.469)\n",
      "Training epoch: [1279/2000][1/4]; Loss: 2.0482116676504303\n",
      "Training Accuracy Epoch: [1278]\tPrec@1 42.969 (42.969)\tPrec@2 56.250 (56.250)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (35.156)\n",
      "Training epoch: [1280/2000][1/4]; Loss: 2.1098854354130947\n",
      "Training Accuracy Epoch: [1279]\tPrec@1 33.594 (33.594)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (30.469)\n",
      "Training epoch: [1281/2000][1/4]; Loss: 2.1066198240809024\n",
      "Training Accuracy Epoch: [1280]\tPrec@1 28.906 (28.906)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (32.031)\n",
      "Training epoch: [1282/2000][1/4]; Loss: 2.088101470323268\n",
      "Training Accuracy Epoch: [1281]\tPrec@1 33.594 (33.594)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.812)\n",
      "Training epoch: [1283/2000][1/4]; Loss: 2.085727037353902\n",
      "Training Accuracy Epoch: [1282]\tPrec@1 37.500 (37.500)\tPrec@2 48.438 (48.438)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>>>\tPrec@1 (14.062)\tPrec@2 (24.219)\n",
      "Training epoch: [1284/2000][1/4]; Loss: 2.0715877486738505\n",
      "Training Accuracy Epoch: [1283]\tPrec@1 29.688 (29.688)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (33.594)\n",
      "Training epoch: [1285/2000][1/4]; Loss: 2.0975471829558017\n",
      "Training Accuracy Epoch: [1284]\tPrec@1 34.375 (34.375)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.031)\n",
      "Training epoch: [1286/2000][1/4]; Loss: 2.0316299329395173\n",
      "Training Accuracy Epoch: [1285]\tPrec@1 37.500 (37.500)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (30.469)\n",
      "Training epoch: [1287/2000][1/4]; Loss: 2.0963452556633686\n",
      "Training Accuracy Epoch: [1286]\tPrec@1 31.250 (31.250)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (28.125)\n",
      "Training epoch: [1288/2000][1/4]; Loss: 2.0571672585658107\n",
      "Training Accuracy Epoch: [1287]\tPrec@1 32.031 (32.031)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (31.250)\n",
      "Training epoch: [1289/2000][1/4]; Loss: 2.0715646400915677\n",
      "Training Accuracy Epoch: [1288]\tPrec@1 35.156 (35.156)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (31.250)\n",
      "Training epoch: [1290/2000][1/4]; Loss: 2.0598802383532435\n",
      "Training Accuracy Epoch: [1289]\tPrec@1 33.594 (33.594)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (32.812)\n",
      "Training epoch: [1291/2000][1/4]; Loss: 2.0149950277433923\n",
      "Training Accuracy Epoch: [1290]\tPrec@1 39.062 (39.062)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.812)\n",
      "Training epoch: [1292/2000][1/4]; Loss: 2.0873238732629282\n",
      "Training Accuracy Epoch: [1291]\tPrec@1 30.469 (30.469)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (29.688)\n",
      "Training epoch: [1293/2000][1/4]; Loss: 2.131640301021746\n",
      "Training Accuracy Epoch: [1292]\tPrec@1 32.031 (32.031)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (26.562)\n",
      "Training epoch: [1294/2000][1/4]; Loss: 2.035057466232831\n",
      "Training Accuracy Epoch: [1293]\tPrec@1 37.500 (37.500)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (13.281)\tPrec@2 (29.688)\n",
      "Training epoch: [1295/2000][1/4]; Loss: 2.029282370712849\n",
      "Training Accuracy Epoch: [1294]\tPrec@1 41.406 (41.406)\tPrec@2 56.250 (56.250)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (35.156)\n",
      "Training epoch: [1296/2000][1/4]; Loss: 2.1144935936147844\n",
      "Training Accuracy Epoch: [1295]\tPrec@1 32.812 (32.812)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.031)\n",
      "Training epoch: [1297/2000][1/4]; Loss: 2.0351620190970077\n",
      "Training Accuracy Epoch: [1296]\tPrec@1 35.938 (35.938)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (32.031)\n",
      "Training epoch: [1298/2000][1/4]; Loss: 2.0391587283357606\n",
      "Training Accuracy Epoch: [1297]\tPrec@1 39.844 (39.844)\tPrec@2 57.031 (57.031)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (30.469)\n",
      "Training epoch: [1299/2000][1/4]; Loss: 2.0509260627097126\n",
      "Training Accuracy Epoch: [1298]\tPrec@1 31.250 (31.250)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (28.906)\n",
      "Training epoch: [1300/2000][1/4]; Loss: 2.089477077896049\n",
      "Training Accuracy Epoch: [1299]\tPrec@1 33.594 (33.594)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (32.812)\n",
      "Training epoch: [1301/2000][1/4]; Loss: 2.040828787457075\n",
      "Training Accuracy Epoch: [1300]\tPrec@1 35.156 (35.156)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (30.469)\n",
      "Training epoch: [1302/2000][1/4]; Loss: 2.0449427483741953\n",
      "Training Accuracy Epoch: [1301]\tPrec@1 35.938 (35.938)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (32.031)\n",
      "Training epoch: [1303/2000][1/4]; Loss: 2.080778977114014\n",
      "Training Accuracy Epoch: [1302]\tPrec@1 32.812 (32.812)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (35.938)\n",
      "Training epoch: [1304/2000][1/4]; Loss: 2.12476015031871\n",
      "Training Accuracy Epoch: [1303]\tPrec@1 25.000 (25.000)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (31.250)\n",
      "Training epoch: [1305/2000][1/4]; Loss: 2.0808458689593166\n",
      "Training Accuracy Epoch: [1304]\tPrec@1 35.156 (35.156)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.812)\n",
      "Training epoch: [1306/2000][1/4]; Loss: 2.0866156540635044\n",
      "Training Accuracy Epoch: [1305]\tPrec@1 32.812 (32.812)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (35.938)\n",
      "Training epoch: [1307/2000][1/4]; Loss: 2.0507700386868755\n",
      "Training Accuracy Epoch: [1306]\tPrec@1 32.031 (32.031)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (34.375)\n",
      "Training epoch: [1308/2000][1/4]; Loss: 2.072606813278747\n",
      "Training Accuracy Epoch: [1307]\tPrec@1 35.938 (35.938)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.812)\n",
      "Training epoch: [1309/2000][1/4]; Loss: 2.0698408282979996\n",
      "Training Accuracy Epoch: [1308]\tPrec@1 30.469 (30.469)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.031)\n",
      "Training epoch: [1310/2000][1/4]; Loss: 2.0587522309305903\n",
      "Training Accuracy Epoch: [1309]\tPrec@1 32.812 (32.812)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (31.250)\n",
      "Training epoch: [1311/2000][1/4]; Loss: 2.0682784543303416\n",
      "Training Accuracy Epoch: [1310]\tPrec@1 35.156 (35.156)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (30.469)\n",
      "Training epoch: [1312/2000][1/4]; Loss: 2.0407357233275794\n",
      "Training Accuracy Epoch: [1311]\tPrec@1 36.719 (36.719)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (35.156)\n",
      "Training epoch: [1313/2000][1/4]; Loss: 2.070802078883867\n",
      "Training Accuracy Epoch: [1312]\tPrec@1 33.594 (33.594)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (34.375)\n",
      "Training epoch: [1314/2000][1/4]; Loss: 2.053171344413597\n",
      "Training Accuracy Epoch: [1313]\tPrec@1 32.812 (32.812)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (32.031)\n",
      "Training epoch: [1315/2000][1/4]; Loss: 2.016917095028081\n",
      "Training Accuracy Epoch: [1314]\tPrec@1 41.406 (41.406)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (29.688)\n",
      "Training epoch: [1316/2000][1/4]; Loss: 2.0468171422032553\n",
      "Training Accuracy Epoch: [1315]\tPrec@1 37.500 (37.500)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (31.250)\n",
      "Training epoch: [1317/2000][1/4]; Loss: 2.1312380842393077\n",
      "Training Accuracy Epoch: [1316]\tPrec@1 25.000 (25.000)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (30.469)\n",
      "Training epoch: [1318/2000][1/4]; Loss: 2.0273048063396564\n",
      "Training Accuracy Epoch: [1317]\tPrec@1 34.375 (34.375)\tPrec@2 56.250 (56.250)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (32.031)\n",
      "Training epoch: [1319/2000][1/4]; Loss: 2.0931341412623845\n",
      "Training Accuracy Epoch: [1318]\tPrec@1 33.594 (33.594)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (29.688)\n",
      "Training epoch: [1320/2000][1/4]; Loss: 1.9985361554762087\n",
      "Training Accuracy Epoch: [1319]\tPrec@1 45.312 (45.312)\tPrec@2 58.594 (58.594)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (31.250)\n",
      "Training epoch: [1321/2000][1/4]; Loss: 2.0841230453950326\n",
      "Training Accuracy Epoch: [1320]\tPrec@1 32.031 (32.031)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (30.469)\n",
      "Training epoch: [1322/2000][1/4]; Loss: 2.1045273113887917\n",
      "Training Accuracy Epoch: [1321]\tPrec@1 26.562 (26.562)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.812)\n",
      "Training epoch: [1323/2000][1/4]; Loss: 2.099937112040964\n",
      "Training Accuracy Epoch: [1322]\tPrec@1 34.375 (34.375)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (31.250)\n",
      "Training epoch: [1324/2000][1/4]; Loss: 1.9916040927666083\n",
      "Training Accuracy Epoch: [1323]\tPrec@1 44.531 (44.531)\tPrec@2 60.156 (60.156)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (35.938)\n",
      "Training epoch: [1325/2000][1/4]; Loss: 2.0245275670062934\n",
      "Training Accuracy Epoch: [1324]\tPrec@1 38.281 (38.281)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (31.250)\n",
      "Training epoch: [1326/2000][1/4]; Loss: 2.0757715450083447\n",
      "Training Accuracy Epoch: [1325]\tPrec@1 30.469 (30.469)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (29.688)\n",
      "Training epoch: [1327/2000][1/4]; Loss: 2.037915685362032\n",
      "Training Accuracy Epoch: [1326]\tPrec@1 36.719 (36.719)\tPrec@2 56.250 (56.250)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (32.031)\n",
      "Training epoch: [1328/2000][1/4]; Loss: 2.031852247010031\n",
      "Training Accuracy Epoch: [1327]\tPrec@1 39.844 (39.844)\tPrec@2 56.250 (56.250)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (28.125)\n",
      "Training epoch: [1329/2000][1/4]; Loss: 2.0596372046721\n",
      "Training Accuracy Epoch: [1328]\tPrec@1 35.938 (35.938)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (34.375)\n",
      "Training epoch: [1330/2000][1/4]; Loss: 2.0268171395120484\n",
      "Training Accuracy Epoch: [1329]\tPrec@1 37.500 (37.500)\tPrec@2 52.344 (52.344)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>>>\tPrec@1 (17.188)\tPrec@2 (29.688)\n",
      "Training epoch: [1331/2000][1/4]; Loss: 2.007280363332551\n",
      "Training Accuracy Epoch: [1330]\tPrec@1 47.656 (47.656)\tPrec@2 57.812 (57.812)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (31.250)\n",
      "Training epoch: [1332/2000][1/4]; Loss: 2.0392964542458754\n",
      "Training Accuracy Epoch: [1331]\tPrec@1 34.375 (34.375)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (31.250)\n",
      "Training epoch: [1333/2000][1/4]; Loss: 2.0640076552826208\n",
      "Training Accuracy Epoch: [1332]\tPrec@1 33.594 (33.594)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.812)\n",
      "Training epoch: [1334/2000][1/4]; Loss: 2.1158006315812585\n",
      "Training Accuracy Epoch: [1333]\tPrec@1 28.906 (28.906)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (32.031)\n",
      "Training epoch: [1335/2000][1/4]; Loss: 2.070126500321755\n",
      "Training Accuracy Epoch: [1334]\tPrec@1 38.281 (38.281)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (31.250)\n",
      "Training epoch: [1336/2000][1/4]; Loss: 2.0636925917933837\n",
      "Training Accuracy Epoch: [1335]\tPrec@1 32.031 (32.031)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (27.344)\n",
      "Training epoch: [1337/2000][1/4]; Loss: 2.0858794567501087\n",
      "Training Accuracy Epoch: [1336]\tPrec@1 30.469 (30.469)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (32.812)\n",
      "Training epoch: [1338/2000][1/4]; Loss: 2.077089598147318\n",
      "Training Accuracy Epoch: [1337]\tPrec@1 32.812 (32.812)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (32.031)\n",
      "Training epoch: [1339/2000][1/4]; Loss: 2.0892230730469152\n",
      "Training Accuracy Epoch: [1338]\tPrec@1 27.344 (27.344)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (29.688)\n",
      "Training epoch: [1340/2000][1/4]; Loss: 2.0447356232600775\n",
      "Training Accuracy Epoch: [1339]\tPrec@1 37.500 (37.500)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (32.812)\n",
      "Training epoch: [1341/2000][1/4]; Loss: 2.0381717974775335\n",
      "Training Accuracy Epoch: [1340]\tPrec@1 35.938 (35.938)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (24.219)\n",
      "Training epoch: [1342/2000][1/4]; Loss: 2.041240580717553\n",
      "Training Accuracy Epoch: [1341]\tPrec@1 39.844 (39.844)\tPrec@2 58.594 (58.594)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (36.719)\n",
      "Training epoch: [1343/2000][1/4]; Loss: 2.0508509489376165\n",
      "Training Accuracy Epoch: [1342]\tPrec@1 30.469 (30.469)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (34.375)\n",
      "Training epoch: [1344/2000][1/4]; Loss: 2.058501869046517\n",
      "Training Accuracy Epoch: [1343]\tPrec@1 35.938 (35.938)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (34.375)\n",
      "Training epoch: [1345/2000][1/4]; Loss: 2.034337609579882\n",
      "Training Accuracy Epoch: [1344]\tPrec@1 34.375 (34.375)\tPrec@2 56.250 (56.250)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (29.688)\n",
      "Training epoch: [1346/2000][1/4]; Loss: 2.013394738270845\n",
      "Training Accuracy Epoch: [1345]\tPrec@1 42.969 (42.969)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (30.469)\n",
      "Training epoch: [1347/2000][1/4]; Loss: 2.086462808124376\n",
      "Training Accuracy Epoch: [1346]\tPrec@1 30.469 (30.469)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (33.594)\n",
      "Training epoch: [1348/2000][1/4]; Loss: 2.090617334176608\n",
      "Training Accuracy Epoch: [1347]\tPrec@1 32.031 (32.031)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.812)\n",
      "Training epoch: [1349/2000][1/4]; Loss: 2.0970842727722196\n",
      "Training Accuracy Epoch: [1348]\tPrec@1 29.688 (29.688)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (32.812)\n",
      "Training epoch: [1350/2000][1/4]; Loss: 2.048787586933372\n",
      "Training Accuracy Epoch: [1349]\tPrec@1 37.500 (37.500)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (32.031)\n",
      "Training epoch: [1351/2000][1/4]; Loss: 2.0761626560826385\n",
      "Training Accuracy Epoch: [1350]\tPrec@1 31.250 (31.250)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (36.719)\n",
      "Training epoch: [1352/2000][1/4]; Loss: 2.086998628483597\n",
      "Training Accuracy Epoch: [1351]\tPrec@1 32.031 (32.031)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (29.688)\n",
      "Training epoch: [1353/2000][1/4]; Loss: 2.0778163317564036\n",
      "Training Accuracy Epoch: [1352]\tPrec@1 32.812 (32.812)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (12.500)\tPrec@2 (24.219)\n",
      "Training epoch: [1354/2000][1/4]; Loss: 2.089833397583416\n",
      "Training Accuracy Epoch: [1353]\tPrec@1 32.031 (32.031)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (30.469)\n",
      "Training epoch: [1355/2000][1/4]; Loss: 2.0583675886133768\n",
      "Training Accuracy Epoch: [1354]\tPrec@1 35.156 (35.156)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (29.688)\n",
      "Training epoch: [1356/2000][1/4]; Loss: 2.0403719845469666\n",
      "Training Accuracy Epoch: [1355]\tPrec@1 39.062 (39.062)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (29.688)\n",
      "Training epoch: [1357/2000][1/4]; Loss: 2.0485542878162786\n",
      "Training Accuracy Epoch: [1356]\tPrec@1 32.031 (32.031)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (32.031)\n",
      "Training epoch: [1358/2000][1/4]; Loss: 2.1001851777120786\n",
      "Training Accuracy Epoch: [1357]\tPrec@1 34.375 (34.375)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (29.688)\n",
      "Training epoch: [1359/2000][1/4]; Loss: 2.024266915469385\n",
      "Training Accuracy Epoch: [1358]\tPrec@1 39.062 (39.062)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (29.688)\n",
      "Training epoch: [1360/2000][1/4]; Loss: 2.0560164884159526\n",
      "Training Accuracy Epoch: [1359]\tPrec@1 32.812 (32.812)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.031)\n",
      "Training epoch: [1361/2000][1/4]; Loss: 2.0476000388213853\n",
      "Training Accuracy Epoch: [1360]\tPrec@1 30.469 (30.469)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (33.594)\n",
      "Training epoch: [1362/2000][1/4]; Loss: 2.0598435773084005\n",
      "Training Accuracy Epoch: [1361]\tPrec@1 32.812 (32.812)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (30.469)\n",
      "Training epoch: [1363/2000][1/4]; Loss: 1.9917559326163365\n",
      "Training Accuracy Epoch: [1362]\tPrec@1 40.625 (40.625)\tPrec@2 60.938 (60.938)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (35.156)\n",
      "Training epoch: [1364/2000][1/4]; Loss: 2.0234159737665207\n",
      "Training Accuracy Epoch: [1363]\tPrec@1 39.844 (39.844)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (29.688)\n",
      "Training epoch: [1365/2000][1/4]; Loss: 2.0570789101624323\n",
      "Training Accuracy Epoch: [1364]\tPrec@1 33.594 (33.594)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (27.344)\n",
      "Training epoch: [1366/2000][1/4]; Loss: 2.0641266588500593\n",
      "Training Accuracy Epoch: [1365]\tPrec@1 38.281 (38.281)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (34.375)\n",
      "Training epoch: [1367/2000][1/4]; Loss: 2.0663899099831364\n",
      "Training Accuracy Epoch: [1366]\tPrec@1 34.375 (34.375)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (27.344)\n",
      "Training epoch: [1368/2000][1/4]; Loss: 2.0828170924763465\n",
      "Training Accuracy Epoch: [1367]\tPrec@1 32.031 (32.031)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (29.688)\n",
      "Training epoch: [1369/2000][1/4]; Loss: 2.096240408525755\n",
      "Training Accuracy Epoch: [1368]\tPrec@1 28.906 (28.906)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.812)\n",
      "Training epoch: [1370/2000][1/4]; Loss: 1.9881691792946181\n",
      "Training Accuracy Epoch: [1369]\tPrec@1 45.312 (45.312)\tPrec@2 61.719 (61.719)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (34.375)\n",
      "Training epoch: [1371/2000][1/4]; Loss: 2.0103401595560304\n",
      "Training Accuracy Epoch: [1370]\tPrec@1 40.625 (40.625)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (29.688)\n",
      "Training epoch: [1372/2000][1/4]; Loss: 2.035690036789352\n",
      "Training Accuracy Epoch: [1371]\tPrec@1 32.812 (32.812)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.812)\n",
      "Training epoch: [1373/2000][1/4]; Loss: 2.0683496612705863\n",
      "Training Accuracy Epoch: [1372]\tPrec@1 34.375 (34.375)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (31.250)\n",
      "Training epoch: [1374/2000][1/4]; Loss: 2.06813071375939\n",
      "Training Accuracy Epoch: [1373]\tPrec@1 34.375 (34.375)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (32.031)\n",
      "Training epoch: [1375/2000][1/4]; Loss: 2.0030220106316268\n",
      "Training Accuracy Epoch: [1374]\tPrec@1 37.500 (37.500)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (28.906)\n",
      "Training epoch: [1376/2000][1/4]; Loss: 2.0786700671715224\n",
      "Training Accuracy Epoch: [1375]\tPrec@1 33.594 (33.594)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (30.469)\n",
      "Training epoch: [1377/2000][1/4]; Loss: 2.1636868975958756\n",
      "Training Accuracy Epoch: [1376]\tPrec@1 23.438 (23.438)\tPrec@2 40.625 (40.625)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>>>\tPrec@1 (21.094)\tPrec@2 (36.719)\n",
      "Training epoch: [1378/2000][1/4]; Loss: 2.0059743706840747\n",
      "Training Accuracy Epoch: [1377]\tPrec@1 38.281 (38.281)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (33.594)\n",
      "Training epoch: [1379/2000][1/4]; Loss: 2.0400846299998214\n",
      "Training Accuracy Epoch: [1378]\tPrec@1 34.375 (34.375)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (30.469)\n",
      "Training epoch: [1380/2000][1/4]; Loss: 2.0284416106579077\n",
      "Training Accuracy Epoch: [1379]\tPrec@1 35.938 (35.938)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.031)\n",
      "Training epoch: [1381/2000][1/4]; Loss: 2.046888489673875\n",
      "Training Accuracy Epoch: [1380]\tPrec@1 39.844 (39.844)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (36.719)\n",
      "Training epoch: [1382/2000][1/4]; Loss: 2.021125488475601\n",
      "Training Accuracy Epoch: [1381]\tPrec@1 42.188 (42.188)\tPrec@2 57.812 (57.812)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (29.688)\n",
      "Training epoch: [1383/2000][1/4]; Loss: 2.0753247971968465\n",
      "Training Accuracy Epoch: [1382]\tPrec@1 32.031 (32.031)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (35.156)\n",
      "Training epoch: [1384/2000][1/4]; Loss: 2.0688648439988513\n",
      "Training Accuracy Epoch: [1383]\tPrec@1 29.688 (29.688)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (31.250)\n",
      "Training epoch: [1385/2000][1/4]; Loss: 2.115505649802312\n",
      "Training Accuracy Epoch: [1384]\tPrec@1 28.906 (28.906)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (28.125)\n",
      "Training epoch: [1386/2000][1/4]; Loss: 2.082700461752065\n",
      "Training Accuracy Epoch: [1385]\tPrec@1 25.000 (25.000)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (32.812)\n",
      "Training epoch: [1387/2000][1/4]; Loss: 2.0660338182883153\n",
      "Training Accuracy Epoch: [1386]\tPrec@1 28.906 (28.906)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (34.375)\n",
      "Training epoch: [1388/2000][1/4]; Loss: 2.0371808976200882\n",
      "Training Accuracy Epoch: [1387]\tPrec@1 40.625 (40.625)\tPrec@2 56.250 (56.250)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (33.594)\n",
      "Training epoch: [1389/2000][1/4]; Loss: 2.0012591228758927\n",
      "Training Accuracy Epoch: [1388]\tPrec@1 37.500 (37.500)\tPrec@2 61.719 (61.719)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (31.250)\n",
      "Training epoch: [1390/2000][1/4]; Loss: 2.0812353096834997\n",
      "Training Accuracy Epoch: [1389]\tPrec@1 30.469 (30.469)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (26.562)\n",
      "Training epoch: [1391/2000][1/4]; Loss: 1.9970120599162682\n",
      "Training Accuracy Epoch: [1390]\tPrec@1 45.312 (45.312)\tPrec@2 58.594 (58.594)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (34.375)\n",
      "Training epoch: [1392/2000][1/4]; Loss: 2.0709005569927847\n",
      "Training Accuracy Epoch: [1391]\tPrec@1 29.688 (29.688)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (30.469)\n",
      "Training epoch: [1393/2000][1/4]; Loss: 2.1108563745541167\n",
      "Training Accuracy Epoch: [1392]\tPrec@1 29.688 (29.688)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (29.688)\n",
      "Training epoch: [1394/2000][1/4]; Loss: 2.0718680230992366\n",
      "Training Accuracy Epoch: [1393]\tPrec@1 28.906 (28.906)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (29.688)\n",
      "Training epoch: [1395/2000][1/4]; Loss: 2.0817407855202275\n",
      "Training Accuracy Epoch: [1394]\tPrec@1 32.031 (32.031)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (34.375)\n",
      "Training epoch: [1396/2000][1/4]; Loss: 2.0214395036280015\n",
      "Training Accuracy Epoch: [1395]\tPrec@1 36.719 (36.719)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (29.688)\n",
      "Training epoch: [1397/2000][1/4]; Loss: 2.0184005270003116\n",
      "Training Accuracy Epoch: [1396]\tPrec@1 39.844 (39.844)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (30.469)\n",
      "Training epoch: [1398/2000][1/4]; Loss: 2.066783421363067\n",
      "Training Accuracy Epoch: [1397]\tPrec@1 28.906 (28.906)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (31.250)\n",
      "Training epoch: [1399/2000][1/4]; Loss: 2.0258804885047232\n",
      "Training Accuracy Epoch: [1398]\tPrec@1 35.156 (35.156)\tPrec@2 56.250 (56.250)\n",
      "===>>>\tPrec@1 (24.219)\tPrec@2 (38.281)\n",
      "Training epoch: [1400/2000][1/4]; Loss: 2.117665527487019\n",
      "Training Accuracy Epoch: [1399]\tPrec@1 25.781 (25.781)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (31.250)\n",
      "Training epoch: [1401/2000][1/4]; Loss: 2.1012114017537415\n",
      "Training Accuracy Epoch: [1400]\tPrec@1 31.250 (31.250)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (31.250)\n",
      "Training epoch: [1402/2000][1/4]; Loss: 1.9908604552658051\n",
      "Training Accuracy Epoch: [1401]\tPrec@1 36.719 (36.719)\tPrec@2 57.031 (57.031)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (31.250)\n",
      "Training epoch: [1403/2000][1/4]; Loss: 1.984151715488097\n",
      "Training Accuracy Epoch: [1402]\tPrec@1 46.875 (46.875)\tPrec@2 61.719 (61.719)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (28.906)\n",
      "Training epoch: [1404/2000][1/4]; Loss: 2.035859622652629\n",
      "Training Accuracy Epoch: [1403]\tPrec@1 34.375 (34.375)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (32.031)\n",
      "Training epoch: [1405/2000][1/4]; Loss: 2.0790037906809227\n",
      "Training Accuracy Epoch: [1404]\tPrec@1 32.031 (32.031)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (36.719)\n",
      "Training epoch: [1406/2000][1/4]; Loss: 2.0229634222057546\n",
      "Training Accuracy Epoch: [1405]\tPrec@1 39.062 (39.062)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.031)\n",
      "Training epoch: [1407/2000][1/4]; Loss: 2.0394855723724246\n",
      "Training Accuracy Epoch: [1406]\tPrec@1 35.156 (35.156)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (33.594)\n",
      "Training epoch: [1408/2000][1/4]; Loss: 2.053946778817931\n",
      "Training Accuracy Epoch: [1407]\tPrec@1 35.938 (35.938)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.812)\n",
      "Training epoch: [1409/2000][1/4]; Loss: 2.029751616275431\n",
      "Training Accuracy Epoch: [1408]\tPrec@1 38.281 (38.281)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.031)\n",
      "Training epoch: [1410/2000][1/4]; Loss: 2.0964878479114213\n",
      "Training Accuracy Epoch: [1409]\tPrec@1 31.250 (31.250)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.812)\n",
      "Training epoch: [1411/2000][1/4]; Loss: 2.070333674078894\n",
      "Training Accuracy Epoch: [1410]\tPrec@1 33.594 (33.594)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (31.250)\n",
      "Training epoch: [1412/2000][1/4]; Loss: 2.0658863183694254\n",
      "Training Accuracy Epoch: [1411]\tPrec@1 34.375 (34.375)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (29.688)\n",
      "Training epoch: [1413/2000][1/4]; Loss: 2.072926565799052\n",
      "Training Accuracy Epoch: [1412]\tPrec@1 32.031 (32.031)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (34.375)\n",
      "Training epoch: [1414/2000][1/4]; Loss: 2.0428604810442827\n",
      "Training Accuracy Epoch: [1413]\tPrec@1 32.812 (32.812)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (30.469)\n",
      "Training epoch: [1415/2000][1/4]; Loss: 2.062186253752433\n",
      "Training Accuracy Epoch: [1414]\tPrec@1 33.594 (33.594)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.031)\n",
      "Training epoch: [1416/2000][1/4]; Loss: 2.0744980579413936\n",
      "Training Accuracy Epoch: [1415]\tPrec@1 32.812 (32.812)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (28.906)\n",
      "Training epoch: [1417/2000][1/4]; Loss: 2.0431176892159244\n",
      "Training Accuracy Epoch: [1416]\tPrec@1 37.500 (37.500)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (30.469)\n",
      "Training epoch: [1418/2000][1/4]; Loss: 2.047563254028302\n",
      "Training Accuracy Epoch: [1417]\tPrec@1 36.719 (36.719)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (32.812)\n",
      "Training epoch: [1419/2000][1/4]; Loss: 2.0938322360171275\n",
      "Training Accuracy Epoch: [1418]\tPrec@1 32.031 (32.031)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.031)\n",
      "Training epoch: [1420/2000][1/4]; Loss: 2.0932113699972117\n",
      "Training Accuracy Epoch: [1419]\tPrec@1 35.938 (35.938)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (30.469)\n",
      "Training epoch: [1421/2000][1/4]; Loss: 2.0586533538759997\n",
      "Training Accuracy Epoch: [1420]\tPrec@1 36.719 (36.719)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (30.469)\n",
      "Training epoch: [1422/2000][1/4]; Loss: 2.054667933036145\n",
      "Training Accuracy Epoch: [1421]\tPrec@1 35.156 (35.156)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (32.812)\n",
      "Training epoch: [1423/2000][1/4]; Loss: 2.0708937819977575\n",
      "Training Accuracy Epoch: [1422]\tPrec@1 32.812 (32.812)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (31.250)\n",
      "Training epoch: [1424/2000][1/4]; Loss: 2.0760882549183033\n",
      "Training Accuracy Epoch: [1423]\tPrec@1 26.562 (26.562)\tPrec@2 44.531 (44.531)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>>>\tPrec@1 (17.969)\tPrec@2 (31.250)\n",
      "Training epoch: [1425/2000][1/4]; Loss: 2.0349405338023128\n",
      "Training Accuracy Epoch: [1424]\tPrec@1 37.500 (37.500)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.906)\n",
      "Training epoch: [1426/2000][1/4]; Loss: 2.0514538922283108\n",
      "Training Accuracy Epoch: [1425]\tPrec@1 36.719 (36.719)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (14.062)\tPrec@2 (26.562)\n",
      "Training epoch: [1427/2000][1/4]; Loss: 2.0205591960975173\n",
      "Training Accuracy Epoch: [1426]\tPrec@1 35.156 (35.156)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (32.812)\n",
      "Training epoch: [1428/2000][1/4]; Loss: 2.0961559235360454\n",
      "Training Accuracy Epoch: [1427]\tPrec@1 32.031 (32.031)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (35.156)\n",
      "Training epoch: [1429/2000][1/4]; Loss: 2.095318633311936\n",
      "Training Accuracy Epoch: [1428]\tPrec@1 31.250 (31.250)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (29.688)\n",
      "Training epoch: [1430/2000][1/4]; Loss: 2.0618020580014194\n",
      "Training Accuracy Epoch: [1429]\tPrec@1 29.688 (29.688)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (30.469)\n",
      "Training epoch: [1431/2000][1/4]; Loss: 2.03134503434665\n",
      "Training Accuracy Epoch: [1430]\tPrec@1 35.938 (35.938)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (30.469)\n",
      "Training epoch: [1432/2000][1/4]; Loss: 2.078527360018004\n",
      "Training Accuracy Epoch: [1431]\tPrec@1 29.688 (29.688)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (28.125)\n",
      "Training epoch: [1433/2000][1/4]; Loss: 2.069683511653709\n",
      "Training Accuracy Epoch: [1432]\tPrec@1 34.375 (34.375)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (30.469)\n",
      "Training epoch: [1434/2000][1/4]; Loss: 2.07318252815319\n",
      "Training Accuracy Epoch: [1433]\tPrec@1 35.938 (35.938)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (31.250)\n",
      "Training epoch: [1435/2000][1/4]; Loss: 2.0948696179303985\n",
      "Training Accuracy Epoch: [1434]\tPrec@1 26.562 (26.562)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (32.812)\n",
      "Training epoch: [1436/2000][1/4]; Loss: 2.0519931162875245\n",
      "Training Accuracy Epoch: [1435]\tPrec@1 35.156 (35.156)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (30.469)\n",
      "Training epoch: [1437/2000][1/4]; Loss: 2.088078342932939\n",
      "Training Accuracy Epoch: [1436]\tPrec@1 28.125 (28.125)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.031)\n",
      "Training epoch: [1438/2000][1/4]; Loss: 2.084140020616526\n",
      "Training Accuracy Epoch: [1437]\tPrec@1 32.031 (32.031)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (30.469)\n",
      "Training epoch: [1439/2000][1/4]; Loss: 1.9967619809057429\n",
      "Training Accuracy Epoch: [1438]\tPrec@1 42.969 (42.969)\tPrec@2 58.594 (58.594)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.812)\n",
      "Training epoch: [1440/2000][1/4]; Loss: 2.0566104455850263\n",
      "Training Accuracy Epoch: [1439]\tPrec@1 34.375 (34.375)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (29.688)\n",
      "Training epoch: [1441/2000][1/4]; Loss: 2.0394156088848696\n",
      "Training Accuracy Epoch: [1440]\tPrec@1 35.938 (35.938)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (30.469)\n",
      "Training epoch: [1442/2000][1/4]; Loss: 1.9829388934803862\n",
      "Training Accuracy Epoch: [1441]\tPrec@1 44.531 (44.531)\tPrec@2 59.375 (59.375)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (35.156)\n",
      "Training epoch: [1443/2000][1/4]; Loss: 2.0560374841690483\n",
      "Training Accuracy Epoch: [1442]\tPrec@1 33.594 (33.594)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (35.938)\n",
      "Training epoch: [1444/2000][1/4]; Loss: 2.109906852627191\n",
      "Training Accuracy Epoch: [1443]\tPrec@1 32.031 (32.031)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (31.250)\n",
      "Training epoch: [1445/2000][1/4]; Loss: 2.051555325966219\n",
      "Training Accuracy Epoch: [1444]\tPrec@1 34.375 (34.375)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (25.781)\n",
      "Training epoch: [1446/2000][1/4]; Loss: 2.1057990810407805\n",
      "Training Accuracy Epoch: [1445]\tPrec@1 29.688 (29.688)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (31.250)\n",
      "Training epoch: [1447/2000][1/4]; Loss: 2.028365281110999\n",
      "Training Accuracy Epoch: [1446]\tPrec@1 39.844 (39.844)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (32.812)\n",
      "Training epoch: [1448/2000][1/4]; Loss: 2.069378443827309\n",
      "Training Accuracy Epoch: [1447]\tPrec@1 35.938 (35.938)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (28.906)\n",
      "Training epoch: [1449/2000][1/4]; Loss: 2.0137404185240504\n",
      "Training Accuracy Epoch: [1448]\tPrec@1 39.062 (39.062)\tPrec@2 56.250 (56.250)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (34.375)\n",
      "Training epoch: [1450/2000][1/4]; Loss: 2.0192658162253942\n",
      "Training Accuracy Epoch: [1449]\tPrec@1 37.500 (37.500)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (34.375)\n",
      "Training epoch: [1451/2000][1/4]; Loss: 2.0703401139817865\n",
      "Training Accuracy Epoch: [1450]\tPrec@1 32.031 (32.031)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (30.469)\n",
      "Training epoch: [1452/2000][1/4]; Loss: 2.0523330099492\n",
      "Training Accuracy Epoch: [1451]\tPrec@1 39.062 (39.062)\tPrec@2 56.250 (56.250)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (33.594)\n",
      "Training epoch: [1453/2000][1/4]; Loss: 2.0727345103764585\n",
      "Training Accuracy Epoch: [1452]\tPrec@1 32.031 (32.031)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.031)\n",
      "Training epoch: [1454/2000][1/4]; Loss: 2.0195608333991917\n",
      "Training Accuracy Epoch: [1453]\tPrec@1 39.844 (39.844)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.125)\n",
      "Training epoch: [1455/2000][1/4]; Loss: 2.0171711948776636\n",
      "Training Accuracy Epoch: [1454]\tPrec@1 39.844 (39.844)\tPrec@2 59.375 (59.375)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (33.594)\n",
      "Training epoch: [1456/2000][1/4]; Loss: 2.0723736862955917\n",
      "Training Accuracy Epoch: [1455]\tPrec@1 31.250 (31.250)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (28.125)\n",
      "Training epoch: [1457/2000][1/4]; Loss: 2.0533856721073507\n",
      "Training Accuracy Epoch: [1456]\tPrec@1 36.719 (36.719)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (34.375)\n",
      "Training epoch: [1458/2000][1/4]; Loss: 2.0389516009976902\n",
      "Training Accuracy Epoch: [1457]\tPrec@1 33.594 (33.594)\tPrec@2 56.250 (56.250)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (33.594)\n",
      "Training epoch: [1459/2000][1/4]; Loss: 2.0516488482106765\n",
      "Training Accuracy Epoch: [1458]\tPrec@1 34.375 (34.375)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (32.031)\n",
      "Training epoch: [1460/2000][1/4]; Loss: 2.120085749397387\n",
      "Training Accuracy Epoch: [1459]\tPrec@1 26.562 (26.562)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (14.062)\tPrec@2 (27.344)\n",
      "Training epoch: [1461/2000][1/4]; Loss: 2.105092600778427\n",
      "Training Accuracy Epoch: [1460]\tPrec@1 30.469 (30.469)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (33.594)\n",
      "Training epoch: [1462/2000][1/4]; Loss: 2.0659684625199333\n",
      "Training Accuracy Epoch: [1461]\tPrec@1 35.938 (35.938)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (31.250)\n",
      "Training epoch: [1463/2000][1/4]; Loss: 2.0299715024782894\n",
      "Training Accuracy Epoch: [1462]\tPrec@1 35.156 (35.156)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (33.594)\n",
      "Training epoch: [1464/2000][1/4]; Loss: 2.019079074878681\n",
      "Training Accuracy Epoch: [1463]\tPrec@1 40.625 (40.625)\tPrec@2 59.375 (59.375)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (35.156)\n",
      "Training epoch: [1465/2000][1/4]; Loss: 2.0690653930993017\n",
      "Training Accuracy Epoch: [1464]\tPrec@1 42.969 (42.969)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (28.906)\n",
      "Training epoch: [1466/2000][1/4]; Loss: 2.0747899538701438\n",
      "Training Accuracy Epoch: [1465]\tPrec@1 32.812 (32.812)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (35.938)\n",
      "Training epoch: [1467/2000][1/4]; Loss: 1.9902299352233142\n",
      "Training Accuracy Epoch: [1466]\tPrec@1 42.188 (42.188)\tPrec@2 59.375 (59.375)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (33.594)\n",
      "Training epoch: [1468/2000][1/4]; Loss: 2.047017560618506\n",
      "Training Accuracy Epoch: [1467]\tPrec@1 36.719 (36.719)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (34.375)\n",
      "Training epoch: [1469/2000][1/4]; Loss: 2.041262013310476\n",
      "Training Accuracy Epoch: [1468]\tPrec@1 35.938 (35.938)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (33.594)\n",
      "Training epoch: [1470/2000][1/4]; Loss: 2.024639963829702\n",
      "Training Accuracy Epoch: [1469]\tPrec@1 37.500 (37.500)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.906)\n",
      "Training epoch: [1471/2000][1/4]; Loss: 2.0270766143595003\n",
      "Training Accuracy Epoch: [1470]\tPrec@1 39.844 (39.844)\tPrec@2 53.125 (53.125)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>>>\tPrec@1 (21.875)\tPrec@2 (32.031)\n",
      "Training epoch: [1472/2000][1/4]; Loss: 2.066533440325915\n",
      "Training Accuracy Epoch: [1471]\tPrec@1 35.938 (35.938)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (30.469)\n",
      "Training epoch: [1473/2000][1/4]; Loss: 2.089200962362169\n",
      "Training Accuracy Epoch: [1472]\tPrec@1 32.031 (32.031)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (28.906)\n",
      "Training epoch: [1474/2000][1/4]; Loss: 2.121174368225831\n",
      "Training Accuracy Epoch: [1473]\tPrec@1 29.688 (29.688)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (14.062)\tPrec@2 (28.125)\n",
      "Training epoch: [1475/2000][1/4]; Loss: 2.068784873583352\n",
      "Training Accuracy Epoch: [1474]\tPrec@1 28.125 (28.125)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (28.125)\n",
      "Training epoch: [1476/2000][1/4]; Loss: 2.0368350235646657\n",
      "Training Accuracy Epoch: [1475]\tPrec@1 36.719 (36.719)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (27.344)\n",
      "Training epoch: [1477/2000][1/4]; Loss: 2.052685221786874\n",
      "Training Accuracy Epoch: [1476]\tPrec@1 35.156 (35.156)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.031)\n",
      "Training epoch: [1478/2000][1/4]; Loss: 2.069599902495408\n",
      "Training Accuracy Epoch: [1477]\tPrec@1 29.688 (29.688)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (26.562)\n",
      "Training epoch: [1479/2000][1/4]; Loss: 1.99363570168713\n",
      "Training Accuracy Epoch: [1478]\tPrec@1 44.531 (44.531)\tPrec@2 58.594 (58.594)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (36.719)\n",
      "Training epoch: [1480/2000][1/4]; Loss: 2.065348912300537\n",
      "Training Accuracy Epoch: [1479]\tPrec@1 34.375 (34.375)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (28.125)\n",
      "Training epoch: [1481/2000][1/4]; Loss: 2.0725400527423394\n",
      "Training Accuracy Epoch: [1480]\tPrec@1 31.250 (31.250)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (33.594)\n",
      "Training epoch: [1482/2000][1/4]; Loss: 2.0502906685857587\n",
      "Training Accuracy Epoch: [1481]\tPrec@1 33.594 (33.594)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (34.375)\n",
      "Training epoch: [1483/2000][1/4]; Loss: 2.1037563448666465\n",
      "Training Accuracy Epoch: [1482]\tPrec@1 32.812 (32.812)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (30.469)\n",
      "Training epoch: [1484/2000][1/4]; Loss: 2.0357861827190757\n",
      "Training Accuracy Epoch: [1483]\tPrec@1 34.375 (34.375)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (30.469)\n",
      "Training epoch: [1485/2000][1/4]; Loss: 2.0661793825604935\n",
      "Training Accuracy Epoch: [1484]\tPrec@1 31.250 (31.250)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (31.250)\n",
      "Training epoch: [1486/2000][1/4]; Loss: 2.05195349402943\n",
      "Training Accuracy Epoch: [1485]\tPrec@1 39.062 (39.062)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (36.719)\n",
      "Training epoch: [1487/2000][1/4]; Loss: 2.0242796975723802\n",
      "Training Accuracy Epoch: [1486]\tPrec@1 40.625 (40.625)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (29.688)\n",
      "Training epoch: [1488/2000][1/4]; Loss: 2.1032610690673743\n",
      "Training Accuracy Epoch: [1487]\tPrec@1 28.125 (28.125)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (32.812)\n",
      "Training epoch: [1489/2000][1/4]; Loss: 2.104973696442906\n",
      "Training Accuracy Epoch: [1488]\tPrec@1 32.031 (32.031)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (35.938)\n",
      "Training epoch: [1490/2000][1/4]; Loss: 2.053818486358238\n",
      "Training Accuracy Epoch: [1489]\tPrec@1 35.156 (35.156)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (34.375)\n",
      "Training epoch: [1491/2000][1/4]; Loss: 2.0475462457671645\n",
      "Training Accuracy Epoch: [1490]\tPrec@1 33.594 (33.594)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (28.125)\n",
      "Training epoch: [1492/2000][1/4]; Loss: 2.0296099402536547\n",
      "Training Accuracy Epoch: [1491]\tPrec@1 38.281 (38.281)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (31.250)\n",
      "Training epoch: [1493/2000][1/4]; Loss: 2.073636048090276\n",
      "Training Accuracy Epoch: [1492]\tPrec@1 28.125 (28.125)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (33.594)\n",
      "Training epoch: [1494/2000][1/4]; Loss: 2.0095945062795364\n",
      "Training Accuracy Epoch: [1493]\tPrec@1 42.969 (42.969)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (31.250)\n",
      "Training epoch: [1495/2000][1/4]; Loss: 2.035322896791972\n",
      "Training Accuracy Epoch: [1494]\tPrec@1 35.156 (35.156)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (25.781)\n",
      "Training epoch: [1496/2000][1/4]; Loss: 2.023450921979321\n",
      "Training Accuracy Epoch: [1495]\tPrec@1 36.719 (36.719)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (34.375)\n",
      "Training epoch: [1497/2000][1/4]; Loss: 2.053519778303791\n",
      "Training Accuracy Epoch: [1496]\tPrec@1 34.375 (34.375)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (28.906)\n",
      "Training epoch: [1498/2000][1/4]; Loss: 2.072830786391621\n",
      "Training Accuracy Epoch: [1497]\tPrec@1 37.500 (37.500)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (30.469)\n",
      "Training epoch: [1499/2000][1/4]; Loss: 2.0618984113810996\n",
      "Training Accuracy Epoch: [1498]\tPrec@1 33.594 (33.594)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (32.812)\n",
      "Training epoch: [1500/2000][1/4]; Loss: 2.0370231141306343\n",
      "Training Accuracy Epoch: [1499]\tPrec@1 33.594 (33.594)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (34.375)\n",
      "Training epoch: [1501/2000][1/4]; Loss: 2.0446780962718507\n",
      "Training Accuracy Epoch: [1500]\tPrec@1 35.156 (35.156)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (30.469)\n",
      "Training epoch: [1502/2000][1/4]; Loss: 2.078817411631215\n",
      "Training Accuracy Epoch: [1501]\tPrec@1 31.250 (31.250)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.031)\n",
      "Training epoch: [1503/2000][1/4]; Loss: 2.093048227871423\n",
      "Training Accuracy Epoch: [1502]\tPrec@1 29.688 (29.688)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (31.250)\n",
      "Training epoch: [1504/2000][1/4]; Loss: 2.015763039097781\n",
      "Training Accuracy Epoch: [1503]\tPrec@1 40.625 (40.625)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.906)\n",
      "Training epoch: [1505/2000][1/4]; Loss: 2.0625605267495417\n",
      "Training Accuracy Epoch: [1504]\tPrec@1 31.250 (31.250)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (30.469)\n",
      "Training epoch: [1506/2000][1/4]; Loss: 2.031762804905431\n",
      "Training Accuracy Epoch: [1505]\tPrec@1 33.594 (33.594)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (27.344)\n",
      "Training epoch: [1507/2000][1/4]; Loss: 2.02917932563198\n",
      "Training Accuracy Epoch: [1506]\tPrec@1 39.844 (39.844)\tPrec@2 59.375 (59.375)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (31.250)\n",
      "Training epoch: [1508/2000][1/4]; Loss: 2.0304861529167963\n",
      "Training Accuracy Epoch: [1507]\tPrec@1 37.500 (37.500)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (35.156)\n",
      "Training epoch: [1509/2000][1/4]; Loss: 2.0746353272445814\n",
      "Training Accuracy Epoch: [1508]\tPrec@1 32.031 (32.031)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (32.031)\n",
      "Training epoch: [1510/2000][1/4]; Loss: 2.0350205261244776\n",
      "Training Accuracy Epoch: [1509]\tPrec@1 36.719 (36.719)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (35.156)\n",
      "Training epoch: [1511/2000][1/4]; Loss: 2.0254755178279296\n",
      "Training Accuracy Epoch: [1510]\tPrec@1 40.625 (40.625)\tPrec@2 58.594 (58.594)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (28.906)\n",
      "Training epoch: [1512/2000][1/4]; Loss: 2.042255097536315\n",
      "Training Accuracy Epoch: [1511]\tPrec@1 35.938 (35.938)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.812)\n",
      "Training epoch: [1513/2000][1/4]; Loss: 2.0383018052394073\n",
      "Training Accuracy Epoch: [1512]\tPrec@1 36.719 (36.719)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (31.250)\n",
      "Training epoch: [1514/2000][1/4]; Loss: 2.1248879119014497\n",
      "Training Accuracy Epoch: [1513]\tPrec@1 25.781 (25.781)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.812)\n",
      "Training epoch: [1515/2000][1/4]; Loss: 2.0322671180394507\n",
      "Training Accuracy Epoch: [1514]\tPrec@1 35.156 (35.156)\tPrec@2 57.031 (57.031)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (33.594)\n",
      "Training epoch: [1516/2000][1/4]; Loss: 2.048537833608209\n",
      "Training Accuracy Epoch: [1515]\tPrec@1 37.500 (37.500)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (28.906)\n",
      "Training epoch: [1517/2000][1/4]; Loss: 2.0219064428638736\n",
      "Training Accuracy Epoch: [1516]\tPrec@1 36.719 (36.719)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (35.156)\n",
      "Training epoch: [1518/2000][1/4]; Loss: 2.0677925760942415\n",
      "Training Accuracy Epoch: [1517]\tPrec@1 33.594 (33.594)\tPrec@2 47.656 (47.656)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>>>\tPrec@1 (20.312)\tPrec@2 (34.375)\n",
      "Training epoch: [1519/2000][1/4]; Loss: 2.0062228781620517\n",
      "Training Accuracy Epoch: [1518]\tPrec@1 45.312 (45.312)\tPrec@2 62.500 (62.500)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (29.688)\n",
      "Training epoch: [1520/2000][1/4]; Loss: 2.0280961477122794\n",
      "Training Accuracy Epoch: [1519]\tPrec@1 35.938 (35.938)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (29.688)\n",
      "Training epoch: [1521/2000][1/4]; Loss: 2.0849252746520013\n",
      "Training Accuracy Epoch: [1520]\tPrec@1 28.125 (28.125)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (25.781)\tPrec@2 (39.062)\n",
      "Training epoch: [1522/2000][1/4]; Loss: 2.046976674820256\n",
      "Training Accuracy Epoch: [1521]\tPrec@1 29.688 (29.688)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (12.500)\tPrec@2 (24.219)\n",
      "Training epoch: [1523/2000][1/4]; Loss: 2.0291886659078924\n",
      "Training Accuracy Epoch: [1522]\tPrec@1 36.719 (36.719)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (32.031)\n",
      "Training epoch: [1524/2000][1/4]; Loss: 2.0957456340658553\n",
      "Training Accuracy Epoch: [1523]\tPrec@1 32.812 (32.812)\tPrec@2 40.625 (40.625)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (32.812)\n",
      "Training epoch: [1525/2000][1/4]; Loss: 2.073623035845479\n",
      "Training Accuracy Epoch: [1524]\tPrec@1 31.250 (31.250)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.812)\n",
      "Training epoch: [1526/2000][1/4]; Loss: 2.080515398600086\n",
      "Training Accuracy Epoch: [1525]\tPrec@1 31.250 (31.250)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (34.375)\n",
      "Training epoch: [1527/2000][1/4]; Loss: 2.028805962069067\n",
      "Training Accuracy Epoch: [1526]\tPrec@1 40.625 (40.625)\tPrec@2 57.031 (57.031)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (34.375)\n",
      "Training epoch: [1528/2000][1/4]; Loss: 2.0151775859362995\n",
      "Training Accuracy Epoch: [1527]\tPrec@1 38.281 (38.281)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (30.469)\n",
      "Training epoch: [1529/2000][1/4]; Loss: 2.050912619833671\n",
      "Training Accuracy Epoch: [1528]\tPrec@1 39.062 (39.062)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (32.031)\n",
      "Training epoch: [1530/2000][1/4]; Loss: 2.05906532274938\n",
      "Training Accuracy Epoch: [1529]\tPrec@1 34.375 (34.375)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (31.250)\n",
      "Training epoch: [1531/2000][1/4]; Loss: 2.0859836178177957\n",
      "Training Accuracy Epoch: [1530]\tPrec@1 35.156 (35.156)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (27.344)\n",
      "Training epoch: [1532/2000][1/4]; Loss: 2.0927313615520324\n",
      "Training Accuracy Epoch: [1531]\tPrec@1 29.688 (29.688)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (36.719)\n",
      "Training epoch: [1533/2000][1/4]; Loss: 2.0540088348686227\n",
      "Training Accuracy Epoch: [1532]\tPrec@1 35.938 (35.938)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (36.719)\n",
      "Training epoch: [1534/2000][1/4]; Loss: 2.0788465017522157\n",
      "Training Accuracy Epoch: [1533]\tPrec@1 32.031 (32.031)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (30.469)\n",
      "Training epoch: [1535/2000][1/4]; Loss: 2.0505697329654877\n",
      "Training Accuracy Epoch: [1534]\tPrec@1 32.812 (32.812)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.031)\n",
      "Training epoch: [1536/2000][1/4]; Loss: 2.0539566231613517\n",
      "Training Accuracy Epoch: [1535]\tPrec@1 35.938 (35.938)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.031)\n",
      "Training epoch: [1537/2000][1/4]; Loss: 2.069870600003659\n",
      "Training Accuracy Epoch: [1536]\tPrec@1 31.250 (31.250)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (33.594)\n",
      "Training epoch: [1538/2000][1/4]; Loss: 2.0855793539361906\n",
      "Training Accuracy Epoch: [1537]\tPrec@1 31.250 (31.250)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (34.375)\n",
      "Training epoch: [1539/2000][1/4]; Loss: 2.040994842218945\n",
      "Training Accuracy Epoch: [1538]\tPrec@1 33.594 (33.594)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.125)\n",
      "Training epoch: [1540/2000][1/4]; Loss: 2.0549593494434806\n",
      "Training Accuracy Epoch: [1539]\tPrec@1 31.250 (31.250)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (35.156)\n",
      "Training epoch: [1541/2000][1/4]; Loss: 2.0928354980116253\n",
      "Training Accuracy Epoch: [1540]\tPrec@1 32.031 (32.031)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (31.250)\n",
      "Training epoch: [1542/2000][1/4]; Loss: 2.0629735882897764\n",
      "Training Accuracy Epoch: [1541]\tPrec@1 32.812 (32.812)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (29.688)\n",
      "Training epoch: [1543/2000][1/4]; Loss: 2.044544715244933\n",
      "Training Accuracy Epoch: [1542]\tPrec@1 36.719 (36.719)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (29.688)\n",
      "Training epoch: [1544/2000][1/4]; Loss: 2.0557881604892017\n",
      "Training Accuracy Epoch: [1543]\tPrec@1 31.250 (31.250)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.031)\n",
      "Training epoch: [1545/2000][1/4]; Loss: 2.034594819949186\n",
      "Training Accuracy Epoch: [1544]\tPrec@1 37.500 (37.500)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (34.375)\n",
      "Training epoch: [1546/2000][1/4]; Loss: 2.047173234743304\n",
      "Training Accuracy Epoch: [1545]\tPrec@1 32.812 (32.812)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (31.250)\n",
      "Training epoch: [1547/2000][1/4]; Loss: 2.058260052213808\n",
      "Training Accuracy Epoch: [1546]\tPrec@1 30.469 (30.469)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (31.250)\n",
      "Training epoch: [1548/2000][1/4]; Loss: 2.054898549758222\n",
      "Training Accuracy Epoch: [1547]\tPrec@1 31.250 (31.250)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (32.031)\n",
      "Training epoch: [1549/2000][1/4]; Loss: 2.020893072163891\n",
      "Training Accuracy Epoch: [1548]\tPrec@1 39.844 (39.844)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.812)\n",
      "Training epoch: [1550/2000][1/4]; Loss: 2.0334963030691258\n",
      "Training Accuracy Epoch: [1549]\tPrec@1 34.375 (34.375)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (33.594)\n",
      "Training epoch: [1551/2000][1/4]; Loss: 2.1026018629550487\n",
      "Training Accuracy Epoch: [1550]\tPrec@1 27.344 (27.344)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (32.031)\n",
      "Training epoch: [1552/2000][1/4]; Loss: 2.045624391873187\n",
      "Training Accuracy Epoch: [1551]\tPrec@1 37.500 (37.500)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.812)\n",
      "Training epoch: [1553/2000][1/4]; Loss: 2.0583760232338113\n",
      "Training Accuracy Epoch: [1552]\tPrec@1 32.031 (32.031)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (32.031)\n",
      "Training epoch: [1554/2000][1/4]; Loss: 2.105488609097599\n",
      "Training Accuracy Epoch: [1553]\tPrec@1 28.125 (28.125)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (28.125)\n",
      "Training epoch: [1555/2000][1/4]; Loss: 2.050920249614351\n",
      "Training Accuracy Epoch: [1554]\tPrec@1 33.594 (33.594)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (31.250)\n",
      "Training epoch: [1556/2000][1/4]; Loss: 2.1138991071987356\n",
      "Training Accuracy Epoch: [1555]\tPrec@1 22.656 (22.656)\tPrec@2 36.719 (36.719)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (34.375)\n",
      "Training epoch: [1557/2000][1/4]; Loss: 2.0602193114512164\n",
      "Training Accuracy Epoch: [1556]\tPrec@1 34.375 (34.375)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (30.469)\n",
      "Training epoch: [1558/2000][1/4]; Loss: 2.0713173160842007\n",
      "Training Accuracy Epoch: [1557]\tPrec@1 35.938 (35.938)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (32.031)\n",
      "Training epoch: [1559/2000][1/4]; Loss: 2.0309272353978645\n",
      "Training Accuracy Epoch: [1558]\tPrec@1 36.719 (36.719)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (31.250)\n",
      "Training epoch: [1560/2000][1/4]; Loss: 2.071284671125478\n",
      "Training Accuracy Epoch: [1559]\tPrec@1 32.812 (32.812)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (27.344)\n",
      "Training epoch: [1561/2000][1/4]; Loss: 2.056453182984622\n",
      "Training Accuracy Epoch: [1560]\tPrec@1 33.594 (33.594)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (34.375)\n",
      "Training epoch: [1562/2000][1/4]; Loss: 2.0302299789036375\n",
      "Training Accuracy Epoch: [1561]\tPrec@1 35.156 (35.156)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (34.375)\n",
      "Training epoch: [1563/2000][1/4]; Loss: 2.029010138426594\n",
      "Training Accuracy Epoch: [1562]\tPrec@1 40.625 (40.625)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (29.688)\n",
      "Training epoch: [1564/2000][1/4]; Loss: 2.063938237775244\n",
      "Training Accuracy Epoch: [1563]\tPrec@1 35.156 (35.156)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (29.688)\n",
      "Training epoch: [1565/2000][1/4]; Loss: 2.0556837207388305\n",
      "Training Accuracy Epoch: [1564]\tPrec@1 35.938 (35.938)\tPrec@2 49.219 (49.219)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>>>\tPrec@1 (17.969)\tPrec@2 (32.031)\n",
      "Training epoch: [1566/2000][1/4]; Loss: 2.0823567846249684\n",
      "Training Accuracy Epoch: [1565]\tPrec@1 34.375 (34.375)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (32.812)\n",
      "Training epoch: [1567/2000][1/4]; Loss: 2.079166453970712\n",
      "Training Accuracy Epoch: [1566]\tPrec@1 32.031 (32.031)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (13.281)\tPrec@2 (29.688)\n",
      "Training epoch: [1568/2000][1/4]; Loss: 2.0992390678859105\n",
      "Training Accuracy Epoch: [1567]\tPrec@1 28.125 (28.125)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (31.250)\n",
      "Training epoch: [1569/2000][1/4]; Loss: 2.027554761137595\n",
      "Training Accuracy Epoch: [1568]\tPrec@1 36.719 (36.719)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (31.250)\n",
      "Training epoch: [1570/2000][1/4]; Loss: 2.069693960180018\n",
      "Training Accuracy Epoch: [1569]\tPrec@1 34.375 (34.375)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (32.031)\n",
      "Training epoch: [1571/2000][1/4]; Loss: 2.0388255921630494\n",
      "Training Accuracy Epoch: [1570]\tPrec@1 35.156 (35.156)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (31.250)\n",
      "Training epoch: [1572/2000][1/4]; Loss: 2.0794288389176625\n",
      "Training Accuracy Epoch: [1571]\tPrec@1 32.031 (32.031)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (29.688)\n",
      "Training epoch: [1573/2000][1/4]; Loss: 2.0339731886690813\n",
      "Training Accuracy Epoch: [1572]\tPrec@1 37.500 (37.500)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (30.469)\n",
      "Training epoch: [1574/2000][1/4]; Loss: 2.0134597059308903\n",
      "Training Accuracy Epoch: [1573]\tPrec@1 38.281 (38.281)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (31.250)\n",
      "Training epoch: [1575/2000][1/4]; Loss: 2.047138436932434\n",
      "Training Accuracy Epoch: [1574]\tPrec@1 39.062 (39.062)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (31.250)\n",
      "Training epoch: [1576/2000][1/4]; Loss: 2.072416866284187\n",
      "Training Accuracy Epoch: [1575]\tPrec@1 36.719 (36.719)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (34.375)\n",
      "Training epoch: [1577/2000][1/4]; Loss: 2.051899513008279\n",
      "Training Accuracy Epoch: [1576]\tPrec@1 33.594 (33.594)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (29.688)\n",
      "Training epoch: [1578/2000][1/4]; Loss: 2.010741127624246\n",
      "Training Accuracy Epoch: [1577]\tPrec@1 41.406 (41.406)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (28.125)\n",
      "Training epoch: [1579/2000][1/4]; Loss: 2.0623464242273903\n",
      "Training Accuracy Epoch: [1578]\tPrec@1 36.719 (36.719)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (32.812)\n",
      "Training epoch: [1580/2000][1/4]; Loss: 2.0578536359037964\n",
      "Training Accuracy Epoch: [1579]\tPrec@1 33.594 (33.594)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.812)\n",
      "Training epoch: [1581/2000][1/4]; Loss: 2.1102252979020477\n",
      "Training Accuracy Epoch: [1580]\tPrec@1 23.438 (23.438)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (35.938)\n",
      "Training epoch: [1582/2000][1/4]; Loss: 2.056792596006943\n",
      "Training Accuracy Epoch: [1581]\tPrec@1 36.719 (36.719)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (24.219)\tPrec@2 (34.375)\n",
      "Training epoch: [1583/2000][1/4]; Loss: 2.087342963155677\n",
      "Training Accuracy Epoch: [1582]\tPrec@1 28.906 (28.906)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.031)\n",
      "Training epoch: [1584/2000][1/4]; Loss: 2.0354993506812242\n",
      "Training Accuracy Epoch: [1583]\tPrec@1 37.500 (37.500)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.812)\n",
      "Training epoch: [1585/2000][1/4]; Loss: 2.050907784098343\n",
      "Training Accuracy Epoch: [1584]\tPrec@1 38.281 (38.281)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (33.594)\n",
      "Training epoch: [1586/2000][1/4]; Loss: 2.070636913074827\n",
      "Training Accuracy Epoch: [1585]\tPrec@1 34.375 (34.375)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (33.594)\n",
      "Training epoch: [1587/2000][1/4]; Loss: 2.0066765543787852\n",
      "Training Accuracy Epoch: [1586]\tPrec@1 37.500 (37.500)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (31.250)\n",
      "Training epoch: [1588/2000][1/4]; Loss: 2.0306318522796394\n",
      "Training Accuracy Epoch: [1587]\tPrec@1 32.031 (32.031)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (31.250)\n",
      "Training epoch: [1589/2000][1/4]; Loss: 2.0437750700108825\n",
      "Training Accuracy Epoch: [1588]\tPrec@1 33.594 (33.594)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (30.469)\n",
      "Training epoch: [1590/2000][1/4]; Loss: 2.0640700577872906\n",
      "Training Accuracy Epoch: [1589]\tPrec@1 33.594 (33.594)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (34.375)\n",
      "Training epoch: [1591/2000][1/4]; Loss: 2.0691894963607123\n",
      "Training Accuracy Epoch: [1590]\tPrec@1 32.031 (32.031)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (29.688)\n",
      "Training epoch: [1592/2000][1/4]; Loss: 2.034803917441601\n",
      "Training Accuracy Epoch: [1591]\tPrec@1 33.594 (33.594)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (36.719)\n",
      "Training epoch: [1593/2000][1/4]; Loss: 2.063212803606699\n",
      "Training Accuracy Epoch: [1592]\tPrec@1 32.812 (32.812)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (34.375)\n",
      "Training epoch: [1594/2000][1/4]; Loss: 2.0470218248086707\n",
      "Training Accuracy Epoch: [1593]\tPrec@1 40.625 (40.625)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (30.469)\n",
      "Training epoch: [1595/2000][1/4]; Loss: 2.0466506179717596\n",
      "Training Accuracy Epoch: [1594]\tPrec@1 34.375 (34.375)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (31.250)\n",
      "Training epoch: [1596/2000][1/4]; Loss: 2.0047990224782275\n",
      "Training Accuracy Epoch: [1595]\tPrec@1 43.750 (43.750)\tPrec@2 57.812 (57.812)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (36.719)\n",
      "Training epoch: [1597/2000][1/4]; Loss: 2.0569332311543342\n",
      "Training Accuracy Epoch: [1596]\tPrec@1 33.594 (33.594)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (29.688)\n",
      "Training epoch: [1598/2000][1/4]; Loss: 2.0449180292238482\n",
      "Training Accuracy Epoch: [1597]\tPrec@1 31.250 (31.250)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (33.594)\n",
      "Training epoch: [1599/2000][1/4]; Loss: 2.042877677460205\n",
      "Training Accuracy Epoch: [1598]\tPrec@1 30.469 (30.469)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (31.250)\n",
      "Training epoch: [1600/2000][1/4]; Loss: 2.0213952595835787\n",
      "Training Accuracy Epoch: [1599]\tPrec@1 39.844 (39.844)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (24.219)\tPrec@2 (32.812)\n",
      "Training epoch: [1601/2000][1/4]; Loss: 2.0395798081185803\n",
      "Training Accuracy Epoch: [1600]\tPrec@1 35.156 (35.156)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (36.719)\n",
      "Training epoch: [1602/2000][1/4]; Loss: 2.0705457208139717\n",
      "Training Accuracy Epoch: [1601]\tPrec@1 35.156 (35.156)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (34.375)\n",
      "Training epoch: [1603/2000][1/4]; Loss: 2.008784162904039\n",
      "Training Accuracy Epoch: [1602]\tPrec@1 37.500 (37.500)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (31.250)\n",
      "Training epoch: [1604/2000][1/4]; Loss: 2.0759363173578023\n",
      "Training Accuracy Epoch: [1603]\tPrec@1 35.938 (35.938)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (32.812)\n",
      "Training epoch: [1605/2000][1/4]; Loss: 2.020090528548943\n",
      "Training Accuracy Epoch: [1604]\tPrec@1 33.594 (33.594)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (30.469)\n",
      "Training epoch: [1606/2000][1/4]; Loss: 2.035235656843631\n",
      "Training Accuracy Epoch: [1605]\tPrec@1 35.938 (35.938)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (31.250)\n",
      "Training epoch: [1607/2000][1/4]; Loss: 2.029699332872821\n",
      "Training Accuracy Epoch: [1606]\tPrec@1 35.156 (35.156)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.031)\n",
      "Training epoch: [1608/2000][1/4]; Loss: 2.072951745252265\n",
      "Training Accuracy Epoch: [1607]\tPrec@1 28.906 (28.906)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.031)\n",
      "Training epoch: [1609/2000][1/4]; Loss: 2.0420921349185983\n",
      "Training Accuracy Epoch: [1608]\tPrec@1 38.281 (38.281)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (31.250)\n",
      "Training epoch: [1610/2000][1/4]; Loss: 2.0408191358506826\n",
      "Training Accuracy Epoch: [1609]\tPrec@1 35.938 (35.938)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (27.344)\n",
      "Training epoch: [1611/2000][1/4]; Loss: 2.0376062279564793\n",
      "Training Accuracy Epoch: [1610]\tPrec@1 40.625 (40.625)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.812)\n",
      "Training epoch: [1612/2000][1/4]; Loss: 2.0819773401863246\n",
      "Training Accuracy Epoch: [1611]\tPrec@1 35.156 (35.156)\tPrec@2 50.781 (50.781)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>>>\tPrec@1 (18.750)\tPrec@2 (31.250)\n",
      "Training epoch: [1613/2000][1/4]; Loss: 2.0725863962062965\n",
      "Training Accuracy Epoch: [1612]\tPrec@1 30.469 (30.469)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.906)\n",
      "Training epoch: [1614/2000][1/4]; Loss: 2.011535853046911\n",
      "Training Accuracy Epoch: [1613]\tPrec@1 41.406 (41.406)\tPrec@2 57.031 (57.031)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (32.812)\n",
      "Training epoch: [1615/2000][1/4]; Loss: 2.056177232871578\n",
      "Training Accuracy Epoch: [1614]\tPrec@1 34.375 (34.375)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (28.906)\n",
      "Training epoch: [1616/2000][1/4]; Loss: 2.081335878516849\n",
      "Training Accuracy Epoch: [1615]\tPrec@1 26.562 (26.562)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.031)\n",
      "Training epoch: [1617/2000][1/4]; Loss: 2.0135945822970855\n",
      "Training Accuracy Epoch: [1616]\tPrec@1 38.281 (38.281)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (31.250)\n",
      "Training epoch: [1618/2000][1/4]; Loss: 2.0643064814453207\n",
      "Training Accuracy Epoch: [1617]\tPrec@1 30.469 (30.469)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (25.000)\tPrec@2 (33.594)\n",
      "Training epoch: [1619/2000][1/4]; Loss: 2.051899185542256\n",
      "Training Accuracy Epoch: [1618]\tPrec@1 34.375 (34.375)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (29.688)\n",
      "Training epoch: [1620/2000][1/4]; Loss: 2.0095462538227404\n",
      "Training Accuracy Epoch: [1619]\tPrec@1 37.500 (37.500)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (33.594)\n",
      "Training epoch: [1621/2000][1/4]; Loss: 2.0332002391586768\n",
      "Training Accuracy Epoch: [1620]\tPrec@1 38.281 (38.281)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (34.375)\n",
      "Training epoch: [1622/2000][1/4]; Loss: 2.03609405474464\n",
      "Training Accuracy Epoch: [1621]\tPrec@1 35.938 (35.938)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (35.156)\n",
      "Training epoch: [1623/2000][1/4]; Loss: 2.0047436461710513\n",
      "Training Accuracy Epoch: [1622]\tPrec@1 39.062 (39.062)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.031)\n",
      "Training epoch: [1624/2000][1/4]; Loss: 2.092529744512345\n",
      "Training Accuracy Epoch: [1623]\tPrec@1 33.594 (33.594)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (32.812)\n",
      "Training epoch: [1625/2000][1/4]; Loss: 2.091202278497737\n",
      "Training Accuracy Epoch: [1624]\tPrec@1 30.469 (30.469)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (33.594)\n",
      "Training epoch: [1626/2000][1/4]; Loss: 2.142044314321606\n",
      "Training Accuracy Epoch: [1625]\tPrec@1 25.781 (25.781)\tPrec@2 38.281 (38.281)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (28.906)\n",
      "Training epoch: [1627/2000][1/4]; Loss: 2.046659406070386\n",
      "Training Accuracy Epoch: [1626]\tPrec@1 30.469 (30.469)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.031)\n",
      "Training epoch: [1628/2000][1/4]; Loss: 1.9726965347957124\n",
      "Training Accuracy Epoch: [1627]\tPrec@1 40.625 (40.625)\tPrec@2 66.406 (66.406)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (29.688)\n",
      "Training epoch: [1629/2000][1/4]; Loss: 2.0614366014118413\n",
      "Training Accuracy Epoch: [1628]\tPrec@1 29.688 (29.688)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (32.031)\n",
      "Training epoch: [1630/2000][1/4]; Loss: 2.0537986031009883\n",
      "Training Accuracy Epoch: [1629]\tPrec@1 32.812 (32.812)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (28.906)\n",
      "Training epoch: [1631/2000][1/4]; Loss: 1.9987221308047352\n",
      "Training Accuracy Epoch: [1630]\tPrec@1 41.406 (41.406)\tPrec@2 57.031 (57.031)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (28.906)\n",
      "Training epoch: [1632/2000][1/4]; Loss: 2.055584743200335\n",
      "Training Accuracy Epoch: [1631]\tPrec@1 34.375 (34.375)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (30.469)\n",
      "Training epoch: [1633/2000][1/4]; Loss: 2.0685189330639724\n",
      "Training Accuracy Epoch: [1632]\tPrec@1 35.156 (35.156)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (36.719)\n",
      "Training epoch: [1634/2000][1/4]; Loss: 2.070137717271997\n",
      "Training Accuracy Epoch: [1633]\tPrec@1 35.938 (35.938)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (31.250)\n",
      "Training epoch: [1635/2000][1/4]; Loss: 2.077475126266733\n",
      "Training Accuracy Epoch: [1634]\tPrec@1 32.812 (32.812)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (32.031)\n",
      "Training epoch: [1636/2000][1/4]; Loss: 2.001149461889621\n",
      "Training Accuracy Epoch: [1635]\tPrec@1 38.281 (38.281)\tPrec@2 64.062 (64.062)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (33.594)\n",
      "Training epoch: [1637/2000][1/4]; Loss: 2.0622162683091467\n",
      "Training Accuracy Epoch: [1636]\tPrec@1 34.375 (34.375)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (29.688)\n",
      "Training epoch: [1638/2000][1/4]; Loss: 1.9872862047342406\n",
      "Training Accuracy Epoch: [1637]\tPrec@1 43.750 (43.750)\tPrec@2 60.156 (60.156)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (30.469)\n",
      "Training epoch: [1639/2000][1/4]; Loss: 2.0445727512130585\n",
      "Training Accuracy Epoch: [1638]\tPrec@1 35.156 (35.156)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (32.031)\n",
      "Training epoch: [1640/2000][1/4]; Loss: 2.071215109013037\n",
      "Training Accuracy Epoch: [1639]\tPrec@1 28.906 (28.906)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (31.250)\n",
      "Training epoch: [1641/2000][1/4]; Loss: 1.9779632243810277\n",
      "Training Accuracy Epoch: [1640]\tPrec@1 42.969 (42.969)\tPrec@2 59.375 (59.375)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (31.250)\n",
      "Training epoch: [1642/2000][1/4]; Loss: 2.0839954778896677\n",
      "Training Accuracy Epoch: [1641]\tPrec@1 28.906 (28.906)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.906)\n",
      "Training epoch: [1643/2000][1/4]; Loss: 2.085089577850183\n",
      "Training Accuracy Epoch: [1642]\tPrec@1 30.469 (30.469)\tPrec@2 38.281 (38.281)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (29.688)\n",
      "Training epoch: [1644/2000][1/4]; Loss: 2.0806818910322957\n",
      "Training Accuracy Epoch: [1643]\tPrec@1 30.469 (30.469)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (32.031)\n",
      "Training epoch: [1645/2000][1/4]; Loss: 2.0079073430214542\n",
      "Training Accuracy Epoch: [1644]\tPrec@1 37.500 (37.500)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (32.031)\n",
      "Training epoch: [1646/2000][1/4]; Loss: 1.9956041950749857\n",
      "Training Accuracy Epoch: [1645]\tPrec@1 38.281 (38.281)\tPrec@2 58.594 (58.594)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (28.906)\n",
      "Training epoch: [1647/2000][1/4]; Loss: 2.013876740767149\n",
      "Training Accuracy Epoch: [1646]\tPrec@1 38.281 (38.281)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (32.812)\n",
      "Training epoch: [1648/2000][1/4]; Loss: 2.0156253795793226\n",
      "Training Accuracy Epoch: [1647]\tPrec@1 38.281 (38.281)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (24.219)\tPrec@2 (37.500)\n",
      "Training epoch: [1649/2000][1/4]; Loss: 2.0445252683048873\n",
      "Training Accuracy Epoch: [1648]\tPrec@1 34.375 (34.375)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (30.469)\n",
      "Training epoch: [1650/2000][1/4]; Loss: 2.0327000290335375\n",
      "Training Accuracy Epoch: [1649]\tPrec@1 37.500 (37.500)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (30.469)\n",
      "Training epoch: [1651/2000][1/4]; Loss: 2.122636919782729\n",
      "Training Accuracy Epoch: [1650]\tPrec@1 26.562 (26.562)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (32.031)\n",
      "Training epoch: [1652/2000][1/4]; Loss: 2.053049075369771\n",
      "Training Accuracy Epoch: [1651]\tPrec@1 37.500 (37.500)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (31.250)\n",
      "Training epoch: [1653/2000][1/4]; Loss: 1.9987761812601612\n",
      "Training Accuracy Epoch: [1652]\tPrec@1 37.500 (37.500)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (35.938)\n",
      "Training epoch: [1654/2000][1/4]; Loss: 2.0507283853349887\n",
      "Training Accuracy Epoch: [1653]\tPrec@1 34.375 (34.375)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (31.250)\n",
      "Training epoch: [1655/2000][1/4]; Loss: 2.0693864407207205\n",
      "Training Accuracy Epoch: [1654]\tPrec@1 32.031 (32.031)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (29.688)\n",
      "Training epoch: [1656/2000][1/4]; Loss: 2.0462707535155404\n",
      "Training Accuracy Epoch: [1655]\tPrec@1 33.594 (33.594)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (30.469)\n",
      "Training epoch: [1657/2000][1/4]; Loss: 2.0913160337873307\n",
      "Training Accuracy Epoch: [1656]\tPrec@1 31.250 (31.250)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (29.688)\n",
      "Training epoch: [1658/2000][1/4]; Loss: 2.0518706600997\n",
      "Training Accuracy Epoch: [1657]\tPrec@1 35.156 (35.156)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (30.469)\n",
      "Training epoch: [1659/2000][1/4]; Loss: 1.9919450096887972\n",
      "Training Accuracy Epoch: [1658]\tPrec@1 41.406 (41.406)\tPrec@2 60.156 (60.156)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>>>\tPrec@1 (21.094)\tPrec@2 (29.688)\n",
      "Training epoch: [1660/2000][1/4]; Loss: 2.0703048806170723\n",
      "Training Accuracy Epoch: [1659]\tPrec@1 36.719 (36.719)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (34.375)\n",
      "Training epoch: [1661/2000][1/4]; Loss: 2.0402334715403203\n",
      "Training Accuracy Epoch: [1660]\tPrec@1 37.500 (37.500)\tPrec@2 56.250 (56.250)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (29.688)\n",
      "Training epoch: [1662/2000][1/4]; Loss: 2.0463240245061463\n",
      "Training Accuracy Epoch: [1661]\tPrec@1 35.156 (35.156)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.812)\n",
      "Training epoch: [1663/2000][1/4]; Loss: 2.042672733895825\n",
      "Training Accuracy Epoch: [1662]\tPrec@1 35.156 (35.156)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.812)\n",
      "Training epoch: [1664/2000][1/4]; Loss: 2.012180929337698\n",
      "Training Accuracy Epoch: [1663]\tPrec@1 37.500 (37.500)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (14.062)\tPrec@2 (28.125)\n",
      "Training epoch: [1665/2000][1/4]; Loss: 2.0858520912096825\n",
      "Training Accuracy Epoch: [1664]\tPrec@1 34.375 (34.375)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (29.688)\n",
      "Training epoch: [1666/2000][1/4]; Loss: 2.086398501001652\n",
      "Training Accuracy Epoch: [1665]\tPrec@1 29.688 (29.688)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (30.469)\n",
      "Training epoch: [1667/2000][1/4]; Loss: 2.0143879068261628\n",
      "Training Accuracy Epoch: [1666]\tPrec@1 36.719 (36.719)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (34.375)\n",
      "Training epoch: [1668/2000][1/4]; Loss: 2.053010924446884\n",
      "Training Accuracy Epoch: [1667]\tPrec@1 35.156 (35.156)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (33.594)\n",
      "Training epoch: [1669/2000][1/4]; Loss: 2.043665544816969\n",
      "Training Accuracy Epoch: [1668]\tPrec@1 32.031 (32.031)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (28.125)\n",
      "Training epoch: [1670/2000][1/4]; Loss: 2.0405578113139375\n",
      "Training Accuracy Epoch: [1669]\tPrec@1 35.156 (35.156)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.906)\n",
      "Training epoch: [1671/2000][1/4]; Loss: 2.086047225989382\n",
      "Training Accuracy Epoch: [1670]\tPrec@1 28.125 (28.125)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (32.031)\n",
      "Training epoch: [1672/2000][1/4]; Loss: 2.0354942573503467\n",
      "Training Accuracy Epoch: [1671]\tPrec@1 34.375 (34.375)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (32.812)\n",
      "Training epoch: [1673/2000][1/4]; Loss: 2.0324989569332343\n",
      "Training Accuracy Epoch: [1672]\tPrec@1 37.500 (37.500)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (35.156)\n",
      "Training epoch: [1674/2000][1/4]; Loss: 2.038861637847991\n",
      "Training Accuracy Epoch: [1673]\tPrec@1 36.719 (36.719)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (31.250)\n",
      "Training epoch: [1675/2000][1/4]; Loss: 2.0453485288268713\n",
      "Training Accuracy Epoch: [1674]\tPrec@1 34.375 (34.375)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (27.344)\n",
      "Training epoch: [1676/2000][1/4]; Loss: 2.045030286672322\n",
      "Training Accuracy Epoch: [1675]\tPrec@1 44.531 (44.531)\tPrec@2 57.031 (57.031)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (29.688)\n",
      "Training epoch: [1677/2000][1/4]; Loss: 2.016254638881034\n",
      "Training Accuracy Epoch: [1676]\tPrec@1 42.969 (42.969)\tPrec@2 56.250 (56.250)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (30.469)\n",
      "Training epoch: [1678/2000][1/4]; Loss: 2.0287328502637614\n",
      "Training Accuracy Epoch: [1677]\tPrec@1 38.281 (38.281)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (29.688)\n",
      "Training epoch: [1679/2000][1/4]; Loss: 2.0540429050301094\n",
      "Training Accuracy Epoch: [1678]\tPrec@1 34.375 (34.375)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (31.250)\n",
      "Training epoch: [1680/2000][1/4]; Loss: 2.0801583752895847\n",
      "Training Accuracy Epoch: [1679]\tPrec@1 39.062 (39.062)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (29.688)\n",
      "Training epoch: [1681/2000][1/4]; Loss: 2.061128105951542\n",
      "Training Accuracy Epoch: [1680]\tPrec@1 36.719 (36.719)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.812)\n",
      "Training epoch: [1682/2000][1/4]; Loss: 2.0986447420884833\n",
      "Training Accuracy Epoch: [1681]\tPrec@1 26.562 (26.562)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (32.031)\n",
      "Training epoch: [1683/2000][1/4]; Loss: 2.0360158859329163\n",
      "Training Accuracy Epoch: [1682]\tPrec@1 35.156 (35.156)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (28.906)\n",
      "Training epoch: [1684/2000][1/4]; Loss: 1.995932994728054\n",
      "Training Accuracy Epoch: [1683]\tPrec@1 40.625 (40.625)\tPrec@2 57.812 (57.812)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (29.688)\n",
      "Training epoch: [1685/2000][1/4]; Loss: 2.0227752947894153\n",
      "Training Accuracy Epoch: [1684]\tPrec@1 39.062 (39.062)\tPrec@2 56.250 (56.250)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (32.812)\n",
      "Training epoch: [1686/2000][1/4]; Loss: 2.038778855498931\n",
      "Training Accuracy Epoch: [1685]\tPrec@1 42.188 (42.188)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (29.688)\n",
      "Training epoch: [1687/2000][1/4]; Loss: 2.101911620532848\n",
      "Training Accuracy Epoch: [1686]\tPrec@1 30.469 (30.469)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.031)\n",
      "Training epoch: [1688/2000][1/4]; Loss: 2.076850664224787\n",
      "Training Accuracy Epoch: [1687]\tPrec@1 36.719 (36.719)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (35.938)\n",
      "Training epoch: [1689/2000][1/4]; Loss: 2.0425374871879685\n",
      "Training Accuracy Epoch: [1688]\tPrec@1 32.031 (32.031)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (35.938)\n",
      "Training epoch: [1690/2000][1/4]; Loss: 2.025303176778709\n",
      "Training Accuracy Epoch: [1689]\tPrec@1 36.719 (36.719)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (29.688)\n",
      "Training epoch: [1691/2000][1/4]; Loss: 2.0474293933038137\n",
      "Training Accuracy Epoch: [1690]\tPrec@1 27.344 (27.344)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (34.375)\n",
      "Training epoch: [1692/2000][1/4]; Loss: 2.0332238947193\n",
      "Training Accuracy Epoch: [1691]\tPrec@1 39.844 (39.844)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (28.906)\n",
      "Training epoch: [1693/2000][1/4]; Loss: 1.986312367108628\n",
      "Training Accuracy Epoch: [1692]\tPrec@1 41.406 (41.406)\tPrec@2 60.156 (60.156)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (34.375)\n",
      "Training epoch: [1694/2000][1/4]; Loss: 2.0588529300980483\n",
      "Training Accuracy Epoch: [1693]\tPrec@1 34.375 (34.375)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.812)\n",
      "Training epoch: [1695/2000][1/4]; Loss: 2.0176138369840926\n",
      "Training Accuracy Epoch: [1694]\tPrec@1 34.375 (34.375)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (32.031)\n",
      "Training epoch: [1696/2000][1/4]; Loss: 2.0993881125463845\n",
      "Training Accuracy Epoch: [1695]\tPrec@1 29.688 (29.688)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.906)\n",
      "Training epoch: [1697/2000][1/4]; Loss: 2.0206882564556397\n",
      "Training Accuracy Epoch: [1696]\tPrec@1 36.719 (36.719)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (30.469)\n",
      "Training epoch: [1698/2000][1/4]; Loss: 2.026626716250826\n",
      "Training Accuracy Epoch: [1697]\tPrec@1 41.406 (41.406)\tPrec@2 57.031 (57.031)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (35.156)\n",
      "Training epoch: [1699/2000][1/4]; Loss: 2.030276640281712\n",
      "Training Accuracy Epoch: [1698]\tPrec@1 39.844 (39.844)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (31.250)\n",
      "Training epoch: [1700/2000][1/4]; Loss: 2.030153615545384\n",
      "Training Accuracy Epoch: [1699]\tPrec@1 35.938 (35.938)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (32.812)\n",
      "Training epoch: [1701/2000][1/4]; Loss: 2.0729209189249707\n",
      "Training Accuracy Epoch: [1700]\tPrec@1 32.031 (32.031)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.031)\n",
      "Training epoch: [1702/2000][1/4]; Loss: 1.988893580481498\n",
      "Training Accuracy Epoch: [1701]\tPrec@1 43.750 (43.750)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (34.375)\n",
      "Training epoch: [1703/2000][1/4]; Loss: 2.0292946413965174\n",
      "Training Accuracy Epoch: [1702]\tPrec@1 35.938 (35.938)\tPrec@2 57.031 (57.031)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (31.250)\n",
      "Training epoch: [1704/2000][1/4]; Loss: 2.0396002124286867\n",
      "Training Accuracy Epoch: [1703]\tPrec@1 35.156 (35.156)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (35.938)\n",
      "Training epoch: [1705/2000][1/4]; Loss: 2.1390686440774695\n",
      "Training Accuracy Epoch: [1704]\tPrec@1 30.469 (30.469)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (32.812)\n",
      "Training epoch: [1706/2000][1/4]; Loss: 1.9991165159263433\n",
      "Training Accuracy Epoch: [1705]\tPrec@1 37.500 (37.500)\tPrec@2 53.906 (53.906)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>>>\tPrec@1 (16.406)\tPrec@2 (29.688)\n",
      "Training epoch: [1707/2000][1/4]; Loss: 2.0374533702351605\n",
      "Training Accuracy Epoch: [1706]\tPrec@1 36.719 (36.719)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (31.250)\n",
      "Training epoch: [1708/2000][1/4]; Loss: 2.0355880341677177\n",
      "Training Accuracy Epoch: [1707]\tPrec@1 33.594 (33.594)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (32.031)\n",
      "Training epoch: [1709/2000][1/4]; Loss: 2.029374404469639\n",
      "Training Accuracy Epoch: [1708]\tPrec@1 32.812 (32.812)\tPrec@2 57.031 (57.031)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (32.031)\n",
      "Training epoch: [1710/2000][1/4]; Loss: 2.062818736766806\n",
      "Training Accuracy Epoch: [1709]\tPrec@1 32.031 (32.031)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (31.250)\n",
      "Training epoch: [1711/2000][1/4]; Loss: 2.0800095807157364\n",
      "Training Accuracy Epoch: [1710]\tPrec@1 33.594 (33.594)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (34.375)\n",
      "Training epoch: [1712/2000][1/4]; Loss: 2.0600872505391536\n",
      "Training Accuracy Epoch: [1711]\tPrec@1 30.469 (30.469)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (32.812)\n",
      "Training epoch: [1713/2000][1/4]; Loss: 2.08468096580123\n",
      "Training Accuracy Epoch: [1712]\tPrec@1 31.250 (31.250)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (34.375)\n",
      "Training epoch: [1714/2000][1/4]; Loss: 2.0111421676880274\n",
      "Training Accuracy Epoch: [1713]\tPrec@1 43.750 (43.750)\tPrec@2 62.500 (62.500)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (36.719)\n",
      "Training epoch: [1715/2000][1/4]; Loss: 2.036052097301016\n",
      "Training Accuracy Epoch: [1714]\tPrec@1 37.500 (37.500)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (31.250)\n",
      "Training epoch: [1716/2000][1/4]; Loss: 2.0706603120067104\n",
      "Training Accuracy Epoch: [1715]\tPrec@1 29.688 (29.688)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (31.250)\n",
      "Training epoch: [1717/2000][1/4]; Loss: 2.0095970910732457\n",
      "Training Accuracy Epoch: [1716]\tPrec@1 39.844 (39.844)\tPrec@2 57.031 (57.031)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (32.031)\n",
      "Training epoch: [1718/2000][1/4]; Loss: 2.0736828057522487\n",
      "Training Accuracy Epoch: [1717]\tPrec@1 28.906 (28.906)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (35.156)\n",
      "Training epoch: [1719/2000][1/4]; Loss: 1.9916480173692033\n",
      "Training Accuracy Epoch: [1718]\tPrec@1 40.625 (40.625)\tPrec@2 56.250 (56.250)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (30.469)\n",
      "Training epoch: [1720/2000][1/4]; Loss: 2.031932559018887\n",
      "Training Accuracy Epoch: [1719]\tPrec@1 40.625 (40.625)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (30.469)\n",
      "Training epoch: [1721/2000][1/4]; Loss: 2.0703896112927107\n",
      "Training Accuracy Epoch: [1720]\tPrec@1 28.906 (28.906)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (33.594)\n",
      "Training epoch: [1722/2000][1/4]; Loss: 2.039884163507285\n",
      "Training Accuracy Epoch: [1721]\tPrec@1 38.281 (38.281)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (30.469)\n",
      "Training epoch: [1723/2000][1/4]; Loss: 2.0461779327963496\n",
      "Training Accuracy Epoch: [1722]\tPrec@1 39.062 (39.062)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (28.125)\n",
      "Training epoch: [1724/2000][1/4]; Loss: 2.0569495250559506\n",
      "Training Accuracy Epoch: [1723]\tPrec@1 37.500 (37.500)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (30.469)\n",
      "Training epoch: [1725/2000][1/4]; Loss: 2.034386090985884\n",
      "Training Accuracy Epoch: [1724]\tPrec@1 35.156 (35.156)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (28.125)\n",
      "Training epoch: [1726/2000][1/4]; Loss: 2.071455905076853\n",
      "Training Accuracy Epoch: [1725]\tPrec@1 28.906 (28.906)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (34.375)\n",
      "Training epoch: [1727/2000][1/4]; Loss: 2.1817764464940006\n",
      "Training Accuracy Epoch: [1726]\tPrec@1 23.438 (23.438)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (34.375)\n",
      "Training epoch: [1728/2000][1/4]; Loss: 2.0583648433262343\n",
      "Training Accuracy Epoch: [1727]\tPrec@1 35.156 (35.156)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (35.156)\n",
      "Training epoch: [1729/2000][1/4]; Loss: 2.0353799478484524\n",
      "Training Accuracy Epoch: [1728]\tPrec@1 39.062 (39.062)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (31.250)\n",
      "Training epoch: [1730/2000][1/4]; Loss: 2.0509679946666353\n",
      "Training Accuracy Epoch: [1729]\tPrec@1 35.156 (35.156)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.812)\n",
      "Training epoch: [1731/2000][1/4]; Loss: 2.0288284726318224\n",
      "Training Accuracy Epoch: [1730]\tPrec@1 33.594 (33.594)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (34.375)\n",
      "Training epoch: [1732/2000][1/4]; Loss: 2.0483642464754817\n",
      "Training Accuracy Epoch: [1731]\tPrec@1 35.938 (35.938)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (34.375)\n",
      "Training epoch: [1733/2000][1/4]; Loss: 1.982393778597501\n",
      "Training Accuracy Epoch: [1732]\tPrec@1 39.844 (39.844)\tPrec@2 61.719 (61.719)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (32.031)\n",
      "Training epoch: [1734/2000][1/4]; Loss: 2.0666216417944976\n",
      "Training Accuracy Epoch: [1733]\tPrec@1 34.375 (34.375)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (32.031)\n",
      "Training epoch: [1735/2000][1/4]; Loss: 2.119601231388205\n",
      "Training Accuracy Epoch: [1734]\tPrec@1 26.562 (26.562)\tPrec@2 38.281 (38.281)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (32.031)\n",
      "Training epoch: [1736/2000][1/4]; Loss: 2.1000305000321626\n",
      "Training Accuracy Epoch: [1735]\tPrec@1 27.344 (27.344)\tPrec@2 39.062 (39.062)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (30.469)\n",
      "Training epoch: [1737/2000][1/4]; Loss: 2.0599555751359646\n",
      "Training Accuracy Epoch: [1736]\tPrec@1 32.812 (32.812)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (36.719)\n",
      "Training epoch: [1738/2000][1/4]; Loss: 2.0960938474525284\n",
      "Training Accuracy Epoch: [1737]\tPrec@1 28.125 (28.125)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (36.719)\n",
      "Training epoch: [1739/2000][1/4]; Loss: 2.059597963149493\n",
      "Training Accuracy Epoch: [1738]\tPrec@1 37.500 (37.500)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (28.906)\n",
      "Training epoch: [1740/2000][1/4]; Loss: 2.0138571838561217\n",
      "Training Accuracy Epoch: [1739]\tPrec@1 35.938 (35.938)\tPrec@2 56.250 (56.250)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (32.031)\n",
      "Training epoch: [1741/2000][1/4]; Loss: 2.024719507100111\n",
      "Training Accuracy Epoch: [1740]\tPrec@1 39.062 (39.062)\tPrec@2 57.812 (57.812)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (28.125)\n",
      "Training epoch: [1742/2000][1/4]; Loss: 2.0241046382922914\n",
      "Training Accuracy Epoch: [1741]\tPrec@1 42.969 (42.969)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (30.469)\n",
      "Training epoch: [1743/2000][1/4]; Loss: 1.9942964318313119\n",
      "Training Accuracy Epoch: [1742]\tPrec@1 42.969 (42.969)\tPrec@2 60.156 (60.156)\n",
      "===>>>\tPrec@1 (14.062)\tPrec@2 (25.781)\n",
      "Training epoch: [1744/2000][1/4]; Loss: 2.102533558896669\n",
      "Training Accuracy Epoch: [1743]\tPrec@1 30.469 (30.469)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (32.031)\n",
      "Training epoch: [1745/2000][1/4]; Loss: 2.032474476695749\n",
      "Training Accuracy Epoch: [1744]\tPrec@1 39.062 (39.062)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (28.906)\n",
      "Training epoch: [1746/2000][1/4]; Loss: 2.041932363797922\n",
      "Training Accuracy Epoch: [1745]\tPrec@1 36.719 (36.719)\tPrec@2 57.031 (57.031)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (29.688)\n",
      "Training epoch: [1747/2000][1/4]; Loss: 2.1073987714432634\n",
      "Training Accuracy Epoch: [1746]\tPrec@1 32.031 (32.031)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (27.344)\n",
      "Training epoch: [1748/2000][1/4]; Loss: 1.9954308870923418\n",
      "Training Accuracy Epoch: [1747]\tPrec@1 39.062 (39.062)\tPrec@2 59.375 (59.375)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (28.125)\n",
      "Training epoch: [1749/2000][1/4]; Loss: 2.0223198149345203\n",
      "Training Accuracy Epoch: [1748]\tPrec@1 33.594 (33.594)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.031)\n",
      "Training epoch: [1750/2000][1/4]; Loss: 2.050704635774327\n",
      "Training Accuracy Epoch: [1749]\tPrec@1 35.156 (35.156)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (33.594)\n",
      "Training epoch: [1751/2000][1/4]; Loss: 2.0324849162764416\n",
      "Training Accuracy Epoch: [1750]\tPrec@1 39.844 (39.844)\tPrec@2 58.594 (58.594)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (30.469)\n",
      "Training epoch: [1752/2000][1/4]; Loss: 2.0830741976693252\n",
      "Training Accuracy Epoch: [1751]\tPrec@1 33.594 (33.594)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.031)\n",
      "Training epoch: [1753/2000][1/4]; Loss: 1.999359293576918\n",
      "Training Accuracy Epoch: [1752]\tPrec@1 40.625 (40.625)\tPrec@2 57.031 (57.031)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>>>\tPrec@1 (21.094)\tPrec@2 (35.156)\n",
      "Training epoch: [1754/2000][1/4]; Loss: 2.0270773987589465\n",
      "Training Accuracy Epoch: [1753]\tPrec@1 34.375 (34.375)\tPrec@2 58.594 (58.594)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (28.125)\n",
      "Training epoch: [1755/2000][1/4]; Loss: 2.028548945508659\n",
      "Training Accuracy Epoch: [1754]\tPrec@1 37.500 (37.500)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (27.344)\n",
      "Training epoch: [1756/2000][1/4]; Loss: 2.077509524442603\n",
      "Training Accuracy Epoch: [1755]\tPrec@1 28.125 (28.125)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.906)\n",
      "Training epoch: [1757/2000][1/4]; Loss: 2.075903087992382\n",
      "Training Accuracy Epoch: [1756]\tPrec@1 28.906 (28.906)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (29.688)\n",
      "Training epoch: [1758/2000][1/4]; Loss: 2.0516736566077145\n",
      "Training Accuracy Epoch: [1757]\tPrec@1 32.812 (32.812)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (34.375)\n",
      "Training epoch: [1759/2000][1/4]; Loss: 2.0443602707037125\n",
      "Training Accuracy Epoch: [1758]\tPrec@1 35.938 (35.938)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.906)\n",
      "Training epoch: [1760/2000][1/4]; Loss: 2.0659219477212467\n",
      "Training Accuracy Epoch: [1759]\tPrec@1 33.594 (33.594)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (34.375)\n",
      "Training epoch: [1761/2000][1/4]; Loss: 2.01535002214876\n",
      "Training Accuracy Epoch: [1760]\tPrec@1 38.281 (38.281)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (36.719)\n",
      "Training epoch: [1762/2000][1/4]; Loss: 2.024469997935558\n",
      "Training Accuracy Epoch: [1761]\tPrec@1 36.719 (36.719)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.812)\n",
      "Training epoch: [1763/2000][1/4]; Loss: 2.085019503946607\n",
      "Training Accuracy Epoch: [1762]\tPrec@1 30.469 (30.469)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.906)\n",
      "Training epoch: [1764/2000][1/4]; Loss: 2.0334425651136767\n",
      "Training Accuracy Epoch: [1763]\tPrec@1 31.250 (31.250)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (27.344)\n",
      "Training epoch: [1765/2000][1/4]; Loss: 1.9877240578675262\n",
      "Training Accuracy Epoch: [1764]\tPrec@1 42.188 (42.188)\tPrec@2 57.031 (57.031)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (27.344)\n",
      "Training epoch: [1766/2000][1/4]; Loss: 2.0440240094809283\n",
      "Training Accuracy Epoch: [1765]\tPrec@1 39.844 (39.844)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.031)\n",
      "Training epoch: [1767/2000][1/4]; Loss: 2.056509054432466\n",
      "Training Accuracy Epoch: [1766]\tPrec@1 30.469 (30.469)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (34.375)\n",
      "Training epoch: [1768/2000][1/4]; Loss: 2.0605761361282173\n",
      "Training Accuracy Epoch: [1767]\tPrec@1 28.906 (28.906)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (32.031)\n",
      "Training epoch: [1769/2000][1/4]; Loss: 2.068537046241957\n",
      "Training Accuracy Epoch: [1768]\tPrec@1 28.906 (28.906)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (31.250)\n",
      "Training epoch: [1770/2000][1/4]; Loss: 2.0363640858820258\n",
      "Training Accuracy Epoch: [1769]\tPrec@1 37.500 (37.500)\tPrec@2 59.375 (59.375)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (33.594)\n",
      "Training epoch: [1771/2000][1/4]; Loss: 1.9823666478418938\n",
      "Training Accuracy Epoch: [1770]\tPrec@1 42.188 (42.188)\tPrec@2 57.812 (57.812)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (32.812)\n",
      "Training epoch: [1772/2000][1/4]; Loss: 2.0639510593662393\n",
      "Training Accuracy Epoch: [1771]\tPrec@1 37.500 (37.500)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (35.156)\n",
      "Training epoch: [1773/2000][1/4]; Loss: 2.064625119636256\n",
      "Training Accuracy Epoch: [1772]\tPrec@1 33.594 (33.594)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (31.250)\n",
      "Training epoch: [1774/2000][1/4]; Loss: 2.0327095522858074\n",
      "Training Accuracy Epoch: [1773]\tPrec@1 38.281 (38.281)\tPrec@2 56.250 (56.250)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (30.469)\n",
      "Training epoch: [1775/2000][1/4]; Loss: 2.0380352231886145\n",
      "Training Accuracy Epoch: [1774]\tPrec@1 35.156 (35.156)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.125)\n",
      "Training epoch: [1776/2000][1/4]; Loss: 2.021509142838286\n",
      "Training Accuracy Epoch: [1775]\tPrec@1 39.062 (39.062)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (36.719)\n",
      "Training epoch: [1777/2000][1/4]; Loss: 2.0867429847832417\n",
      "Training Accuracy Epoch: [1776]\tPrec@1 31.250 (31.250)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (14.062)\tPrec@2 (28.125)\n",
      "Training epoch: [1778/2000][1/4]; Loss: 2.0230427973037326\n",
      "Training Accuracy Epoch: [1777]\tPrec@1 36.719 (36.719)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (26.562)\n",
      "Training epoch: [1779/2000][1/4]; Loss: 2.069313302498799\n",
      "Training Accuracy Epoch: [1778]\tPrec@1 36.719 (36.719)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (32.031)\n",
      "Training epoch: [1780/2000][1/4]; Loss: 2.0944601150332915\n",
      "Training Accuracy Epoch: [1779]\tPrec@1 27.344 (27.344)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (30.469)\n",
      "Training epoch: [1781/2000][1/4]; Loss: 2.1038763345414733\n",
      "Training Accuracy Epoch: [1780]\tPrec@1 34.375 (34.375)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (29.688)\n",
      "Training epoch: [1782/2000][1/4]; Loss: 2.029955754597843\n",
      "Training Accuracy Epoch: [1781]\tPrec@1 35.156 (35.156)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (31.250)\n",
      "Training epoch: [1783/2000][1/4]; Loss: 2.0655777298917006\n",
      "Training Accuracy Epoch: [1782]\tPrec@1 36.719 (36.719)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.906)\n",
      "Training epoch: [1784/2000][1/4]; Loss: 2.0584004879702067\n",
      "Training Accuracy Epoch: [1783]\tPrec@1 32.031 (32.031)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (28.906)\n",
      "Training epoch: [1785/2000][1/4]; Loss: 2.0511287225439863\n",
      "Training Accuracy Epoch: [1784]\tPrec@1 33.594 (33.594)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (29.688)\n",
      "Training epoch: [1786/2000][1/4]; Loss: 2.0949980376599227\n",
      "Training Accuracy Epoch: [1785]\tPrec@1 28.906 (28.906)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (31.250)\n",
      "Training epoch: [1787/2000][1/4]; Loss: 2.0607568823574116\n",
      "Training Accuracy Epoch: [1786]\tPrec@1 33.594 (33.594)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (31.250)\n",
      "Training epoch: [1788/2000][1/4]; Loss: 2.1036579884563937\n",
      "Training Accuracy Epoch: [1787]\tPrec@1 28.125 (28.125)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (31.250)\n",
      "Training epoch: [1789/2000][1/4]; Loss: 2.090814024570776\n",
      "Training Accuracy Epoch: [1788]\tPrec@1 23.438 (23.438)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (33.594)\n",
      "Training epoch: [1790/2000][1/4]; Loss: 2.084943169684981\n",
      "Training Accuracy Epoch: [1789]\tPrec@1 27.344 (27.344)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (31.250)\n",
      "Training epoch: [1791/2000][1/4]; Loss: 2.012609426134592\n",
      "Training Accuracy Epoch: [1790]\tPrec@1 41.406 (41.406)\tPrec@2 60.156 (60.156)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (32.812)\n",
      "Training epoch: [1792/2000][1/4]; Loss: 2.0499930375314435\n",
      "Training Accuracy Epoch: [1791]\tPrec@1 34.375 (34.375)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (25.781)\n",
      "Training epoch: [1793/2000][1/4]; Loss: 2.099402254724232\n",
      "Training Accuracy Epoch: [1792]\tPrec@1 27.344 (27.344)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (30.469)\n",
      "Training epoch: [1794/2000][1/4]; Loss: 2.0612413411866934\n",
      "Training Accuracy Epoch: [1793]\tPrec@1 31.250 (31.250)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (29.688)\n",
      "Training epoch: [1795/2000][1/4]; Loss: 2.029489559054993\n",
      "Training Accuracy Epoch: [1794]\tPrec@1 41.406 (41.406)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.125)\n",
      "Training epoch: [1796/2000][1/4]; Loss: 2.070473191088077\n",
      "Training Accuracy Epoch: [1795]\tPrec@1 36.719 (36.719)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (34.375)\n",
      "Training epoch: [1797/2000][1/4]; Loss: 1.9808430608797762\n",
      "Training Accuracy Epoch: [1796]\tPrec@1 41.406 (41.406)\tPrec@2 56.250 (56.250)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (32.812)\n",
      "Training epoch: [1798/2000][1/4]; Loss: 2.0655123525537897\n",
      "Training Accuracy Epoch: [1797]\tPrec@1 31.250 (31.250)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.031)\n",
      "Training epoch: [1799/2000][1/4]; Loss: 2.0935734938901733\n",
      "Training Accuracy Epoch: [1798]\tPrec@1 26.562 (26.562)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (31.250)\n",
      "Training epoch: [1800/2000][1/4]; Loss: 2.077039453156661\n",
      "Training Accuracy Epoch: [1799]\tPrec@1 33.594 (33.594)\tPrec@2 47.656 (47.656)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>>>\tPrec@1 (17.969)\tPrec@2 (32.812)\n",
      "Training epoch: [1801/2000][1/4]; Loss: 2.0631995579055036\n",
      "Training Accuracy Epoch: [1800]\tPrec@1 33.594 (33.594)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (26.562)\n",
      "Training epoch: [1802/2000][1/4]; Loss: 2.052842701840041\n",
      "Training Accuracy Epoch: [1801]\tPrec@1 35.156 (35.156)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (28.906)\n",
      "Training epoch: [1803/2000][1/4]; Loss: 2.0191720987248507\n",
      "Training Accuracy Epoch: [1802]\tPrec@1 35.938 (35.938)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (30.469)\n",
      "Training epoch: [1804/2000][1/4]; Loss: 2.055014380952948\n",
      "Training Accuracy Epoch: [1803]\tPrec@1 35.156 (35.156)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (29.688)\n",
      "Training epoch: [1805/2000][1/4]; Loss: 2.0270064715639777\n",
      "Training Accuracy Epoch: [1804]\tPrec@1 35.938 (35.938)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.812)\n",
      "Training epoch: [1806/2000][1/4]; Loss: 2.0667141975280385\n",
      "Training Accuracy Epoch: [1805]\tPrec@1 29.688 (29.688)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (32.031)\n",
      "Training epoch: [1807/2000][1/4]; Loss: 2.0371536821285807\n",
      "Training Accuracy Epoch: [1806]\tPrec@1 35.156 (35.156)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (29.688)\n",
      "Training epoch: [1808/2000][1/4]; Loss: 2.0679635178722857\n",
      "Training Accuracy Epoch: [1807]\tPrec@1 32.031 (32.031)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (29.688)\n",
      "Training epoch: [1809/2000][1/4]; Loss: 2.0519883062500783\n",
      "Training Accuracy Epoch: [1808]\tPrec@1 34.375 (34.375)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (30.469)\n",
      "Training epoch: [1810/2000][1/4]; Loss: 2.07061654302668\n",
      "Training Accuracy Epoch: [1809]\tPrec@1 25.781 (25.781)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (33.594)\n",
      "Training epoch: [1811/2000][1/4]; Loss: 2.0168774343267803\n",
      "Training Accuracy Epoch: [1810]\tPrec@1 40.625 (40.625)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (31.250)\n",
      "Training epoch: [1812/2000][1/4]; Loss: 2.071374787512308\n",
      "Training Accuracy Epoch: [1811]\tPrec@1 32.812 (32.812)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (25.000)\n",
      "Training epoch: [1813/2000][1/4]; Loss: 2.017834424386148\n",
      "Training Accuracy Epoch: [1812]\tPrec@1 33.594 (33.594)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (32.812)\n",
      "Training epoch: [1814/2000][1/4]; Loss: 2.0272752703989463\n",
      "Training Accuracy Epoch: [1813]\tPrec@1 36.719 (36.719)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (31.250)\n",
      "Training epoch: [1815/2000][1/4]; Loss: 2.0157394971241986\n",
      "Training Accuracy Epoch: [1814]\tPrec@1 35.156 (35.156)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (34.375)\n",
      "Training epoch: [1816/2000][1/4]; Loss: 2.017920305800993\n",
      "Training Accuracy Epoch: [1815]\tPrec@1 39.844 (39.844)\tPrec@2 57.031 (57.031)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (33.594)\n",
      "Training epoch: [1817/2000][1/4]; Loss: 2.0590432255347797\n",
      "Training Accuracy Epoch: [1816]\tPrec@1 32.812 (32.812)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (31.250)\n",
      "Training epoch: [1818/2000][1/4]; Loss: 2.1084860207731024\n",
      "Training Accuracy Epoch: [1817]\tPrec@1 28.906 (28.906)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (32.812)\n",
      "Training epoch: [1819/2000][1/4]; Loss: 2.0441260049178833\n",
      "Training Accuracy Epoch: [1818]\tPrec@1 35.156 (35.156)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (35.156)\n",
      "Training epoch: [1820/2000][1/4]; Loss: 2.0056009398457673\n",
      "Training Accuracy Epoch: [1819]\tPrec@1 37.500 (37.500)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (33.594)\n",
      "Training epoch: [1821/2000][1/4]; Loss: 2.013405172420911\n",
      "Training Accuracy Epoch: [1820]\tPrec@1 38.281 (38.281)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (31.250)\n",
      "Training epoch: [1822/2000][1/4]; Loss: 2.0946139283323837\n",
      "Training Accuracy Epoch: [1821]\tPrec@1 35.156 (35.156)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (32.812)\n",
      "Training epoch: [1823/2000][1/4]; Loss: 2.0440750700457837\n",
      "Training Accuracy Epoch: [1822]\tPrec@1 33.594 (33.594)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (28.906)\n",
      "Training epoch: [1824/2000][1/4]; Loss: 2.085118802258602\n",
      "Training Accuracy Epoch: [1823]\tPrec@1 32.812 (32.812)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (30.469)\n",
      "Training epoch: [1825/2000][1/4]; Loss: 2.0610052965053716\n",
      "Training Accuracy Epoch: [1824]\tPrec@1 32.812 (32.812)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (31.250)\n",
      "Training epoch: [1826/2000][1/4]; Loss: 2.0982956319855846\n",
      "Training Accuracy Epoch: [1825]\tPrec@1 31.250 (31.250)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (30.469)\n",
      "Training epoch: [1827/2000][1/4]; Loss: 2.050570374749957\n",
      "Training Accuracy Epoch: [1826]\tPrec@1 39.844 (39.844)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.812)\n",
      "Training epoch: [1828/2000][1/4]; Loss: 2.0379742646085823\n",
      "Training Accuracy Epoch: [1827]\tPrec@1 36.719 (36.719)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (34.375)\n",
      "Training epoch: [1829/2000][1/4]; Loss: 2.0593421242201146\n",
      "Training Accuracy Epoch: [1828]\tPrec@1 31.250 (31.250)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (32.031)\n",
      "Training epoch: [1830/2000][1/4]; Loss: 2.0651011141181836\n",
      "Training Accuracy Epoch: [1829]\tPrec@1 34.375 (34.375)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (35.156)\n",
      "Training epoch: [1831/2000][1/4]; Loss: 2.053067349617093\n",
      "Training Accuracy Epoch: [1830]\tPrec@1 32.031 (32.031)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (34.375)\n",
      "Training epoch: [1832/2000][1/4]; Loss: 2.0058700424237776\n",
      "Training Accuracy Epoch: [1831]\tPrec@1 39.062 (39.062)\tPrec@2 58.594 (58.594)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.906)\n",
      "Training epoch: [1833/2000][1/4]; Loss: 2.074524226678978\n",
      "Training Accuracy Epoch: [1832]\tPrec@1 31.250 (31.250)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.812)\n",
      "Training epoch: [1834/2000][1/4]; Loss: 2.0471864962663844\n",
      "Training Accuracy Epoch: [1833]\tPrec@1 35.156 (35.156)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (32.031)\n",
      "Training epoch: [1835/2000][1/4]; Loss: 2.0205818739154235\n",
      "Training Accuracy Epoch: [1834]\tPrec@1 36.719 (36.719)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (35.938)\n",
      "Training epoch: [1836/2000][1/4]; Loss: 2.0594411226191247\n",
      "Training Accuracy Epoch: [1835]\tPrec@1 30.469 (30.469)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (32.031)\n",
      "Training epoch: [1837/2000][1/4]; Loss: 2.029111072997333\n",
      "Training Accuracy Epoch: [1836]\tPrec@1 37.500 (37.500)\tPrec@2 56.250 (56.250)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (34.375)\n",
      "Training epoch: [1838/2000][1/4]; Loss: 2.0320132781449\n",
      "Training Accuracy Epoch: [1837]\tPrec@1 35.156 (35.156)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (34.375)\n",
      "Training epoch: [1839/2000][1/4]; Loss: 2.074134775856105\n",
      "Training Accuracy Epoch: [1838]\tPrec@1 30.469 (30.469)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (31.250)\n",
      "Training epoch: [1840/2000][1/4]; Loss: 2.0173883534264645\n",
      "Training Accuracy Epoch: [1839]\tPrec@1 39.062 (39.062)\tPrec@2 57.812 (57.812)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.812)\n",
      "Training epoch: [1841/2000][1/4]; Loss: 2.063172768268314\n",
      "Training Accuracy Epoch: [1840]\tPrec@1 32.031 (32.031)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (31.250)\n",
      "Training epoch: [1842/2000][1/4]; Loss: 2.0440921975744404\n",
      "Training Accuracy Epoch: [1841]\tPrec@1 32.812 (32.812)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (31.250)\n",
      "Training epoch: [1843/2000][1/4]; Loss: 2.065287511788982\n",
      "Training Accuracy Epoch: [1842]\tPrec@1 33.594 (33.594)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (28.906)\n",
      "Training epoch: [1844/2000][1/4]; Loss: 2.046380756534799\n",
      "Training Accuracy Epoch: [1843]\tPrec@1 32.812 (32.812)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (32.031)\n",
      "Training epoch: [1845/2000][1/4]; Loss: 2.08196093090727\n",
      "Training Accuracy Epoch: [1844]\tPrec@1 32.812 (32.812)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.125)\n",
      "Training epoch: [1846/2000][1/4]; Loss: 2.0657457789448253\n",
      "Training Accuracy Epoch: [1845]\tPrec@1 32.031 (32.031)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (25.781)\tPrec@2 (32.812)\n",
      "Training epoch: [1847/2000][1/4]; Loss: 2.0280970142517463\n",
      "Training Accuracy Epoch: [1846]\tPrec@1 34.375 (34.375)\tPrec@2 50.781 (50.781)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>>>\tPrec@1 (17.969)\tPrec@2 (30.469)\n",
      "Training epoch: [1848/2000][1/4]; Loss: 2.060982566914097\n",
      "Training Accuracy Epoch: [1847]\tPrec@1 32.812 (32.812)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (31.250)\n",
      "Training epoch: [1849/2000][1/4]; Loss: 1.9961948642120217\n",
      "Training Accuracy Epoch: [1848]\tPrec@1 42.188 (42.188)\tPrec@2 59.375 (59.375)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (29.688)\n",
      "Training epoch: [1850/2000][1/4]; Loss: 2.070277009938112\n",
      "Training Accuracy Epoch: [1849]\tPrec@1 34.375 (34.375)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (29.688)\n",
      "Training epoch: [1851/2000][1/4]; Loss: 2.0500128825827835\n",
      "Training Accuracy Epoch: [1850]\tPrec@1 33.594 (33.594)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (30.469)\n",
      "Training epoch: [1852/2000][1/4]; Loss: 2.07093701766915\n",
      "Training Accuracy Epoch: [1851]\tPrec@1 30.469 (30.469)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (32.812)\n",
      "Training epoch: [1853/2000][1/4]; Loss: 2.0597320141588336\n",
      "Training Accuracy Epoch: [1852]\tPrec@1 38.281 (38.281)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.812)\n",
      "Training epoch: [1854/2000][1/4]; Loss: 2.0245848622613174\n",
      "Training Accuracy Epoch: [1853]\tPrec@1 36.719 (36.719)\tPrec@2 59.375 (59.375)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (29.688)\n",
      "Training epoch: [1855/2000][1/4]; Loss: 1.969268583592115\n",
      "Training Accuracy Epoch: [1854]\tPrec@1 50.000 (50.000)\tPrec@2 63.281 (63.281)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (29.688)\n",
      "Training epoch: [1856/2000][1/4]; Loss: 2.029983794394205\n",
      "Training Accuracy Epoch: [1855]\tPrec@1 32.031 (32.031)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (32.812)\n",
      "Training epoch: [1857/2000][1/4]; Loss: 2.0022659954886897\n",
      "Training Accuracy Epoch: [1856]\tPrec@1 39.844 (39.844)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (29.688)\n",
      "Training epoch: [1858/2000][1/4]; Loss: 2.0441676512364095\n",
      "Training Accuracy Epoch: [1857]\tPrec@1 35.156 (35.156)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (29.688)\n",
      "Training epoch: [1859/2000][1/4]; Loss: 2.054411697775538\n",
      "Training Accuracy Epoch: [1858]\tPrec@1 37.500 (37.500)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (30.469)\n",
      "Training epoch: [1860/2000][1/4]; Loss: 2.0085493407379924\n",
      "Training Accuracy Epoch: [1859]\tPrec@1 36.719 (36.719)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (34.375)\n",
      "Training epoch: [1861/2000][1/4]; Loss: 2.019733458342732\n",
      "Training Accuracy Epoch: [1860]\tPrec@1 37.500 (37.500)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (34.375)\n",
      "Training epoch: [1862/2000][1/4]; Loss: 2.057089041722835\n",
      "Training Accuracy Epoch: [1861]\tPrec@1 35.156 (35.156)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (31.250)\n",
      "Training epoch: [1863/2000][1/4]; Loss: 2.0736574480112275\n",
      "Training Accuracy Epoch: [1862]\tPrec@1 28.125 (28.125)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.812)\n",
      "Training epoch: [1864/2000][1/4]; Loss: 2.020244532507123\n",
      "Training Accuracy Epoch: [1863]\tPrec@1 36.719 (36.719)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (32.031)\n",
      "Training epoch: [1865/2000][1/4]; Loss: 2.0499393208095076\n",
      "Training Accuracy Epoch: [1864]\tPrec@1 34.375 (34.375)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (32.031)\n",
      "Training epoch: [1866/2000][1/4]; Loss: 2.0595795601055222\n",
      "Training Accuracy Epoch: [1865]\tPrec@1 32.031 (32.031)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (28.906)\n",
      "Training epoch: [1867/2000][1/4]; Loss: 2.0622641292227266\n",
      "Training Accuracy Epoch: [1866]\tPrec@1 34.375 (34.375)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (32.031)\n",
      "Training epoch: [1868/2000][1/4]; Loss: 2.022573640480133\n",
      "Training Accuracy Epoch: [1867]\tPrec@1 39.062 (39.062)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (31.250)\n",
      "Training epoch: [1869/2000][1/4]; Loss: 2.0406487122643404\n",
      "Training Accuracy Epoch: [1868]\tPrec@1 38.281 (38.281)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (28.906)\n",
      "Training epoch: [1870/2000][1/4]; Loss: 2.0169692132746344\n",
      "Training Accuracy Epoch: [1869]\tPrec@1 35.938 (35.938)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (36.719)\n",
      "Training epoch: [1871/2000][1/4]; Loss: 2.0985990181069343\n",
      "Training Accuracy Epoch: [1870]\tPrec@1 31.250 (31.250)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (33.594)\n",
      "Training epoch: [1872/2000][1/4]; Loss: 2.069150579456492\n",
      "Training Accuracy Epoch: [1871]\tPrec@1 34.375 (34.375)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.031)\n",
      "Training epoch: [1873/2000][1/4]; Loss: 2.081143781183267\n",
      "Training Accuracy Epoch: [1872]\tPrec@1 29.688 (29.688)\tPrec@2 37.500 (37.500)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (32.031)\n",
      "Training epoch: [1874/2000][1/4]; Loss: 2.0037959579012536\n",
      "Training Accuracy Epoch: [1873]\tPrec@1 38.281 (38.281)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (31.250)\n",
      "Training epoch: [1875/2000][1/4]; Loss: 2.1094668717762923\n",
      "Training Accuracy Epoch: [1874]\tPrec@1 26.562 (26.562)\tPrec@2 42.969 (42.969)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (30.469)\n",
      "Training epoch: [1876/2000][1/4]; Loss: 2.076183761784564\n",
      "Training Accuracy Epoch: [1875]\tPrec@1 33.594 (33.594)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (31.250)\n",
      "Training epoch: [1877/2000][1/4]; Loss: 2.075343462308398\n",
      "Training Accuracy Epoch: [1876]\tPrec@1 28.906 (28.906)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (30.469)\n",
      "Training epoch: [1878/2000][1/4]; Loss: 2.0256146886148434\n",
      "Training Accuracy Epoch: [1877]\tPrec@1 35.938 (35.938)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (30.469)\n",
      "Training epoch: [1879/2000][1/4]; Loss: 2.0450538915813556\n",
      "Training Accuracy Epoch: [1878]\tPrec@1 35.938 (35.938)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (33.594)\n",
      "Training epoch: [1880/2000][1/4]; Loss: 2.0426433556890258\n",
      "Training Accuracy Epoch: [1879]\tPrec@1 39.062 (39.062)\tPrec@2 57.031 (57.031)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (30.469)\n",
      "Training epoch: [1881/2000][1/4]; Loss: 2.1159866026926695\n",
      "Training Accuracy Epoch: [1880]\tPrec@1 25.000 (25.000)\tPrec@2 41.406 (41.406)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (33.594)\n",
      "Training epoch: [1882/2000][1/4]; Loss: 2.0699250776774556\n",
      "Training Accuracy Epoch: [1881]\tPrec@1 32.031 (32.031)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (30.469)\n",
      "Training epoch: [1883/2000][1/4]; Loss: 2.0962881475838575\n",
      "Training Accuracy Epoch: [1882]\tPrec@1 35.938 (35.938)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.031)\n",
      "Training epoch: [1884/2000][1/4]; Loss: 1.986680755519377\n",
      "Training Accuracy Epoch: [1883]\tPrec@1 42.969 (42.969)\tPrec@2 57.812 (57.812)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (35.156)\n",
      "Training epoch: [1885/2000][1/4]; Loss: 2.042428366082758\n",
      "Training Accuracy Epoch: [1884]\tPrec@1 35.156 (35.156)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (30.469)\n",
      "Training epoch: [1886/2000][1/4]; Loss: 2.0855815574630974\n",
      "Training Accuracy Epoch: [1885]\tPrec@1 28.906 (28.906)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (29.688)\n",
      "Training epoch: [1887/2000][1/4]; Loss: 2.0647455089165816\n",
      "Training Accuracy Epoch: [1886]\tPrec@1 33.594 (33.594)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (31.250)\n",
      "Training epoch: [1888/2000][1/4]; Loss: 2.026036313709134\n",
      "Training Accuracy Epoch: [1887]\tPrec@1 36.719 (36.719)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (28.125)\n",
      "Training epoch: [1889/2000][1/4]; Loss: 2.036972926016238\n",
      "Training Accuracy Epoch: [1888]\tPrec@1 36.719 (36.719)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (31.250)\n",
      "Training epoch: [1890/2000][1/4]; Loss: 2.029359148428014\n",
      "Training Accuracy Epoch: [1889]\tPrec@1 38.281 (38.281)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.031)\n",
      "Training epoch: [1891/2000][1/4]; Loss: 2.011919010651897\n",
      "Training Accuracy Epoch: [1890]\tPrec@1 42.188 (42.188)\tPrec@2 56.250 (56.250)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (33.594)\n",
      "Training epoch: [1892/2000][1/4]; Loss: 2.0939498132326038\n",
      "Training Accuracy Epoch: [1891]\tPrec@1 34.375 (34.375)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (32.031)\n",
      "Training epoch: [1893/2000][1/4]; Loss: 2.0459727666606513\n",
      "Training Accuracy Epoch: [1892]\tPrec@1 33.594 (33.594)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (35.156)\n",
      "Training epoch: [1894/2000][1/4]; Loss: 2.0392117145663735\n",
      "Training Accuracy Epoch: [1893]\tPrec@1 37.500 (37.500)\tPrec@2 50.000 (50.000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>>>\tPrec@1 (17.969)\tPrec@2 (31.250)\n",
      "Training epoch: [1895/2000][1/4]; Loss: 2.083080068212323\n",
      "Training Accuracy Epoch: [1894]\tPrec@1 28.906 (28.906)\tPrec@2 42.188 (42.188)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (29.688)\n",
      "Training epoch: [1896/2000][1/4]; Loss: 2.036500666811617\n",
      "Training Accuracy Epoch: [1895]\tPrec@1 41.406 (41.406)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (28.125)\n",
      "Training epoch: [1897/2000][1/4]; Loss: 2.0114445361504316\n",
      "Training Accuracy Epoch: [1896]\tPrec@1 38.281 (38.281)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (32.031)\n",
      "Training epoch: [1898/2000][1/4]; Loss: 2.032368278212398\n",
      "Training Accuracy Epoch: [1897]\tPrec@1 38.281 (38.281)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (31.250)\n",
      "Training epoch: [1899/2000][1/4]; Loss: 2.030312696489376\n",
      "Training Accuracy Epoch: [1898]\tPrec@1 39.062 (39.062)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (32.031)\n",
      "Training epoch: [1900/2000][1/4]; Loss: 2.0717069935962256\n",
      "Training Accuracy Epoch: [1899]\tPrec@1 33.594 (33.594)\tPrec@2 44.531 (44.531)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (29.688)\n",
      "Training epoch: [1901/2000][1/4]; Loss: 2.0528378572592607\n",
      "Training Accuracy Epoch: [1900]\tPrec@1 35.156 (35.156)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (30.469)\n",
      "Training epoch: [1902/2000][1/4]; Loss: 1.9797671450443524\n",
      "Training Accuracy Epoch: [1901]\tPrec@1 37.500 (37.500)\tPrec@2 64.062 (64.062)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (29.688)\n",
      "Training epoch: [1903/2000][1/4]; Loss: 2.042904492925644\n",
      "Training Accuracy Epoch: [1902]\tPrec@1 36.719 (36.719)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (33.594)\n",
      "Training epoch: [1904/2000][1/4]; Loss: 2.0302949318054018\n",
      "Training Accuracy Epoch: [1903]\tPrec@1 36.719 (36.719)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (27.344)\n",
      "Training epoch: [1905/2000][1/4]; Loss: 1.970569740119588\n",
      "Training Accuracy Epoch: [1904]\tPrec@1 42.969 (42.969)\tPrec@2 58.594 (58.594)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (33.594)\n",
      "Training epoch: [1906/2000][1/4]; Loss: 2.052080220840698\n",
      "Training Accuracy Epoch: [1905]\tPrec@1 33.594 (33.594)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (34.375)\n",
      "Training epoch: [1907/2000][1/4]; Loss: 2.06235547198816\n",
      "Training Accuracy Epoch: [1906]\tPrec@1 35.938 (35.938)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (30.469)\n",
      "Training epoch: [1908/2000][1/4]; Loss: 2.0299228124011783\n",
      "Training Accuracy Epoch: [1907]\tPrec@1 39.062 (39.062)\tPrec@2 56.250 (56.250)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (30.469)\n",
      "Training epoch: [1909/2000][1/4]; Loss: 2.0516164777318373\n",
      "Training Accuracy Epoch: [1908]\tPrec@1 33.594 (33.594)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.125)\n",
      "Training epoch: [1910/2000][1/4]; Loss: 2.0758836634162146\n",
      "Training Accuracy Epoch: [1909]\tPrec@1 27.344 (27.344)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (35.938)\n",
      "Training epoch: [1911/2000][1/4]; Loss: 2.0812588644883583\n",
      "Training Accuracy Epoch: [1910]\tPrec@1 30.469 (30.469)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (13.281)\tPrec@2 (25.000)\n",
      "Training epoch: [1912/2000][1/4]; Loss: 2.030547526972779\n",
      "Training Accuracy Epoch: [1911]\tPrec@1 38.281 (38.281)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (30.469)\n",
      "Training epoch: [1913/2000][1/4]; Loss: 2.0719513426422056\n",
      "Training Accuracy Epoch: [1912]\tPrec@1 36.719 (36.719)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (29.688)\n",
      "Training epoch: [1914/2000][1/4]; Loss: 2.071665978338697\n",
      "Training Accuracy Epoch: [1913]\tPrec@1 35.938 (35.938)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (34.375)\n",
      "Training epoch: [1915/2000][1/4]; Loss: 2.072850324830085\n",
      "Training Accuracy Epoch: [1914]\tPrec@1 33.594 (33.594)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.906)\n",
      "Training epoch: [1916/2000][1/4]; Loss: 2.0118952887011776\n",
      "Training Accuracy Epoch: [1915]\tPrec@1 34.375 (34.375)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (29.688)\n",
      "Training epoch: [1917/2000][1/4]; Loss: 2.002823077909146\n",
      "Training Accuracy Epoch: [1916]\tPrec@1 44.531 (44.531)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.125)\n",
      "Training epoch: [1918/2000][1/4]; Loss: 2.032581321805397\n",
      "Training Accuracy Epoch: [1917]\tPrec@1 37.500 (37.500)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (30.469)\n",
      "Training epoch: [1919/2000][1/4]; Loss: 2.046595080487991\n",
      "Training Accuracy Epoch: [1918]\tPrec@1 35.156 (35.156)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (32.031)\n",
      "Training epoch: [1920/2000][1/4]; Loss: 2.0906019216858365\n",
      "Training Accuracy Epoch: [1919]\tPrec@1 29.688 (29.688)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (32.812)\n",
      "Training epoch: [1921/2000][1/4]; Loss: 1.9972248085951125\n",
      "Training Accuracy Epoch: [1920]\tPrec@1 40.625 (40.625)\tPrec@2 56.250 (56.250)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.812)\n",
      "Training epoch: [1922/2000][1/4]; Loss: 2.024331345777458\n",
      "Training Accuracy Epoch: [1921]\tPrec@1 38.281 (38.281)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (30.469)\n",
      "Training epoch: [1923/2000][1/4]; Loss: 2.0072162015474655\n",
      "Training Accuracy Epoch: [1922]\tPrec@1 37.500 (37.500)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (14.844)\tPrec@2 (28.906)\n",
      "Training epoch: [1924/2000][1/4]; Loss: 2.056288466879668\n",
      "Training Accuracy Epoch: [1923]\tPrec@1 28.906 (28.906)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (29.688)\n",
      "Training epoch: [1925/2000][1/4]; Loss: 2.028684798872113\n",
      "Training Accuracy Epoch: [1924]\tPrec@1 36.719 (36.719)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (31.250)\n",
      "Training epoch: [1926/2000][1/4]; Loss: 2.018284633124402\n",
      "Training Accuracy Epoch: [1925]\tPrec@1 35.938 (35.938)\tPrec@2 56.250 (56.250)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (32.812)\n",
      "Training epoch: [1927/2000][1/4]; Loss: 2.0656249263759854\n",
      "Training Accuracy Epoch: [1926]\tPrec@1 32.812 (32.812)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (29.688)\n",
      "Training epoch: [1928/2000][1/4]; Loss: 2.0349300037181606\n",
      "Training Accuracy Epoch: [1927]\tPrec@1 37.500 (37.500)\tPrec@2 56.250 (56.250)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.906)\n",
      "Training epoch: [1929/2000][1/4]; Loss: 2.002442192466001\n",
      "Training Accuracy Epoch: [1928]\tPrec@1 42.969 (42.969)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (35.156)\n",
      "Training epoch: [1930/2000][1/4]; Loss: 2.0158721165659412\n",
      "Training Accuracy Epoch: [1929]\tPrec@1 39.062 (39.062)\tPrec@2 59.375 (59.375)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (27.344)\n",
      "Training epoch: [1931/2000][1/4]; Loss: 2.05098766730161\n",
      "Training Accuracy Epoch: [1930]\tPrec@1 31.250 (31.250)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (26.562)\n",
      "Training epoch: [1932/2000][1/4]; Loss: 2.0368802210575767\n",
      "Training Accuracy Epoch: [1931]\tPrec@1 39.062 (39.062)\tPrec@2 57.031 (57.031)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (31.250)\n",
      "Training epoch: [1933/2000][1/4]; Loss: 2.029340867105491\n",
      "Training Accuracy Epoch: [1932]\tPrec@1 38.281 (38.281)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.812)\n",
      "Training epoch: [1934/2000][1/4]; Loss: 2.0063089873002635\n",
      "Training Accuracy Epoch: [1933]\tPrec@1 41.406 (41.406)\tPrec@2 59.375 (59.375)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (31.250)\n",
      "Training epoch: [1935/2000][1/4]; Loss: 2.089183693961428\n",
      "Training Accuracy Epoch: [1934]\tPrec@1 37.500 (37.500)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (31.250)\n",
      "Training epoch: [1936/2000][1/4]; Loss: 2.0572082604333453\n",
      "Training Accuracy Epoch: [1935]\tPrec@1 39.062 (39.062)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (31.250)\n",
      "Training epoch: [1937/2000][1/4]; Loss: 2.0595790481888177\n",
      "Training Accuracy Epoch: [1936]\tPrec@1 33.594 (33.594)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (31.250)\n",
      "Training epoch: [1938/2000][1/4]; Loss: 2.047250042924895\n",
      "Training Accuracy Epoch: [1937]\tPrec@1 35.938 (35.938)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (31.250)\n",
      "Training epoch: [1939/2000][1/4]; Loss: 2.0351151410968567\n",
      "Training Accuracy Epoch: [1938]\tPrec@1 35.938 (35.938)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (35.156)\n",
      "Training epoch: [1940/2000][1/4]; Loss: 2.045622528251966\n",
      "Training Accuracy Epoch: [1939]\tPrec@1 32.812 (32.812)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (28.906)\n",
      "Training epoch: [1941/2000][1/4]; Loss: 2.03723494787489\n",
      "Training Accuracy Epoch: [1940]\tPrec@1 35.938 (35.938)\tPrec@2 48.438 (48.438)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.812)\n",
      "Training epoch: [1942/2000][1/4]; Loss: 2.0248187067998056\n",
      "Training Accuracy Epoch: [1941]\tPrec@1 38.281 (38.281)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (28.906)\n",
      "Training epoch: [1943/2000][1/4]; Loss: 2.0908772105445665\n",
      "Training Accuracy Epoch: [1942]\tPrec@1 31.250 (31.250)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (31.250)\n",
      "Training epoch: [1944/2000][1/4]; Loss: 2.0020639814173653\n",
      "Training Accuracy Epoch: [1943]\tPrec@1 32.812 (32.812)\tPrec@2 57.812 (57.812)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (35.156)\n",
      "Training epoch: [1945/2000][1/4]; Loss: 2.036755517491842\n",
      "Training Accuracy Epoch: [1944]\tPrec@1 38.281 (38.281)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (32.031)\n",
      "Training epoch: [1946/2000][1/4]; Loss: 2.0508172905427844\n",
      "Training Accuracy Epoch: [1945]\tPrec@1 35.156 (35.156)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (28.125)\n",
      "Training epoch: [1947/2000][1/4]; Loss: 2.043122830920516\n",
      "Training Accuracy Epoch: [1946]\tPrec@1 35.156 (35.156)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (33.594)\n",
      "Training epoch: [1948/2000][1/4]; Loss: 2.0797330600746387\n",
      "Training Accuracy Epoch: [1947]\tPrec@1 27.344 (27.344)\tPrec@2 43.750 (43.750)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (31.250)\n",
      "Training epoch: [1949/2000][1/4]; Loss: 2.034474599202878\n",
      "Training Accuracy Epoch: [1948]\tPrec@1 30.469 (30.469)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (34.375)\n",
      "Training epoch: [1950/2000][1/4]; Loss: 2.0330998547395\n",
      "Training Accuracy Epoch: [1949]\tPrec@1 32.031 (32.031)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (34.375)\n",
      "Training epoch: [1951/2000][1/4]; Loss: 2.052846168107874\n",
      "Training Accuracy Epoch: [1950]\tPrec@1 33.594 (33.594)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (34.375)\n",
      "Training epoch: [1952/2000][1/4]; Loss: 2.048460898646513\n",
      "Training Accuracy Epoch: [1951]\tPrec@1 38.281 (38.281)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (32.031)\n",
      "Training epoch: [1953/2000][1/4]; Loss: 2.015816166148318\n",
      "Training Accuracy Epoch: [1952]\tPrec@1 39.844 (39.844)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (29.688)\n",
      "Training epoch: [1954/2000][1/4]; Loss: 2.0310933509963456\n",
      "Training Accuracy Epoch: [1953]\tPrec@1 41.406 (41.406)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (29.688)\n",
      "Training epoch: [1955/2000][1/4]; Loss: 2.0257979949746274\n",
      "Training Accuracy Epoch: [1954]\tPrec@1 37.500 (37.500)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (32.031)\n",
      "Training epoch: [1956/2000][1/4]; Loss: 2.070446975514826\n",
      "Training Accuracy Epoch: [1955]\tPrec@1 34.375 (34.375)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (29.688)\n",
      "Training epoch: [1957/2000][1/4]; Loss: 2.0356425884584084\n",
      "Training Accuracy Epoch: [1956]\tPrec@1 35.938 (35.938)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (34.375)\n",
      "Training epoch: [1958/2000][1/4]; Loss: 2.085752530043954\n",
      "Training Accuracy Epoch: [1957]\tPrec@1 35.156 (35.156)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (33.594)\n",
      "Training epoch: [1959/2000][1/4]; Loss: 2.0616717423407804\n",
      "Training Accuracy Epoch: [1958]\tPrec@1 39.844 (39.844)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.812)\n",
      "Training epoch: [1960/2000][1/4]; Loss: 2.065759392241054\n",
      "Training Accuracy Epoch: [1959]\tPrec@1 32.031 (32.031)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (35.156)\n",
      "Training epoch: [1961/2000][1/4]; Loss: 2.027079042299856\n",
      "Training Accuracy Epoch: [1960]\tPrec@1 34.375 (34.375)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (33.594)\n",
      "Training epoch: [1962/2000][1/4]; Loss: 2.0446752163140007\n",
      "Training Accuracy Epoch: [1961]\tPrec@1 34.375 (34.375)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (33.594)\n",
      "Training epoch: [1963/2000][1/4]; Loss: 2.0343176866414825\n",
      "Training Accuracy Epoch: [1962]\tPrec@1 38.281 (38.281)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (32.812)\n",
      "Training epoch: [1964/2000][1/4]; Loss: 2.0109976146133546\n",
      "Training Accuracy Epoch: [1963]\tPrec@1 40.625 (40.625)\tPrec@2 57.812 (57.812)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (33.594)\n",
      "Training epoch: [1965/2000][1/4]; Loss: 2.043460012880492\n",
      "Training Accuracy Epoch: [1964]\tPrec@1 31.250 (31.250)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (35.938)\n",
      "Training epoch: [1966/2000][1/4]; Loss: 2.0536594020674297\n",
      "Training Accuracy Epoch: [1965]\tPrec@1 37.500 (37.500)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (32.812)\n",
      "Training epoch: [1967/2000][1/4]; Loss: 2.072086744098957\n",
      "Training Accuracy Epoch: [1966]\tPrec@1 31.250 (31.250)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (30.469)\n",
      "Training epoch: [1968/2000][1/4]; Loss: 2.050019674756087\n",
      "Training Accuracy Epoch: [1967]\tPrec@1 35.938 (35.938)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (29.688)\n",
      "Training epoch: [1969/2000][1/4]; Loss: 2.101195383212923\n",
      "Training Accuracy Epoch: [1968]\tPrec@1 30.469 (30.469)\tPrec@2 50.000 (50.000)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (30.469)\n",
      "Training epoch: [1970/2000][1/4]; Loss: 2.0239119970251243\n",
      "Training Accuracy Epoch: [1969]\tPrec@1 39.844 (39.844)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (33.594)\n",
      "Training epoch: [1971/2000][1/4]; Loss: 2.036213942015686\n",
      "Training Accuracy Epoch: [1970]\tPrec@1 39.844 (39.844)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (23.438)\tPrec@2 (30.469)\n",
      "Training epoch: [1972/2000][1/4]; Loss: 2.0042735401291507\n",
      "Training Accuracy Epoch: [1971]\tPrec@1 42.188 (42.188)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (19.531)\tPrec@2 (30.469)\n",
      "Training epoch: [1973/2000][1/4]; Loss: 2.085071182404661\n",
      "Training Accuracy Epoch: [1972]\tPrec@1 28.125 (28.125)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (30.469)\n",
      "Training epoch: [1974/2000][1/4]; Loss: 2.060858987348528\n",
      "Training Accuracy Epoch: [1973]\tPrec@1 35.156 (35.156)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (34.375)\n",
      "Training epoch: [1975/2000][1/4]; Loss: 2.075459076388675\n",
      "Training Accuracy Epoch: [1974]\tPrec@1 30.469 (30.469)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (34.375)\n",
      "Training epoch: [1976/2000][1/4]; Loss: 2.0188249448945776\n",
      "Training Accuracy Epoch: [1975]\tPrec@1 32.031 (32.031)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (32.812)\n",
      "Training epoch: [1977/2000][1/4]; Loss: 2.0408731002933678\n",
      "Training Accuracy Epoch: [1976]\tPrec@1 40.625 (40.625)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (27.344)\n",
      "Training epoch: [1978/2000][1/4]; Loss: 2.0636498546050612\n",
      "Training Accuracy Epoch: [1977]\tPrec@1 34.375 (34.375)\tPrec@2 47.656 (47.656)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (35.156)\n",
      "Training epoch: [1979/2000][1/4]; Loss: 2.060270334222377\n",
      "Training Accuracy Epoch: [1978]\tPrec@1 34.375 (34.375)\tPrec@2 46.094 (46.094)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (30.469)\n",
      "Training epoch: [1980/2000][1/4]; Loss: 2.0170235737778444\n",
      "Training Accuracy Epoch: [1979]\tPrec@1 36.719 (36.719)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (22.656)\tPrec@2 (33.594)\n",
      "Training epoch: [1981/2000][1/4]; Loss: 2.057496843875849\n",
      "Training Accuracy Epoch: [1980]\tPrec@1 32.812 (32.812)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (31.250)\n",
      "Training epoch: [1982/2000][1/4]; Loss: 2.0310199278750543\n",
      "Training Accuracy Epoch: [1981]\tPrec@1 35.938 (35.938)\tPrec@2 56.250 (56.250)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (32.031)\n",
      "Training epoch: [1983/2000][1/4]; Loss: 2.0853779784195914\n",
      "Training Accuracy Epoch: [1982]\tPrec@1 30.469 (30.469)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (30.469)\n",
      "Training epoch: [1984/2000][1/4]; Loss: 2.0496265451460016\n",
      "Training Accuracy Epoch: [1983]\tPrec@1 39.062 (39.062)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (29.688)\n",
      "Training epoch: [1985/2000][1/4]; Loss: 2.0048981817703413\n",
      "Training Accuracy Epoch: [1984]\tPrec@1 39.062 (39.062)\tPrec@2 54.688 (54.688)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (32.031)\n",
      "Training epoch: [1986/2000][1/4]; Loss: 2.0149430815441565\n",
      "Training Accuracy Epoch: [1985]\tPrec@1 40.625 (40.625)\tPrec@2 55.469 (55.469)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.906)\n",
      "Training epoch: [1987/2000][1/4]; Loss: 2.023298305871558\n",
      "Training Accuracy Epoch: [1986]\tPrec@1 39.062 (39.062)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (30.469)\n",
      "Training epoch: [1988/2000][1/4]; Loss: 2.0278822190076498\n",
      "Training Accuracy Epoch: [1987]\tPrec@1 39.844 (39.844)\tPrec@2 54.688 (54.688)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===>>>\tPrec@1 (16.406)\tPrec@2 (28.125)\n",
      "Training epoch: [1989/2000][1/4]; Loss: 2.025450024315959\n",
      "Training Accuracy Epoch: [1988]\tPrec@1 37.500 (37.500)\tPrec@2 51.562 (51.562)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (33.594)\n",
      "Training epoch: [1990/2000][1/4]; Loss: 2.009711660009947\n",
      "Training Accuracy Epoch: [1989]\tPrec@1 38.281 (38.281)\tPrec@2 50.781 (50.781)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (31.250)\n",
      "Training epoch: [1991/2000][1/4]; Loss: 2.0459867447656483\n",
      "Training Accuracy Epoch: [1990]\tPrec@1 37.500 (37.500)\tPrec@2 48.438 (48.438)\n",
      "===>>>\tPrec@1 (21.094)\tPrec@2 (32.812)\n",
      "Training epoch: [1992/2000][1/4]; Loss: 2.0725490857407913\n",
      "Training Accuracy Epoch: [1991]\tPrec@1 31.250 (31.250)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (20.312)\tPrec@2 (30.469)\n",
      "Training epoch: [1993/2000][1/4]; Loss: 2.027833875946699\n",
      "Training Accuracy Epoch: [1992]\tPrec@1 35.938 (35.938)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (21.875)\tPrec@2 (32.812)\n",
      "Training epoch: [1994/2000][1/4]; Loss: 2.0341740928210426\n",
      "Training Accuracy Epoch: [1993]\tPrec@1 43.750 (43.750)\tPrec@2 57.031 (57.031)\n",
      "===>>>\tPrec@1 (17.969)\tPrec@2 (28.906)\n",
      "Training epoch: [1995/2000][1/4]; Loss: 2.006549956868357\n",
      "Training Accuracy Epoch: [1994]\tPrec@1 37.500 (37.500)\tPrec@2 53.906 (53.906)\n",
      "===>>>\tPrec@1 (17.188)\tPrec@2 (30.469)\n",
      "Training epoch: [1996/2000][1/4]; Loss: 2.059139742426068\n",
      "Training Accuracy Epoch: [1995]\tPrec@1 28.906 (28.906)\tPrec@2 45.312 (45.312)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (27.344)\n",
      "Training epoch: [1997/2000][1/4]; Loss: 2.071935333205405\n",
      "Training Accuracy Epoch: [1996]\tPrec@1 32.031 (32.031)\tPrec@2 46.875 (46.875)\n",
      "===>>>\tPrec@1 (16.406)\tPrec@2 (32.031)\n",
      "Training epoch: [1998/2000][1/4]; Loss: 2.038527565785178\n",
      "Training Accuracy Epoch: [1997]\tPrec@1 35.938 (35.938)\tPrec@2 53.125 (53.125)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (33.594)\n",
      "Training epoch: [1999/2000][1/4]; Loss: 2.06515062708543\n",
      "Training Accuracy Epoch: [1998]\tPrec@1 30.469 (30.469)\tPrec@2 49.219 (49.219)\n",
      "===>>>\tPrec@1 (18.750)\tPrec@2 (28.906)\n",
      "Training epoch: [2000/2000][1/4]; Loss: 2.0355075099942668\n",
      "Training Accuracy Epoch: [1999]\tPrec@1 37.500 (37.500)\tPrec@2 52.344 (52.344)\n",
      "===>>>\tPrec@1 (15.625)\tPrec@2 (29.688)\n",
      "Model saved at: models/ERK/2020-02-21-15:39:22_WW_Archive.pytorch\n",
      "Elapsed time: 703.4328055381775\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(dataset=data_train,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4,\n",
    "                          drop_last=True)\n",
    "test_loader = DataLoader(dataset=data_test,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True,\n",
    "                         num_workers=4,\n",
    "                         drop_last=True)\n",
    "\n",
    "t0 = time.time()\n",
    "mymodel = TrainModel(model, optimizer, criterion, scheduler, train_loader, test_loader, nepochs)\n",
    "t1 = time.time()\n",
    "\n",
    "print('Elapsed time: {}'.format(t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
