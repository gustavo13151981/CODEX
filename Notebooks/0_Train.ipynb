{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training notebook\n",
    "\n",
    "## Import libraries\n",
    "\n",
    "Make sure to import model class, dataset class and associated preprocessing operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import datetime\n",
    "from tensorboardX import SummaryWriter\n",
    "import os\n",
    "import zipfile\n",
    "import time\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Custom functions/classes\n",
    "from load_data import DataProcesser\n",
    "from train_utils import accuracy, AverageMeter\n",
    "from models import ConvNetCam, ConvNetCamBi\n",
    "from class_dataset import myDataset, ToTensor, Subtract, RandomShift, RandomNoise, RandomCrop, FixedCrop\n",
    "\n",
    "# For reproducibility\n",
    "myseed = 7\n",
    "torch.manual_seed(myseed)\n",
    "torch.cuda.manual_seed(myseed)\n",
    "np.random.seed(myseed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Hyperparameters for training\n",
    "\n",
    "Hyperparameters for training:\n",
    "- nepochs: int, number of training epochs\n",
    "- batch size\n",
    "- lr: float, initial learning rate.\n",
    "\n",
    "Hyperparameters to setup the model dimensions:\n",
    "- length: int, length of a trajectory. This is the length as the model will expect it. Setting it to smaller value than the actual length can be used for preprocessing/data jittering.\n",
    "- nclass: int, number of output classes.\n",
    "- nfeatures: int, size of the input representation before output layer. This also corresponds to the number of filters in the last convolution layer.\n",
    "- selected_classes: list, select only some classes from input dataset. Leave empty to use all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nepochs = 3\n",
    "batch_size = 128\n",
    "lr = 1e-2\n",
    "\n",
    "length = 200\n",
    "nclass = 6\n",
    "nfeatures = 10\n",
    "selected_classes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load and process data, Data augmentation\n",
    "\n",
    "Define which data to load and whether/how to preprocess the batch. \n",
    "- data_file: str, path to a .zip that can be loaded as a DataProcesser. The archove must contain 3 files: one for the\n",
    " data, one for the split train/validation, one with the classes informations. See DataProcesser.read_archive().\n",
    "- meas_var: list of str, names of the measurement variables. In DataProcesser convention, this is the prefix in a\n",
    " column name that contains a measurement (time being the suffix). Pay attention to the order since this is how the dimensions of a sample of data will be ordered (i.e. 1st in the list will form 1st row of measurements in the sample, 2nd is the 2nd, etc...)\n",
    "- start_time/end_time: int, use to subset data to a specific time range. Useful to completely exclude some acquisition times where irrelevant measurements are acquired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_file = '/home/marc/Dropbox/Work/TSclass/data/fly_hctsa.zip'\n",
    "#data_file = '/home/marc/Dropbox/Work/TSclass_GF/data/ErkAkt_6GF_len240_repl2_trim100.zip'\n",
    "meas_var = ['V']\n",
    "start_time = 0\n",
    "end_time = 599"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "data = DataProcesser(data_file)\n",
    "data.subset(sel_groups=meas_var, start_time=start_time, end_time=end_time)\n",
    "if selected_classes:\n",
    "    data.dataset = data.dataset[data.dataset[data.col_class].isin(selected_classes)]\n",
    "data.get_stats()\n",
    "# data.process(method='center_train', independent_groups=True)\n",
    "data.split_sets()\n",
    "data_train = myDataset(dataset=data.train_set, transform=transforms.Compose([\n",
    "    RandomCrop(output_size=length, ignore_na_tails=True),\n",
    "    #transforms.RandomApply([RandomNoise(mu=0, sigma=0.02)]),\n",
    "    #Subtract([data.stats['mu']['ERK']['train'], data.stats['mu']['AKT']['train']]),\n",
    "    Subtract(data.stats['mu']['V']['train']),\n",
    "    ToTensor()\n",
    "]))\n",
    "data_test = myDataset(dataset=data.validation_set, transform=transforms.Compose([\n",
    "    RandomCrop(output_size=length, ignore_na_tails=True),\n",
    "    #Subtract([data.stats['mu']['ERK']['train'], data.stats['mu']['AKT']['train']]),\n",
    "    Subtract(data.stats['mu']['V']['train']),\n",
    "    ToTensor()\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some trajectories to check that the data loading and processing is properly done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_smpl = 6\n",
    "indx_smpl = np.random.randint(0, len(data_train), n_smpl)\n",
    "\n",
    "col_ids = []\n",
    "col_lab = []\n",
    "col_mes = []\n",
    "# Long format for seaborn grid, for loop to avoid multiple indexing\n",
    "# This would triggers preprocessing multiple times and add randomness\n",
    "for i in indx_smpl:\n",
    "    smpl = data_train[i]\n",
    "    col_ids.append(smpl['identifier'])\n",
    "    col_lab.append(smpl['label'].item())\n",
    "    col_mes.append(smpl['series'].numpy().transpose())\n",
    "col_ids = pd.Series(np.hstack(np.repeat(col_ids, length)))\n",
    "col_lab = pd.Series(np.hstack(np.repeat(col_lab, length)))\n",
    "col_mes = pd.DataFrame(np.vstack(col_mes), columns=meas_var)\n",
    "col_tim = pd.Series(np.tile(np.arange(0, length), n_smpl))\n",
    "\n",
    "df_smpl = pd.concat([col_ids, col_lab, col_tim, col_mes], axis=1)\n",
    "df_smpl.rename(columns={0: 'identifier', 1: 'label', 2:'time'}, inplace=True)\n",
    "df_smpl = df_smpl.melt(id_vars=['identifier', 'label', 'time'], value_vars=meas_var)\n",
    "\n",
    "sns.set_style('white')\n",
    "sns.set_context('notebook')\n",
    "grid = sns.FacetGrid(data=df_smpl, col='identifier', col_wrap=3, sharex=True)\n",
    "grid.map_dataframe(sns.lineplot, x='time', y='value', hue='variable')\n",
    "grid.set(xlabel='Time', ylabel='Measurement Value')\n",
    "grid.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Resume training or new model\n",
    "\n",
    "Set to None for new model, otherwise provide the path to a saved model file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "load_model = None\n",
    "# load_model = 'path/to/file.pytorch'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Tensorboard logs and model save file\n",
    "\n",
    "Unique name for model with timestamp. Can follow training online with tensorboard with these logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "file_logs = os.path.splitext(os.path.basename(data_file))[0]  # file name without extension\n",
    "logs_str = 'logs/' + '_'.join(meas_var) + '/' + datetime.datetime.now().strftime('%Y-%m-%d-%H:%M:%S') + \\\n",
    "           '_' + file_logs + '/'\n",
    "writer = SummaryWriter(logs_str)\n",
    "save_model = 'models/' + logs_str.lstrip('logs/').rstrip('/') + '.pytorch'\n",
    "\n",
    "if not os.path.exists(file_logs):\n",
    "    os.makedirs(file_logs)\n",
    "if not os.path.exists('models/' + '_'.join(meas_var)):\n",
    "    os.makedirs('models/' + '_'.join(meas_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup model, loss and optimizer\n",
    "\n",
    "The model dimensions are tuned to fit the previous parameters. \n",
    "\n",
    "L2 regularization is controlled by the \"weight_decay\" in the optimizer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = ConvNetCam(batch_size=batch_size, nclass=nclass, length=length, nfeatures=nfeatures)\n",
    "if load_model:\n",
    "    model.load_state_dict(torch.load(load_model))\n",
    "model.double()\n",
    "cuda_available = torch.cuda.is_available()\n",
    "if cuda_available:\n",
    "    model = model.cuda()\n",
    "    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=1e-3)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[200, 400, 600, 800, 1000, 1500], gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def TrainModel(model, optimizer, criterion, scheduler, train_loader, test_loader, nepochs,\n",
    "               save_model=save_model, logs=True, save_pyfiles=True):\n",
    "    # ------------------------------------------------------------------------------------------------------------------\n",
    "    # Model, loss, optimizer\n",
    "    top1 = AverageMeter()\n",
    "    top2 = AverageMeter()\n",
    "\n",
    "    # Create zip archive with all python file at execution time\n",
    "    if save_pyfiles:\n",
    "        lpy = [i for i in os.listdir(\".\") if i.endswith(\".py\")]\n",
    "        with zipfile.ZipFile(logs_str + \"AllPyFiles.zip\", mode='w') as zipMe:\n",
    "            for file in lpy:\n",
    "                zipMe.write(file, compress_type=zipfile.ZIP_DEFLATED)\n",
    "    if logs:\n",
    "        print('Train logs saved at: {}'.format(logs_str))\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------------------------------\n",
    "    # Get adequate size of sample for nn.Conv layers\n",
    "    # Add a dummy channel dimension for conv1D layer (if multivariate, treat as a 2D plane with 1 channel)\n",
    "    assert len(train_loader.dataset[0]['series'].shape) == 2\n",
    "    nchannel, univar_length = train_loader.dataset[0]['series'].shape\n",
    "    if nchannel == 1:\n",
    "        view_size = (batch_size, 1, univar_length)\n",
    "    elif nchannel >= 2:\n",
    "        view_size = (batch_size, 1, nchannel, univar_length)\n",
    "\n",
    "    # ------------------------------------------------------------------------------------------------------------------\n",
    "    # Training loop\n",
    "    for epoch in range(nepochs):\n",
    "        scheduler.step()\n",
    "        model.train()\n",
    "        top1.reset()\n",
    "        top2.reset()\n",
    "\n",
    "        loss_train = []\n",
    "        for i_batch, sample_batch in enumerate(train_loader):\n",
    "            series, label = sample_batch['series'], sample_batch['label']\n",
    "            if cuda_available:\n",
    "                series, label = series.cuda(), label.cuda()\n",
    "            series = series.view(view_size)\n",
    "\n",
    "            prediction = model(series)\n",
    "\n",
    "            loss = criterion(prediction, label)\n",
    "            loss_train.append(loss.cpu().detach().numpy())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i_batch % 25 == 0:\n",
    "                print('Training epoch: [{0}/{4}][{1}/{2}]; Loss: {3}'.format(epoch + 1, i_batch + 1, len(train_loader),\n",
    "                                                                             loss, nepochs))\n",
    "\n",
    "            prec1, prec2 = accuracy(prediction, label, topk=(1, 2))\n",
    "            top1.update(prec1[0], series.size(0))\n",
    "            top2.update(prec2[0], series.size(0))\n",
    "\n",
    "            if i_batch % 100 == 0:\n",
    "                print('Training Accuracy Epoch: [{0}]\\t'\n",
    "                      'Prec@1 {top1.val.data:.3f} ({top1.avg.data:.3f})\\t'\n",
    "                      'Prec@2 {top2.val.data:.3f} ({top2.avg.data:.3f})'.format(\n",
    "                    epoch, top1=top1, top2=top2))\n",
    "            if logs:\n",
    "                writer.add_scalar('Train/Loss', loss, epoch * len(train_loader) + i_batch + 1)\n",
    "                writer.add_scalar('Train/Top1', top1.val, epoch * len(train_loader) + i_batch + 1)\n",
    "                writer.add_scalar('Train/Top2', top2.val, epoch * len(train_loader) + i_batch + 1)\n",
    "        if logs:\n",
    "            writer.add_scalar('MeanEpoch/Train_Loss', np.mean(loss_train), epoch)\n",
    "            writer.add_scalar('MeanEpoch/Train_Top1', top1.avg, epoch)\n",
    "            writer.add_scalar('MeanEpoch/Train_Top2', top2.avg, epoch)\n",
    "\n",
    "        # --------------------------------------------------------------------------------------------------------------\n",
    "        # Evaluation loop\n",
    "        model.eval()\n",
    "        top1.reset()\n",
    "        top2.reset()\n",
    "        loss_eval = []\n",
    "        for i_batch, sample_batch in enumerate(test_loader):\n",
    "            series, label = sample_batch['series'], sample_batch['label']\n",
    "            if cuda_available:\n",
    "                series, label = series.cuda(), label.cuda()\n",
    "            series = series.view(view_size)\n",
    "            label = torch.autograd.Variable(label)\n",
    "\n",
    "            prediction = model(series)\n",
    "            loss = criterion(prediction, label)\n",
    "            loss_eval.append(loss.cpu().detach().numpy())\n",
    "\n",
    "            prec1, prec2 = accuracy(prediction, label, topk=(1, 2))\n",
    "            top1.update(prec1[0], series.size(0))\n",
    "            top2.update(prec2[0], series.size(0))\n",
    "\n",
    "        # For validation loss, report only after the whole batch is processed\n",
    "        if logs:\n",
    "            writer.add_scalar('Val/Loss', loss, epoch * len(train_loader) + i_batch + 1)\n",
    "            writer.add_scalar('Val/Top1', top1.val, epoch * len(train_loader) + i_batch + 1)\n",
    "            writer.add_scalar('Val/Top2', top2.val, epoch * len(train_loader) + i_batch + 1)\n",
    "            writer.add_scalar('MeanEpoch/Val_Loss', np.mean(loss_eval), epoch)\n",
    "            writer.add_scalar('MeanEpoch/Val_Top1', top1.avg, epoch)\n",
    "            writer.add_scalar('MeanEpoch/Val_Top2', top2.avg, epoch)\n",
    "\n",
    "\n",
    "        print('===>>>\\t'\n",
    "              'Prec@1 ({top1.avg.data:.3f})\\t'\n",
    "              'Prec@2 ({top2.avg.data:.3f})'.format(top1=top1, top2=top2))\n",
    "\n",
    "    if save_model:\n",
    "        torch.save(model, save_model)\n",
    "        print('Model saved at: {}'.format(save_model))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Run the training\n",
    "\n",
    "Can follow the training in tensorboard with:\n",
    "```\n",
    "tensorboard --logdir \"path/to/logs\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=data_train,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4,\n",
    "                          drop_last=True)\n",
    "test_loader = DataLoader(dataset=data_test,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True,\n",
    "                         num_workers=4,\n",
    "                         drop_last=True)\n",
    "\n",
    "t0 = time.time()\n",
    "mymodel = TrainModel(model, optimizer, criterion, scheduler, train_loader, test_loader, nepochs)\n",
    "t1 = time.time()\n",
    "\n",
    "print('Elapsed time: {}'.format(t1 - t0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
